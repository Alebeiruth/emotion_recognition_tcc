{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c92bc290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "CUDA version: 12.1\n",
      "Memory: 11.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: EfficientViT PyTorch Implementation for Emotion Classification - Imports and Configuration\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# GPU configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 224  # Standard input size for vision models\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "# EfficientViT specific configuration\n",
    "EFFICIENTVIT_CONFIG = {\n",
    "    'base_learning_rate': 0.001,\n",
    "    'fine_tune_learning_rate': 0.0001,\n",
    "    'dropout_rate': 0.3,\n",
    "    'fine_tune_layers': 30,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patch_size': 16,\n",
    "    'embed_dim': 192,\n",
    "    'depth': 12,\n",
    "    'num_heads': 3,\n",
    "    'mlp_ratio': 4.0,\n",
    "    'qkv_bias': True,\n",
    "    'drop_rate': 0.1,\n",
    "    'attn_drop_rate': 0.1,\n",
    "    'drop_path_rate': 0.1,\n",
    "    # Adicionar as configurações que estão faltando:\n",
    "    'embed_dims': [64, 128, 192],  # Lista de dimensões para cada estágio\n",
    "    'depths': [2, 4, 6],           # Lista de profundidades para cada estágio\n",
    "    'num_heads': [2, 4, 6]         # Lista de cabeças de atenção para cada estágio\n",
    "}\n",
    "\n",
    "# Emotion labels mapping\n",
    "EMOTION_LABELS = {\n",
    "    'Raiva': 0, 'Nojo': 1, 'Medo': 2, 'Felicidade': 3, \n",
    "    'Neutro': 4, 'Tristeza': 5, 'Surpresa': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c177a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset and Core EfficientViT Components\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Custom dataset for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image format for transforms\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Ensure image is in uint8 format [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# EfficientViT core components\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"2D Image to Patch Embedding with improved efficiency\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b138d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: EfficientViT Attention and Block Components\n",
    "\n",
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"Efficient Multi-Head Self-Attention with linear complexity\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            x_ = self.norm(x_)\n",
    "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EfficientViTBlock(nn.Module):\n",
    "    \"\"\"EfficientViT Transformer Block with efficient attention and MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = EfficientAttention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class EfficientViTStage(nn.Module):\n",
    "    \"\"\"EfficientViT Stage with multiple transformer blocks\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, depth, num_heads, mlp_ratio, qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, depth)]\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            EfficientViTBlock(\n",
    "                dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                drop=drop, attn_drop=attn_drop, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                sr_ratio=sr_ratio)\n",
    "            for i in range(depth)])\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main EfficientViT Model Architecture\n",
    "\n",
    "class EfficientViTEmotionClassifier(nn.Module):\n",
    "    \"\"\"EfficientViT model for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=7, embed_dims=[64, 128, 192], \n",
    "                 depths=[2, 4, 6], num_heads=[2, 4, 6], mlp_ratios=[4, 4, 4], qkv_bias=True,\n",
    "                 drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.1, norm_layer=nn.LayerNorm,\n",
    "                 sr_ratios=[8, 4, 2], dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.embed_dims = embed_dims\n",
    "        \n",
    "        # Patch embeddings for different stages\n",
    "        self.patch_embed1 = PatchEmbed(\n",
    "            img_size=img_size, patch_size=4, in_chans=in_chans, embed_dim=embed_dims[0])\n",
    "        self.patch_embed2 = PatchEmbed(\n",
    "            img_size=img_size // 4, patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "        self.patch_embed3 = PatchEmbed(\n",
    "            img_size=img_size // 8, patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "\n",
    "        # Position embeddings\n",
    "        self.pos_embed1 = nn.Parameter(torch.zeros(1, self.patch_embed1.num_patches, embed_dims[0]))\n",
    "        self.pos_embed2 = nn.Parameter(torch.zeros(1, self.patch_embed2.num_patches, embed_dims[1]))\n",
    "        self.pos_embed3 = nn.Parameter(torch.zeros(1, self.patch_embed3.num_patches, embed_dims[2]))\n",
    "        \n",
    "        self.pos_drop1 = nn.Dropout(p=drop_rate)\n",
    "        self.pos_drop2 = nn.Dropout(p=drop_rate)\n",
    "        self.pos_drop3 = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # Stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "\n",
    "        # Stage 1\n",
    "        self.stage1 = EfficientViTStage(\n",
    "            dim=embed_dims[0], depth=depths[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n",
    "            qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "            drop_path=drop_path_rate, sr_ratio=sr_ratios[0])\n",
    "        cur += depths[0]\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = EfficientViTStage(\n",
    "            dim=embed_dims[1], depth=depths[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n",
    "            qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "            drop_path=drop_path_rate, sr_ratio=sr_ratios[1])\n",
    "        cur += depths[1]\n",
    "\n",
    "        # Stage 3\n",
    "        self.stage3 = EfficientViTStage(\n",
    "            dim=embed_dims[2], depth=depths[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n",
    "            qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "            drop_path=drop_path_rate, sr_ratio=sr_ratios[2])\n",
    "\n",
    "        # Norm layers\n",
    "        self.norm1 = norm_layer(embed_dims[0])\n",
    "        self.norm2 = norm_layer(embed_dims[1])\n",
    "        self.norm3 = norm_layer(embed_dims[2])\n",
    "\n",
    "        # Classification head with enhanced capacity\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dims[2]),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(embed_dims[2], 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if 'classifier' in name:\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                else:\n",
    "                    nn.init.trunc_normal_(m.weight, std=.02)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Initialize position embeddings\n",
    "        nn.init.trunc_normal_(self.pos_embed1, std=.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed2, std=.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed3, std=.02)\n",
    "\n",
    "    def freeze_patch_emb(self):\n",
    "        \"\"\"Freeze patch embeddings for feature extraction phase\"\"\"\n",
    "        for param in [self.pos_embed1, self.pos_embed2, self.pos_embed3]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for module in [self.patch_embed1, self.patch_embed2, self.patch_embed3]:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def freeze_stages(self, stages_to_freeze=[]):\n",
    "        \"\"\"Freeze specific stages\"\"\"\n",
    "        stage_modules = [self.stage1, self.stage2, self.stage3]\n",
    "        norm_modules = [self.norm1, self.norm2, self.norm3]\n",
    "        \n",
    "        for stage_idx in stages_to_freeze:\n",
    "            if stage_idx < len(stage_modules):\n",
    "                for param in stage_modules[stage_idx].parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in norm_modules[stage_idx].parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def unfreeze_last_stages(self, num_stages=1):\n",
    "        \"\"\"Unfreeze last stages for fine-tuning\"\"\"\n",
    "        stage_modules = [self.stage1, self.stage2, self.stage3]\n",
    "        norm_modules = [self.norm1, self.norm2, self.norm3]\n",
    "        \n",
    "        for i in range(max(0, len(stage_modules) - num_stages), len(stage_modules)):\n",
    "            for param in stage_modules[i].parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in norm_modules[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Stage 1\n",
    "        x, (H, W) = self.patch_embed1(x), (self.patch_embed1.grid_size[0], self.patch_embed1.grid_size[1])\n",
    "        x = x + self.pos_embed1\n",
    "        x = self.pos_drop1(x)\n",
    "        x = self.stage1(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Stage 2\n",
    "        x, (H, W) = self.patch_embed2(x), (self.patch_embed2.grid_size[0], self.patch_embed2.grid_size[1])\n",
    "        x = x + self.pos_embed2\n",
    "        x = self.pos_drop2(x)\n",
    "        x = self.stage2(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Stage 3\n",
    "        x, (H, W) = self.patch_embed3(x), (self.patch_embed3.grid_size[0], self.patch_embed3.grid_size[1])\n",
    "        x = x + self.pos_embed3\n",
    "        x = self.pos_drop3(x)\n",
    "        x = self.stage3(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)  # [B, embed_dim]\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee0a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EfficientViT Monitor and Data Loading Functions\n",
    "\n",
    "class EfficientViTMonitor:\n",
    "    \"\"\"Monitor for EfficientViT training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.peak_memory_mb = 0\n",
    "        self.initial_memory_mb = 0\n",
    "        self.phase1_time = 0\n",
    "        self.phase2_time = 0\n",
    "        self.process = psutil.Process()\n",
    "        self.training_phases = {'phase1': None, 'phase2': None}\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start monitoring\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_memory_mb = self._get_memory_usage()\n",
    "        self.peak_memory_mb = self.initial_memory_mb\n",
    "        print(f\"Starting EfficientViT training...\")\n",
    "        print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    def start_phase(self, phase_name):\n",
    "        \"\"\"Start a training phase\"\"\"\n",
    "        self.training_phases[phase_name] = time.time()\n",
    "        print(f\"Starting {phase_name} - EfficientViT\")\n",
    "        \n",
    "    def end_phase(self, phase_name):\n",
    "        \"\"\"End a training phase\"\"\"\n",
    "        if self.training_phases[phase_name] is not None:\n",
    "            phase_duration = time.time() - self.training_phases[phase_name]\n",
    "            if phase_name == 'phase1':\n",
    "                self.phase1_time = phase_duration\n",
    "            elif phase_name == 'phase2':\n",
    "                self.phase2_time = phase_duration\n",
    "            print(f\"{phase_name} completed in: {timedelta(seconds=int(phase_duration))}\")\n",
    "            return phase_duration\n",
    "        return 0\n",
    "        \n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "        \n",
    "    def update_peak_memory(self):\n",
    "        \"\"\"Update peak memory if necessary\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        if current_memory > self.peak_memory_mb:\n",
    "            self.peak_memory_mb = current_memory\n",
    "            \n",
    "    def get_efficiency_metrics(self):\n",
    "        \"\"\"Calculate efficiency metrics\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        total_time = self.phase1_time + self.phase2_time\n",
    "        \n",
    "        return {\n",
    "            'memory_efficiency': self.initial_memory_mb / self.peak_memory_mb if self.peak_memory_mb > 0 else 0,\n",
    "            'time_efficiency': total_time / 3600,  # Hours\n",
    "            'peak_memory_gb': self.peak_memory_mb / 1024,\n",
    "            'memory_growth_factor': self.peak_memory_mb / self.initial_memory_mb if self.initial_memory_mb > 0 else 1,\n",
    "            'phase1_ratio': self.phase1_time / total_time if total_time > 0 else 0,\n",
    "            'phase2_ratio': self.phase2_time / total_time if total_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "    def end_monitoring(self):\n",
    "        \"\"\"End monitoring and return statistics\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        total_time_seconds = self.end_time - self.start_time\n",
    "        total_time_formatted = str(timedelta(seconds=int(total_time_seconds)))\n",
    "        \n",
    "        final_memory_mb = self._get_memory_usage()\n",
    "        memory_increase = final_memory_mb - self.initial_memory_mb\n",
    "        \n",
    "        efficiency_metrics = self.get_efficiency_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EFFICIENTVIT MONITORING REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total training time: {total_time_formatted}\")\n",
    "        print(f\"  • Phase 1 (Feature extraction): {timedelta(seconds=int(self.phase1_time))}\")\n",
    "        print(f\"  • Phase 2 (Fine-tuning): {timedelta(seconds=int(self.phase2_time))}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Final memory: {final_memory_mb:.2f} MB\")\n",
    "        print(f\"Peak memory: {self.peak_memory_mb:.2f} MB\")\n",
    "        print(f\"Memory efficiency: {efficiency_metrics['memory_efficiency']:.3f}\")\n",
    "        print(f\"Growth factor: {efficiency_metrics['memory_growth_factor']:.2f}x\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'total_time_seconds': total_time_seconds,\n",
    "            'total_time_formatted': total_time_formatted,\n",
    "            'initial_memory_mb': self.initial_memory_mb,\n",
    "            'final_memory_mb': final_memory_mb,\n",
    "            'peak_memory_mb': self.peak_memory_mb,\n",
    "            'memory_increase_mb': memory_increase,\n",
    "            'phase1_time': self.phase1_time,\n",
    "            'phase2_time': self.phase2_time,\n",
    "            'efficiency_metrics': efficiency_metrics\n",
    "        }\n",
    "\n",
    "def load_preprocessed_data_efficientvit_from_images():\n",
    "    \"\"\"Load preprocessed data from images for EfficientViT\"\"\"\n",
    "    print(\"Loading preprocessed JPG data for EfficientViT...\")\n",
    "    \n",
    "    BASE_PATH = r\"../data/augmented/raf_db_balanced\"\n",
    "    \n",
    "    def load_images_from_directory(directory_path, set_name):\n",
    "        \"\"\"Load images from directory\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(f\"Loading {set_name} from: {directory_path}\")\n",
    "        \n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"❌ Directory not found: {directory_path}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        subdirs = [d for d in os.listdir(directory_path) \n",
    "                  if os.path.isdir(os.path.join(directory_path, d))]\n",
    "        \n",
    "        print(f\"📁 Subdirectories found: {subdirs}\")\n",
    "        \n",
    "        for emotion, label in EMOTION_LABELS.items():\n",
    "            emotion_path = os.path.join(directory_path, emotion)\n",
    "            \n",
    "            if not os.path.exists(emotion_path):\n",
    "                print(f\"⚠️ Folder '{emotion}' not found in {directory_path}\")\n",
    "                continue\n",
    "            \n",
    "            count = 0\n",
    "            image_files = [f for f in os.listdir(emotion_path) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            for image_file in image_files:\n",
    "                try:\n",
    "                    image_path = os.path.join(emotion_path, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "                    \n",
    "                    if image is not None:\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                        images.append(image)\n",
    "                        labels.append(label)\n",
    "                        count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {image_path}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  • {emotion}: {count} images loaded\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    # Load training data\n",
    "    train_path = os.path.join(BASE_PATH, \"train\")\n",
    "    X_train, y_train = load_images_from_directory(train_path, \"training\")\n",
    "    \n",
    "    # Load test data  \n",
    "    test_path = os.path.join(BASE_PATH, \"test\")\n",
    "    X_test, y_test = load_images_from_directory(test_path, \"test\")\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"❌ Failed to load data. Check directory structure.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(f\"\\n✅ Data loaded successfully:\")\n",
    "    print(f\"  • Training: {X_train.shape}\")\n",
    "    print(f\"  • Test: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9e4fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Transforms and Training Functions\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Get transforms for training and validation with ViT-specific augmentations\"\"\"\n",
    "    # Training transforms with augmentation optimized for ViT\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAutocontrast(p=0.3),\n",
    "        transforms.RandomEqualize(p=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "        transforms.RandomErasing(p=0.1, scale=(0.02, 0.2))  # Random erasing for better generalization\n",
    "    ])\n",
    "    \n",
    "    # Validation/test transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Create PyTorch data loaders\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = EmotionDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = EmotionDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders with appropriate workers\n",
    "    num_workers = min(4, os.cpu_count() or 1)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                             num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch with gradient clipping for stability\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a58c4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Two-Phase Training for EfficientViT\n",
    "\n",
    "def train_efficientvit_two_phase(model, train_loader, val_loader, monitor):\n",
    "    \"\"\"Train EfficientViT in two phases\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING EFFICIENTVIT 2-PHASE TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Feature extraction with frozen backbone\n",
    "    print(\"PHASE 1: FEATURE EXTRACTION\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase1')\n",
    "    \n",
    "    # Freeze patch embeddings and most stages, only train classifier\n",
    "    model.freeze_patch_emb()\n",
    "    model.freeze_stages([0, 1])  # Freeze first two stages\n",
    "    \n",
    "    # Setup optimizer and criterion for phase 1\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing for better generalization\n",
    "    \n",
    "    # Only optimize unfrozen parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(trainable_params, lr=EFFICIENTVIT_CONFIG['base_learning_rate'], \n",
    "                           weight_decay=EFFICIENTVIT_CONFIG['weight_decay'], betas=(0.9, 0.999))\n",
    "    \n",
    "    # Cosine annealing scheduler\n",
    "    phase1_epochs = 25\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=phase1_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_acc_phase1 = 0\n",
    "    patience_counter = 0\n",
    "    patience = 8\n",
    "    \n",
    "    history_phase1 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"Training {len(trainable_params)} parameters in Phase 1\")\n",
    "    \n",
    "    for epoch in range(phase1_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history_phase1['train_loss'].append(train_loss)\n",
    "        history_phase1['train_acc'].append(train_acc)\n",
    "        history_phase1['val_loss'].append(val_loss)\n",
    "        history_phase1['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase1:\n",
    "            best_val_acc_phase1 = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save phase 1 best model\n",
    "            torch.save(model.state_dict(), 'best_efficientvit_phase1.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            print(f'Phase 1 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    phase1_duration = monitor.end_phase('phase1')\n",
    "    print(f\"Phase 1 - Best val_accuracy: {best_val_acc_phase1:.4f}\")\n",
    "    \n",
    "    # Load best phase 1 model\n",
    "    model.load_state_dict(torch.load('best_efficientvit_phase1.pth'))\n",
    "    \n",
    "    # Phase 2: Fine-tuning\n",
    "    print(\"\\nPHASE 2: FINE-TUNING\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase2')\n",
    "    \n",
    "    # Unfreeze more layers for fine-tuning\n",
    "    model.unfreeze_last_stages(2)  # Unfreeze last 2 stages\n",
    "    \n",
    "    # Setup optimizer for phase 2 with lower learning rate and different parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(trainable_params, lr=EFFICIENTVIT_CONFIG['fine_tune_learning_rate'], \n",
    "                           weight_decay=EFFICIENTVIT_CONFIG['weight_decay'], betas=(0.9, 0.999))\n",
    "    \n",
    "    # Scheduler with warm restarts\n",
    "    phase2_epochs = EPOCHS - phase1_epochs if phase1_epochs < EPOCHS else 35\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-7)\n",
    "    \n",
    "    # Training phase 2\n",
    "    best_val_acc_phase2 = 0\n",
    "    patience_counter = 0\n",
    "    patience = 12\n",
    "    \n",
    "    history_phase2 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"Training {len(trainable_params)} parameters in Phase 2\")\n",
    "    \n",
    "    for epoch in range(phase2_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history_phase2['train_loss'].append(train_loss)\n",
    "        history_phase2['train_acc'].append(train_acc)\n",
    "        history_phase2['val_loss'].append(val_loss)\n",
    "        history_phase2['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase2:\n",
    "            best_val_acc_phase2 = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_efficientvit_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            print(f'Phase 2 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    phase2_duration = monitor.end_phase('phase2')\n",
    "    print(f\"Phase 2 - Best val_accuracy: {best_val_acc_phase2:.4f}\")\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_history = {\n",
    "        'train_loss': history_phase1['train_loss'] + history_phase2['train_loss'],\n",
    "        'train_acc': history_phase1['train_acc'] + history_phase2['train_acc'],\n",
    "        'val_loss': history_phase1['val_loss'] + history_phase2['val_loss'],\n",
    "        'val_acc': history_phase1['val_acc'] + history_phase2['val_acc'],\n",
    "        'phase1_epochs': len(history_phase1['train_loss']),\n",
    "        'phase2_epochs': len(history_phase2['train_loss']),\n",
    "        'phase1_duration': phase1_duration,\n",
    "        'phase2_duration': phase2_duration,\n",
    "        'best_val_acc_phase1': best_val_acc_phase1,\n",
    "        'best_val_acc_phase2': best_val_acc_phase2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTRAINING COMPLETE:\")\n",
    "    print(f\"  • Total epochs: {combined_history['phase1_epochs'] + combined_history['phase2_epochs']}\")\n",
    "    print(f\"  • Best final accuracy: {max(combined_history['val_acc']):.4f}\")\n",
    "    print(f\"  • Total time: {timedelta(seconds=int(phase1_duration + phase2_duration))}\")\n",
    "    \n",
    "    return combined_history\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set with detailed metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            output = model(data)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    class_report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=emotion_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return accuracy, f1, precision, recall, conf_matrix, class_report, np.array(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d788c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 Detailed Analysis Functions\n",
    "\n",
    "def print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names):\n",
    "    \"\"\"Detailed textual analysis of imbalance impact\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED ANALYSIS - IMBALANCED DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Imbalance statistics\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    print(\"CLASS DISTRIBUTION:\")\n",
    "    total_samples = len(y_true)\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        count = class_distribution.get(i, 0)\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  • {emotion}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Imbalance analysis\n",
    "    max_samples = max(counts)\n",
    "    min_samples = min(counts)\n",
    "    imbalance_ratio = max_samples / min_samples\n",
    "    class_std = np.std(counts)\n",
    "    class_cv = class_std / np.mean(counts)\n",
    "    \n",
    "    print(f\"\\nIMBALANCE STATISTICS:\")\n",
    "    print(f\"  • Maximum/minimum ratio: {imbalance_ratio:.2f}x\")\n",
    "    print(f\"  • Standard deviation: {class_std:.1f} samples\")\n",
    "    print(f\"  • Coefficient of variation: {class_cv:.3f}\")\n",
    "    print(f\"  • Majority class: {emotion_names[np.argmax(counts)]} ({max_samples} samples)\")\n",
    "    print(f\"  • Minority class: {emotion_names[np.argmin(counts)]} ({min_samples} samples)\")\n",
    "    \n",
    "    # Performance impact\n",
    "    print(f\"\\nPERFORMANCE IMPACT:\")\n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Correlation between quantity and performance\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    print(f\"  • Correlation samples-F1: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation > 0.5:\n",
    "        print(f\"    → STRONG positive correlation: more samples = better F1\")\n",
    "    elif correlation > 0.3:\n",
    "        print(f\"    → MODERATE positive correlation\")\n",
    "    elif correlation > -0.3:\n",
    "        print(f\"    → WEAK correlation: model relatively robust\")\n",
    "    else:\n",
    "        print(f\"    → Negative correlation: possible overfitting on majority classes\")\n",
    "    \n",
    "    # Best/worst performing classes\n",
    "    best_class_idx = np.argmax(f1_scores)\n",
    "    worst_class_idx = np.argmin(f1_scores)\n",
    "    \n",
    "    print(f\"  • Best F1: {emotion_names[best_class_idx]} ({f1_scores[best_class_idx]:.3f}) - {support_counts[best_class_idx]} samples\")\n",
    "    print(f\"  • Worst F1: {emotion_names[worst_class_idx]} ({f1_scores[worst_class_idx]:.3f}) - {support_counts[worst_class_idx]} samples\")\n",
    "    \n",
    "    # Confusion analysis\n",
    "    print(f\"\\nMAJOR CONFUSIONS:\")\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    # Find biggest errors (off-diagonal)\n",
    "    np.fill_diagonal(conf_matrix_norm, 0)\n",
    "    max_confusion_idx = np.unravel_index(np.argmax(conf_matrix_norm), conf_matrix_norm.shape)\n",
    "    max_confusion_value = conf_matrix_norm[max_confusion_idx]\n",
    "    \n",
    "    print(f\"  • {emotion_names[max_confusion_idx[0]]} → {emotion_names[max_confusion_idx[1]]}: {max_confusion_value:.3f}\")\n",
    "    \n",
    "    # Top 3 confusions\n",
    "    flat_indices = np.argsort(conf_matrix_norm.flatten())[-3:][::-1]\n",
    "    for idx in flat_indices:\n",
    "        i, j = np.unravel_index(idx, conf_matrix_norm.shape)\n",
    "        if i != j:  # Ignore diagonal\n",
    "            print(f\"  • {emotion_names[i]} → {emotion_names[j]}: {conf_matrix_norm[i, j]:.3f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nRECOMMENDATIONS:\")\n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"  • SEVERE imbalance - consider balancing techniques\")\n",
    "        print(\"  • Suggestions: SMOTE, class weighting, focal loss\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(\"  • MODERATE imbalance - monitor per-class metrics\")\n",
    "        print(\"  • Suggestions: class weighting, stratified sampling\")\n",
    "    else:\n",
    "        print(\"  • MILD imbalance - model can handle adequately\")\n",
    "    \n",
    "    if correlation < 0.3:\n",
    "        print(\"  • Model shows robustness to imbalance\")\n",
    "        print(\"  • Vision Transformer attention mechanism helps\")\n",
    "    else:\n",
    "        print(\"  • Consider strategies for minority classes\")\n",
    "        print(\"  • Data augmentation for underrepresented emotions\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c965dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 Comprehensive Visualizations for EfficientViT\n",
    "\n",
    "def create_comprehensive_visualizations_efficientvit_pytorch(history, conf_matrix, metrics, class_report, experiment_id, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations for EfficientViT results with focus on imbalanced data\n",
    "    \n",
    "    Args:\n",
    "        history: Training history\n",
    "        conf_matrix: Confusion matrix\n",
    "        metrics: Model metrics\n",
    "        class_report: Classification report\n",
    "        experiment_id: Experiment ID\n",
    "        y_true: True labels\n",
    "        y_pred: Model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Figure configuration\n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    \n",
    "    # 1. TRAINING HISTORY IN 2 PHASES\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    epochs_phase1 = range(1, history['phase1_epochs'] + 1)\n",
    "    epochs_phase2 = range(history['phase1_epochs'] + 1, \n",
    "                         history['phase1_epochs'] + history['phase2_epochs'] + 1)\n",
    "    \n",
    "    # Plot accuracy by phase\n",
    "    plt.plot(epochs_phase1, history['train_acc'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Phase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_acc'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Phase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_acc'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Phase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_acc'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Phase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7, \n",
    "                label='Phase Transition')\n",
    "    plt.title('EfficientViT: Accuracy - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. LOSS IN 2 PHASES\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    plt.plot(epochs_phase1, history['train_loss'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Phase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_loss'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Phase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_loss'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Phase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_loss'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Phase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7)\n",
    "    plt.title('EfficientViT: Loss - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. RAW CONFUSION MATRIX (SHOWS IMBALANCE)\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    \n",
    "    # Raw matrix shows imbalance\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax3,\n",
    "                cbar_kws={'label': 'Number of Samples'})\n",
    "    plt.title('Raw Confusion Matrix\\n(Shows Imbalance)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 4. NORMALIZED CONFUSION MATRIX (BY ROW)\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    \n",
    "    # Row normalization (recall per class)\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.3f', cmap='Greens',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax4,\n",
    "                cbar_kws={'label': 'Proportion (Recall)'})\n",
    "    plt.title('Normalized Confusion Matrix\\n(Recall per Class)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 5. CLASS DISTRIBUTION (IMBALANCE)\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    \n",
    "    # Count samples per class in test set\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    # Sort by quantity\n",
    "    sorted_classes = sorted(class_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "    class_names_sorted = [emotion_names[i] for i, count in sorted_classes]\n",
    "    class_counts_sorted = [count for i, count in sorted_classes]\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names_sorted)))\n",
    "    bars = plt.bar(class_names_sorted, class_counts_sorted, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('Class Distribution - Test\\n(Imbalanced Data)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, count in zip(bars, class_counts_sorted):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_samples = np.mean(class_counts_sorted)\n",
    "    plt.axhline(y=mean_samples, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Mean: {mean_samples:.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 6. F1-SCORE PER EMOTION WITH IMBALANCE ANALYSIS\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    \n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Create colored bar chart by sample count\n",
    "    colors = plt.cm.viridis(np.array(support_counts) / max(support_counts))\n",
    "    bars = plt.bar(emotion_names, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('F1-Score per Emotion\\n(Color = Sample Count)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add values and support\n",
    "    for bar, score, support in zip(bars, f1_scores, support_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}\\n(n={support})', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Colorbar to indicate sample count\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, \n",
    "                               norm=plt.Normalize(vmin=min(support_counts), vmax=max(support_counts)))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=ax6, label='Test Samples')\n",
    "    \n",
    "    # 7. PRECISION, RECALL, F1 PER CLASS\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    \n",
    "    precision_scores = [class_report[emotion]['precision'] for emotion in emotion_names]\n",
    "    recall_scores = [class_report[emotion]['recall'] for emotion in emotion_names]\n",
    "    \n",
    "    x = np.arange(len(emotion_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x, recall_scores, width, label='Recall', alpha=0.8, color='lightblue')\n",
    "    bars3 = plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    plt.title('Detailed Metrics per Class', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.xticks(x, emotion_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 8. IMBALANCE vs PERFORMANCE CORRELATION\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    \n",
    "    # Scatter plot: Sample count vs F1-Score\n",
    "    plt.scatter(support_counts, f1_scores, c=support_counts, cmap='viridis', \n",
    "                s=100, alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        plt.annotate(emotion, (support_counts[i], f1_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Trend line\n",
    "    z = np.polyfit(support_counts, f1_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(support_counts, p(support_counts), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title('Samples vs Performance\\n(Correlation)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Number of Test Samples')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "             transform=ax8.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 9. ERROR HEATMAP PER CLASS\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    \n",
    "    # Error matrix (off-diagonal elements)\n",
    "    error_matrix = conf_matrix.copy()\n",
    "    np.fill_diagonal(error_matrix, 0)  # Remove diagonal (correct predictions)\n",
    "    \n",
    "    # Normalize by row to show proportion of errors\n",
    "    error_matrix_norm = error_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    error_matrix_norm = np.nan_to_num(error_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(error_matrix_norm, annot=True, fmt='.3f', cmap='Reds',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax9,\n",
    "                cbar_kws={'label': 'Error Proportion'})\n",
    "    plt.title('Error Heatmap\\n(Class Confusions)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class (Error)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 10. COMPARATIVE METRICS: WEIGHTED vs MACRO\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    \n",
    "    # Calculate weighted and macro metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics_comparison = {\n",
    "        'Precision': [precision_macro, precision_weighted],\n",
    "        'Recall': [recall_macro, recall_weighted],\n",
    "        'F1-Score': [f1_macro, f1_weighted]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(metrics_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    macro_values = [metrics_comparison[metric][0] for metric in metrics_comparison]\n",
    "    weighted_values = [metrics_comparison[metric][1] for metric in metrics_comparison]\n",
    "    \n",
    "    bars1 = plt.bar(x - width/2, macro_values, width, label='Macro (Unbalanced)', \n",
    "                   alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x + width/2, weighted_values, width, label='Weighted (Balanced)', \n",
    "                   alpha=0.8, color='lightblue')\n",
    "    \n",
    "    plt.title('Macro vs Weighted Metrics\\n(Imbalance Impact)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.xticks(x, metrics_comparison.keys())\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 11. COMPUTATIONAL RESOURCES AND EFFICIENCY\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    \n",
    "    resource_data = {\n",
    "        'Time (min)': metrics['training_time_seconds'] / 60,\n",
    "        'Memory (GB)': metrics['peak_memory_mb'] / 1024,\n",
    "        'Parameters (M)': metrics['total_parameters'] / 1_000_000,\n",
    "        'Efficiency\\n(Acc/M_params)': metrics['test_accuracy'] / (metrics['total_parameters'] / 1_000_000)\n",
    "    }\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    bars = plt.bar(range(len(resource_data)), list(resource_data.values()), \n",
    "                  color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.title('Computational Resources\\nEfficientViT', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(range(len(resource_data)), resource_data.keys(), rotation=45)\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Add values on bars\n",
    "    for bar, (key, value) in zip(bars, resource_data.items()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.02,\n",
    "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 12. SCIENTIFIC SUMMARY\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    # Calculate imbalance statistics\n",
    "    imbalance_ratio = max(support_counts) / min(support_counts)\n",
    "    class_std = np.std(support_counts)\n",
    "    class_cv = class_std / np.mean(support_counts)  # Coefficient of variation\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "EFFICIENTVIT PYTORCH\n",
    "SCIENTIFIC ANALYSIS\n",
    "\n",
    "PERFORMANCE:\n",
    "• Accuracy: {metrics['test_accuracy']:.4f}\n",
    "• F1-Macro: {f1_macro:.4f}\n",
    "• F1-Weighted: {f1_weighted:.4f}\n",
    "\n",
    "IMBALANCE:\n",
    "• Ratio max/min: {imbalance_ratio:.1f}x\n",
    "• Coef. Variation: {class_cv:.3f}\n",
    "• Correlation samples-F1: {correlation:.3f}\n",
    "\n",
    "EFFICIENCY:\n",
    "• Parameters: {metrics['total_parameters']/1_000_000:.1f}M\n",
    "• Time: {metrics['training_time_seconds']/60:.1f} min\n",
    "• Memory: {metrics['peak_memory_mb']/1024:.2f} GB\n",
    "\n",
    "VISION TRANSFORMER:\n",
    "• Multi-head attention layers\n",
    "• Patch-based processing\n",
    "• Hierarchical feature learning\n",
    "\n",
    "CONCLUSION:\n",
    "Model {'robust' if f1_macro > 0.7 else 'limited'} for imbalanced\n",
    "data. {'Good' if correlation > 0.3 else 'Low'} correlation\n",
    "between sample count and performance.\n",
    "Attention mechanism handles spatial\n",
    "relationships effectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.05, 0.95, summary_text, fontsize=10, verticalalignment='top',\n",
    "             transform=ax12.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs('plots/efficientvit', exist_ok=True)\n",
    "    plt.savefig(f'plots/efficientvit/efficientvit_comprehensive_analysis_pytorch_{experiment_id}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === DETAILED TEXTUAL ANALYSIS ===\n",
    "    print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd052c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EfficientViT training...\n",
      "Start time: 2025-09-18 16:41:42\n",
      "Initial memory: 3423.79 MB\n",
      "Device: cuda\n",
      "--------------------------------------------------\n",
      "Experiment ID: efficientvit_pytorch_emotion_20250918_164142\n",
      "Loading data...\n",
      "Loading preprocessed JPG data for EfficientViT...\n",
      "Loading training from: ../data/augmented/raf_db_balanced/train\n",
      "📁 Subdirectories found: ['Felicidade', 'Surpresa', 'Raiva', 'Tristeza', 'Medo', 'Nojo', 'Neutro']\n",
      "  • Raiva: 1000 images loaded\n",
      "  • Nojo: 1000 images loaded\n",
      "  • Medo: 1000 images loaded\n",
      "  • Felicidade: 1000 images loaded\n",
      "  • Neutro: 1000 images loaded\n",
      "  • Tristeza: 1000 images loaded\n",
      "  • Surpresa: 1000 images loaded\n",
      "Loading test from: ../data/augmented/raf_db_balanced/test\n",
      "📁 Subdirectories found: ['Felicidade', 'Surpresa', 'Raiva', 'Tristeza', 'Medo', 'Nojo', 'Neutro']\n",
      "  • Raiva: 91 images loaded\n",
      "  • Nojo: 110 images loaded\n",
      "  • Medo: 37 images loaded\n",
      "  • Felicidade: 487 images loaded\n",
      "  • Neutro: 409 images loaded\n",
      "  • Tristeza: 226 images loaded\n",
      "  • Surpresa: 166 images loaded\n",
      "\n",
      "✅ Data loaded successfully:\n",
      "  • Training: (7000, 224, 224, 3)\n",
      "  • Test: (1526, 224, 224, 3)\n",
      "Data split:\n",
      "  • Train: (4900, 224, 224, 3)\n",
      "  • Validation: (2100, 224, 224, 3)\n",
      "  • Test: (1526, 224, 224, 3)\n",
      "\n",
      "Data distribution analysis:\n",
      "Training set:\n",
      "  • Raiva: 700 samples\n",
      "  • Nojo: 700 samples\n",
      "  • Medo: 700 samples\n",
      "  • Felicidade: 700 samples\n",
      "  • Neutro: 700 samples\n",
      "  • Tristeza: 700 samples\n",
      "  • Surpresa: 700 samples\n",
      "Test set:\n",
      "  • Raiva: 91 samples\n",
      "  • Nojo: 110 samples\n",
      "  • Medo: 37 samples\n",
      "  • Felicidade: 487 samples\n",
      "  • Neutro: 409 samples\n",
      "  • Tristeza: 226 samples\n",
      "  • Surpresa: 166 samples\n",
      "Test set imbalance ratio: 13.2:1\n",
      "\n",
      "Creating EfficientViT model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linspace() received an invalid combination of arguments - got (int, list, int), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Create EfficientViT model\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCreating EfficientViT model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m model = \u001b[43mEfficientViTEmotionClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43membed_dims\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdepths\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_heads\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdrop_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattn_drop_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdrop_path_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEFFICIENTVIT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdropout_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m model = model.to(device)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Print model info\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mEfficientViTEmotionClassifier.__init__\u001b[39m\u001b[34m(self, img_size, patch_size, in_chans, num_classes, embed_dims, depths, num_heads, mlp_ratios, qkv_bias, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, sr_ratios, dropout_rate)\u001b[39m\n\u001b[32m     35\u001b[39m cur = \u001b[32m0\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Stage 1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28mself\u001b[39m.stage1 = \u001b[43mEfficientViTStage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlp_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m cur += depths[\u001b[32m0\u001b[39m]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Stage 2\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mEfficientViTStage.__init__\u001b[39m\u001b[34m(self, dim, depth, num_heads, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, norm_layer, sr_ratio)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# stochastic depth decay rule\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m dpr = [x.item() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks = nn.ModuleList([\n\u001b[32m     80\u001b[39m     EfficientViTBlock(\n\u001b[32m     81\u001b[39m         dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n\u001b[32m     82\u001b[39m         drop=drop, attn_drop=attn_drop, drop_path=dpr[i], norm_layer=norm_layer,\n\u001b[32m     83\u001b[39m         sr_ratio=sr_ratio)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)])\n",
      "\u001b[31mTypeError\u001b[39m: linspace() received an invalid combination of arguments - got (int, list, int), but expected one of:\n * (Tensor start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Tensor end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Tensor start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (Number start, Number end, int steps, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Main Execution Script for EfficientViT\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize monitor\n",
    "    monitor = EfficientViTMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Generate experiment ID\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_id = f\"efficientvit_pytorch_emotion_{timestamp}\"\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('plots/efficientvit', exist_ok=True)\n",
    "    os.makedirs('models/efficientvit', exist_ok=True)\n",
    "    os.makedirs('metrics/efficientvit', exist_ok=True)\n",
    "    \n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_test, y_test = load_preprocessed_data_efficientvit_from_images()\n",
    "    \n",
    "    if X_train is not None:\n",
    "        # Split training data into train and validation\n",
    "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            stratify=y_train,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split:\")\n",
    "        print(f\"  • Train: {X_train_split.shape}\")\n",
    "        print(f\"  • Validation: {X_val.shape}\")\n",
    "        print(f\"  • Test: {X_test.shape}\")\n",
    "        \n",
    "        # Analyze data distribution\n",
    "        print(f\"\\nData distribution analysis:\")\n",
    "        unique_train, counts_train = np.unique(y_train_split, return_counts=True)\n",
    "        unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "        \n",
    "        emotion_names = list(EMOTION_LABELS.keys())\n",
    "        print(\"Training set:\")\n",
    "        for i, count in enumerate(counts_train):\n",
    "            print(f\"  • {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        print(\"Test set:\")\n",
    "        for i, count in enumerate(counts_test):\n",
    "            print(f\"  • {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        test_imbalance_ratio = max(counts_test) / min(counts_test)\n",
    "        print(f\"Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            X_train_split, y_train_split, X_val, y_val, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Create EfficientViT model\n",
    "        print(\"\\nCreating EfficientViT model...\")\n",
    "        model = EfficientViTEmotionClassifier(\n",
    "            img_size=IMG_SIZE,\n",
    "            num_classes=7,\n",
    "            embed_dims=EFFICIENTVIT_CONFIG.get('embed_dims', [64, 128, 192]),\n",
    "            depths=EFFICIENTVIT_CONFIG.get('depths', [2, 4, 6]),\n",
    "            num_heads=EFFICIENTVIT_CONFIG.get('num_heads', [2, 4, 6]),\n",
    "            drop_rate=EFFICIENTVIT_CONFIG['drop_rate'],\n",
    "            attn_drop_rate=EFFICIENTVIT_CONFIG['attn_drop_rate'],\n",
    "            drop_path_rate=EFFICIENTVIT_CONFIG['drop_path_rate'],\n",
    "            dropout_rate=EFFICIENTVIT_CONFIG['dropout_rate']\n",
    "        )\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"EfficientViT created successfully:\")\n",
    "        print(f\"  • Total parameters: {total_params:,}\")\n",
    "        print(f\"  • Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  • Efficiency: {total_params/1000000:.1f}M parameters\")\n",
    "        print(f\"  • Trainable ratio: {(trainable_params/total_params)*100:.1f}%\")\n",
    "        \n",
    "        # Update monitor with GPU memory if available\n",
    "        if torch.cuda.is_available():\n",
    "            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            print(f\"  • Initial GPU memory: {initial_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING EFFICIENTVIT TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        history = train_efficientvit_two_phase(model, train_loader, val_loader, monitor)\n",
    "        \n",
    "        # Load best model for evaluation\n",
    "        if os.path.exists('best_efficientvit_model.pth'):\n",
    "            model.load_state_dict(torch.load('best_efficientvit_model.pth'))\n",
    "            print(\"Loaded best model weights for evaluation\")\n",
    "        \n",
    "        # Update memory tracking\n",
    "        monitor.update_peak_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "            print(f\"Peak GPU memory: {peak_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Comprehensive evaluation\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Detailed evaluation with inference timing\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "        \n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        inference_times = []\n",
    "        \n",
    "        print(\"Measuring inference performance...\")\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "                start_time = time.time()\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                output = model(data)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                inference_time = time.time() - start_time\n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "                _, predicted = output.max(1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Weighted metrics for imbalanced data\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "        class_report = classification_report(\n",
    "            all_targets, all_preds,\n",
    "            target_names=emotion_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Compile comprehensive metrics\n",
    "        avg_inference_time = np.mean(inference_times)\n",
    "        total_inference_time = sum(inference_times)\n",
    "        samples_per_second = len(all_targets) / total_inference_time\n",
    "        \n",
    "        metrics = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'test_accuracy': accuracy,\n",
    "            'f1_score_macro': f1,\n",
    "            'f1_score_weighted': f1_weighted,\n",
    "            'precision_macro': precision,\n",
    "            'precision_weighted': precision_weighted,\n",
    "            'recall_macro': recall,\n",
    "            'recall_weighted': recall_weighted,\n",
    "            'training_time_seconds': history['phase1_duration'] + history['phase2_duration'],\n",
    "            'phase1_duration': history['phase1_duration'],\n",
    "            'phase2_duration': history['phase2_duration'],\n",
    "            'peak_memory_mb': monitor.peak_memory_mb,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'avg_inference_time_seconds': avg_inference_time,\n",
    "            'samples_per_second': samples_per_second,\n",
    "            'test_samples': len(all_targets),\n",
    "            'imbalance_ratio': test_imbalance_ratio,\n",
    "            'best_val_acc_phase1': history['best_val_acc_phase1'],\n",
    "            'best_val_acc_phase2': history['best_val_acc_phase2'],\n",
    "            'model_architecture': 'EfficientViT',\n",
    "            'embed_dims': EFFICIENTVIT_CONFIG.get('embed_dims', [64, 128, 192]),\n",
    "            'depths': EFFICIENTVIT_CONFIG.get('depths', [2, 4, 6]),\n",
    "            'num_heads': EFFICIENTVIT_CONFIG.get('num_heads', [2, 4, 6])\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nCOMPREHENSIVE RESULTS:\")\n",
    "        print(f\"  • Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"  • F1-Score (Macro): {f1:.4f}\")\n",
    "        print(f\"  • F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "        print(f\"  • Precision (Macro): {precision:.4f}\")\n",
    "        print(f\"  • Recall (Macro): {recall:.4f}\")\n",
    "        print(f\"  • Inference Speed: {samples_per_second:.1f} samples/second\")\n",
    "        print(f\"  • Total Training Time: {timedelta(seconds=int(metrics['training_time_seconds']))}\")\n",
    "        \n",
    "        # Create comprehensive visualizations with imbalance analysis\n",
    "        print(\"\\nGenerating comprehensive visualizations...\")\n",
    "        create_comprehensive_visualizations_efficientvit_pytorch(\n",
    "            history, conf_matrix, metrics, class_report, experiment_id, all_targets, all_preds\n",
    "        )\n",
    "        \n",
    "        # Save model if performance is good\n",
    "        performance_threshold = 0.70\n",
    "        if accuracy >= performance_threshold:\n",
    "            model_path = f'models/efficientvit/efficientvit_model_{experiment_id}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'history': history,\n",
    "                'model_config': {\n",
    "                    'num_classes': 7,\n",
    "                    'img_size': IMG_SIZE,\n",
    "                    'embed_dims': EFFICIENTVIT_CONFIG.get('embed_dims', [64, 128, 192]),\n",
    "                    'depths': EFFICIENTVIT_CONFIG.get('depths', [2, 4, 6]),\n",
    "                    'num_heads': EFFICIENTVIT_CONFIG.get('num_heads', [2, 4, 6]),\n",
    "                    'architecture': 'EfficientViT'\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"Model saved: {model_path}\")\n",
    "        else:\n",
    "            print(f\"Model not saved (accuracy {accuracy:.4f} < {performance_threshold})\")\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_csv = f'metrics/efficientvit/efficientvit_metrics_{experiment_id}.csv'\n",
    "        metrics_df.to_csv(metrics_csv, index=False)\n",
    "        print(f\"Metrics saved: {metrics_csv}\")\n",
    "        \n",
    "        # End monitoring\n",
    "        monitor_stats = monitor.end_monitoring()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Experiment ID: {experiment_id}\")\n",
    "        print(f\"Files generated:\")\n",
    "        print(f\"  • Visualizations: plots/efficientvit/efficientvit_comprehensive_analysis_pytorch_{experiment_id}.png\")\n",
    "        print(f\"  • Metrics: {metrics_csv}\")\n",
    "        if accuracy >= performance_threshold:\n",
    "            print(f\"  • Model: models/efficientvit/efficientvit_model_{experiment_id}.pth\")\n",
    "        print(f\"  • Best model: best_efficientvit_model.pth\")\n",
    "        \n",
    "        # Final summary for imbalanced data\n",
    "        print(f\"\\nIMBALANCE ANALYSIS SUMMARY:\")\n",
    "        print(f\"  • Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        print(f\"  • Macro F1 (unweighted): {f1:.4f}\")\n",
    "        print(f\"  • Weighted F1 (balanced): {f1_weighted:.4f}\")\n",
    "        print(f\"  • Performance difference: {abs(f1_weighted - f1):.4f}\")\n",
    "        \n",
    "        if f1_weighted - f1 > 0.05:\n",
    "            print(\"  → Model benefits significantly from class weighting\")\n",
    "        elif abs(f1_weighted - f1) < 0.02:\n",
    "            print(\"  → Model is robust to class imbalance\")\n",
    "        else:\n",
    "            print(\"  → Moderate impact of class imbalance\")\n",
    "        \n",
    "        print(f\"\\nVISION TRANSFORMER ADVANTAGES:\")\n",
    "        print(f\"  • Multi-head attention captures spatial relationships\")\n",
    "        print(f\"  • Hierarchical feature learning with {len(EFFICIENTVIT_CONFIG.get('embed_dims', [64, 128, 192]))} stages\")\n",
    "        print(f\"  • Efficient attention mechanism reduces computational cost\")\n",
    "        print(f\"  • Patch-based processing enables fine-grained analysis\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to load data. Please check the data path and directory structure.\")\n",
    "        print(\"Expected structure: ../data/augmented/raf_db_balanced/train|test/Emotion_Name/*.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
