{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecff91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Memory Efficient Data Generator - PyTorch\n",
    "# Otimizado para datasets grandes (50k+ imagens)\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração de reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configuração otimizada de memória\n",
    "torch.backends.cudnn.benchmark = True  # Otimiza convoluções\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Memory Efficient Data Generator - Configurado\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c865f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AdvancedMemoryMonitor:\n",
    "    \"\"\"Monitor avançado de memória para datasets grandes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        self.initial_memory = self._get_memory_mb()\n",
    "        self.peak_memory = self.initial_memory\n",
    "        self.memory_history = []\n",
    "        \n",
    "    def _get_memory_mb(self):\n",
    "        \"\"\"Retorna uso de memória em MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    def _get_gpu_memory_mb(self):\n",
    "        \"\"\"Retorna uso de memória GPU em MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        return 0\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Atualiza estatísticas de memória\"\"\"\n",
    "        cpu_mem = self._get_memory_mb()\n",
    "        gpu_mem = self._get_gpu_memory_mb()\n",
    "        \n",
    "        self.peak_memory = max(self.peak_memory, cpu_mem)\n",
    "        self.memory_history.append({\n",
    "            'timestamp': time.time(),\n",
    "            'cpu_memory': cpu_mem,\n",
    "            'gpu_memory': gpu_mem\n",
    "        })\n",
    "        \n",
    "        return cpu_mem, gpu_mem\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Retorna status atual da memória\"\"\"\n",
    "        cpu_mem, gpu_mem = self.update()\n",
    "        return {\n",
    "            'current_cpu_mb': cpu_mem,\n",
    "            'current_gpu_mb': gpu_mem,\n",
    "            'peak_cpu_mb': self.peak_memory,\n",
    "            'memory_increase_mb': cpu_mem - self.initial_memory,\n",
    "            'total_system_memory_gb': psutil.virtual_memory().total / 1024**3,\n",
    "            'available_memory_gb': psutil.virtual_memory().available / 1024**3\n",
    "        }\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"Imprime status detalhado da memória\"\"\"\n",
    "        status = self.get_status()\n",
    "        print(f\"Memory Status:\")\n",
    "        print(f\"  CPU: {status['current_cpu_mb']:.1f} MB (Peak: {status['peak_cpu_mb']:.1f} MB)\")\n",
    "        print(f\"  GPU: {status['current_gpu_mb']:.1f} MB\")\n",
    "        print(f\"  Available: {status['available_memory_gb']:.1f} GB\")\n",
    "        print(f\"  Increase: +{status['memory_increase_mb']:.1f} MB\")\n",
    "\n",
    "# Instancia monitor\n",
    "memory_monitor = AdvancedMemoryMonitor()\n",
    "memory_monitor.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82372bf1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MemoryEfficientEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset eficiente que carrega imagens sob demanda.\n",
    "    Otimizado para datasets grandes (50k+ imagens).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, split='train', transform=None, \n",
    "                 cache_size=1000, img_size=224, lazy_load=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Caminho para os dados\n",
    "            split: 'train', 'val', ou 'test'\n",
    "            transform: Transformações do PyTorch\n",
    "            cache_size: Número de imagens em cache\n",
    "            img_size: Tamanho da imagem\n",
    "            lazy_load: Se True, carrega imagens sob demanda\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.lazy_load = lazy_load\n",
    "        \n",
    "        # Configuração de cache\n",
    "        self.cache_size = cache_size\n",
    "        self.image_cache = {}\n",
    "        self.cache_access_count = {}\n",
    "        \n",
    "        # Mapeamento de emoções\n",
    "        self.emotion_labels = {\n",
    "            'Raiva': 0, 'Nojo': 1, 'Medo': 2, 'Felicidade': 3, \n",
    "            'Neutro': 4, 'Tristeza': 5, 'Surpresa': 6\n",
    "        }\n",
    "        \n",
    "        # Carrega lista de arquivos (não as imagens)\n",
    "        self.image_paths, self.labels = self._load_file_paths()\n",
    "        \n",
    "        print(f\"Dataset {split} inicializado:\")\n",
    "        print(f\"  Total de imagens: {len(self.image_paths)}\")\n",
    "        print(f\"  Lazy loading: {'Ativado' if lazy_load else 'Desativado'}\")\n",
    "        print(f\"  Cache size: {cache_size}\")\n",
    "        print(f\"  Distribuição:\", dict(Counter(self.labels)))\n",
    "        \n",
    "    def _load_file_paths(self):\n",
    "        \"\"\"Carrega apenas os caminhos dos arquivos, não as imagens\"\"\"\n",
    "        split_path = self.data_path / self.split\n",
    "        \n",
    "        if not split_path.exists():\n",
    "            raise ValueError(f\"Caminho não encontrado: {split_path}\")\n",
    "        \n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for emotion, label_id in self.emotion_labels.items():\n",
    "            emotion_path = split_path / emotion\n",
    "            \n",
    "            if not emotion_path.exists():\n",
    "                print(f\"Aviso: {emotion_path} não encontrado\")\n",
    "                continue\n",
    "            \n",
    "            # Lista todos os arquivos de imagem\n",
    "            image_files = list(emotion_path.glob('*.jpg')) + \\\n",
    "                         list(emotion_path.glob('*.jpeg')) + \\\n",
    "                         list(emotion_path.glob('*.png'))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                image_paths.append(str(img_path))\n",
    "                labels.append(label_id)\n",
    "            \n",
    "            print(f\"  {emotion}: {len(image_files)} imagens encontradas\")\n",
    "        \n",
    "        return image_paths, labels\n",
    "    \n",
    "    def _load_image(self, image_path):\n",
    "        \"\"\"Carrega uma única imagem de forma eficiente\"\"\"\n",
    "        try:\n",
    "            # Carrega com OpenCV (mais rápido para preprocessing)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Não foi possível carregar: {image_path}\")\n",
    "            \n",
    "            # Converte BGR para RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Redimensiona apenas se necessário\n",
    "            if img.shape[:2] != (self.img_size, self.img_size):\n",
    "                img = cv2.resize(img, (self.img_size, self.img_size), \n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            return img\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {image_path}: {e}\")\n",
    "            # Retorna imagem preta como fallback\n",
    "            return np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    def _manage_cache(self, image_path, image):\n",
    "        \"\"\"Gerencia cache de imagens de forma inteligente\"\"\"\n",
    "        if len(self.image_cache) >= self.cache_size:\n",
    "            # Remove imagem menos acessada\n",
    "            least_used = min(self.cache_access_count.items(), key=lambda x: x[1])\n",
    "            least_used_path = least_used[0]\n",
    "            \n",
    "            del self.image_cache[least_used_path]\n",
    "            del self.cache_access_count[least_used_path]\n",
    "        \n",
    "        # Adiciona nova imagem\n",
    "        self.image_cache[image_path] = image\n",
    "        self.cache_access_count[image_path] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Verifica cache primeiro\n",
    "        if image_path in self.image_cache:\n",
    "            image = self.image_cache[image_path]\n",
    "            self.cache_access_count[image_path] += 1\n",
    "        else:\n",
    "            # Carrega imagem sob demanda\n",
    "            image = self._load_image(image_path)\n",
    "            \n",
    "            # Adiciona ao cache se ativado\n",
    "            if self.cache_size > 0:\n",
    "                self._manage_cache(image_path, image)\n",
    "        \n",
    "        # Converte para PIL para transforms\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # Aplica transformações\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Retorna estatísticas do cache\"\"\"\n",
    "        return {\n",
    "            'cache_size': len(self.image_cache),\n",
    "            'cache_limit': self.cache_size,\n",
    "            'hit_rate': len(self.image_cache) / max(1, len(self.image_paths)),\n",
    "            'most_accessed': max(self.cache_access_count.items(), \n",
    "                               key=lambda x: x[1]) if self.cache_access_count else None\n",
    "        }\n",
    "\n",
    "# Teste do dataset\n",
    "print(\"Testando Dataset eficiente...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397a14a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configurações para o dataset grande\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 16\n",
    "BASE_PATH = \"/home/leandro/Documents/TCC/emotion_recognition_tcc/data/augmented/raf_db_balanced\"\n",
    "CACHE_SIZE = 2000  # Número de imagens em cache\n",
    "NUM_WORKERS = 4 if torch.cuda.is_available() else 2\n",
    "\n",
    "# Transformações otimizadas\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Cria datasets eficientes\n",
    "print(\"Criando datasets eficientes...\")\n",
    "memory_monitor.print_status()\n",
    "\n",
    "try:\n",
    "    train_dataset = MemoryEfficientEmotionDataset(\n",
    "        BASE_PATH, split='train', transform=train_transform, \n",
    "        cache_size=CACHE_SIZE, img_size=IMG_SIZE\n",
    "    )\n",
    "    \n",
    "    test_dataset = MemoryEfficientEmotionDataset(\n",
    "        BASE_PATH, split='test', transform=val_test_transform,\n",
    "        cache_size=CACHE_SIZE//2, img_size=IMG_SIZE  # Cache menor para test\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets criados com sucesso!\")\n",
    "    memory_monitor.print_status()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar datasets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b098c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MemoryEfficientDataLoader:\n",
    "    \"\"\"DataLoader wrapper com controle de memória\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, shuffle=True, num_workers=2, \n",
    "                 memory_threshold_gb=8.0):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_threshold_gb = memory_threshold_gb\n",
    "        \n",
    "        # Ajusta batch_size baseado na memória disponível\n",
    "        available_memory = psutil.virtual_memory().available / 1024**3\n",
    "        if available_memory < memory_threshold_gb:\n",
    "            adjusted_batch_size = max(8, batch_size // 2)\n",
    "            print(f\"Ajustando batch_size: {batch_size} -> {adjusted_batch_size} \"\n",
    "                  f\"(memória disponível: {available_memory:.1f}GB)\")\n",
    "            self.batch_size = adjusted_batch_size\n",
    "        \n",
    "        self.dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=True if num_workers > 0 else False,\n",
    "            prefetch_factor=2 if num_workers > 0 else 2\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.dataloader)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "# Cria DataLoaders otimizados\n",
    "print(\"Criando DataLoaders otimizados...\")\n",
    "\n",
    "# Split treino/validação eficiente\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# DataLoaders com controle de memória\n",
    "train_loader = MemoryEfficientDataLoader(\n",
    "    train_subset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = MemoryEfficientDataLoader(\n",
    "    val_subset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = MemoryEfficientDataLoader(\n",
    "    test_dataset, BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders criados:\")\n",
    "print(f\"  Train batches: {len(train_loader.dataloader)}\")\n",
    "print(f\"  Val batches: {len(val_loader.dataloader)}\")\n",
    "print(f\"  Test batches: {len(test_loader.dataloader)}\")\n",
    "print(f\"  Batch size efetivo: {train_loader.batch_size}\")\n",
    "\n",
    "memory_monitor.print_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fb7a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_memory_efficiency():\n",
    "    \"\"\"Testa eficiência de memória carregando alguns batches\"\"\"\n",
    "    print(\"Testando eficiência de memória...\")\n",
    "    \n",
    "    # Memoria inicial\n",
    "    initial_status = memory_monitor.get_status()\n",
    "    print(f\"Memória inicial: {initial_status['current_cpu_mb']:.1f} MB\")\n",
    "    \n",
    "    # Carrega alguns batches para teste\n",
    "    test_batches = 5\n",
    "    batch_times = []\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if i >= test_batches:\n",
    "            break\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simula processamento\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        batch_time = time.time() - start_time\n",
    "        batch_times.append(batch_time)\n",
    "        \n",
    "        # Monitora memória\n",
    "        memory_monitor.update()\n",
    "        \n",
    "        print(f\"Batch {i+1}: {images.shape}, \"\n",
    "              f\"Time: {batch_time:.3f}s, \"\n",
    "              f\"Memory: {memory_monitor.get_status()['current_cpu_mb']:.1f}MB\")\n",
    "        \n",
    "        # Limpeza\n",
    "        if torch.cuda.is_available():\n",
    "            del images, labels\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    # Estatísticas finais\n",
    "    final_status = memory_monitor.get_status()\n",
    "    print(f\"\\nResultados do teste:\")\n",
    "    print(f\"  Memória inicial: {initial_status['current_cpu_mb']:.1f} MB\")\n",
    "    print(f\"  Memória final: {final_status['current_cpu_mb']:.1f} MB\")\n",
    "    print(f\"  Aumento: {final_status['current_cpu_mb'] - initial_status['current_cpu_mb']:.1f} MB\")\n",
    "    print(f\"  Tempo médio por batch: {np.mean(batch_times):.3f}s\")\n",
    "    print(f\"  Cache stats train:\", train_dataset.get_cache_stats())\n",
    "\n",
    "# Executa teste\n",
    "test_memory_efficiency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dd1ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Limpeza agressiva de memória\"\"\"\n",
    "    print(\"Executando limpeza de memória...\")\n",
    "    \n",
    "    # Limpa cache dos datasets\n",
    "    if 'train_dataset' in globals():\n",
    "        train_dataset.image_cache.clear()\n",
    "        train_dataset.cache_access_count.clear()\n",
    "    \n",
    "    if 'test_dataset' in globals():\n",
    "        test_dataset.image_cache.clear()\n",
    "        test_dataset.cache_access_count.clear()\n",
    "    \n",
    "    # Limpeza Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # Limpeza CUDA se disponível\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"Limpeza concluída!\")\n",
    "    memory_monitor.print_status()\n",
    "\n",
    "def memory_checkpoint(checkpoint_name=\"\"):\n",
    "    \"\"\"Cria checkpoint de memória\"\"\"\n",
    "    status = memory_monitor.get_status()\n",
    "    print(f\"Checkpoint {checkpoint_name}:\")\n",
    "    print(f\"  CPU Memory: {status['current_cpu_mb']:.1f} MB\")\n",
    "    print(f\"  GPU Memory: {status['current_gpu_mb']:.1f} MB\")\n",
    "    print(f\"  Available: {status['available_memory_gb']:.1f} GB\")\n",
    "    return status\n",
    "\n",
    "# Checkpoint inicial\n",
    "initial_checkpoint = memory_checkpoint(\"Inicial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b09d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MemoryOptimizedResNet50(nn.Module):\n",
    "    \"\"\"ResNet50 otimizado para uso eficiente de memória\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=7, checkpoint_segments=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Carrega ResNet50 base\n",
    "        self.backbone = models.resnet50(weights='IMAGENET1K_V2')\n",
    "        \n",
    "        # Remove classificador original\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Classifier otimizado\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Gradient checkpointing para economizar memória\n",
    "        self.use_checkpointing = checkpoint_segments > 0\n",
    "        if self.use_checkpointing:\n",
    "            self._setup_checkpointing()\n",
    "    \n",
    "    def _setup_checkpointing(self):\n",
    "        \"\"\"Configura gradient checkpointing\"\"\"\n",
    "        # Aplica checkpointing nas camadas pesadas\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if 'layer' in name and len(list(module.children())) > 0:\n",
    "                module = torch.utils.checkpoint.checkpoint_sequential(\n",
    "                    module, segments=2\n",
    "                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Backbone com checkpointing opcional\n",
    "        if self.use_checkpointing and self.training:\n",
    "            features = torch.utils.checkpoint.checkpoint(self.backbone, x)\n",
    "        else:\n",
    "            features = self.backbone(x)\n",
    "        \n",
    "        # Classifier\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Cria modelo otimizado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Criando modelo otimizado para {device}...\")\n",
    "\n",
    "model = MemoryOptimizedResNet50(num_classes=7, checkpoint_segments=4).to(device)\n",
    "\n",
    "# Conta parâmetros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Modelo criado:\")\n",
    "print(f\"  Total parâmetros: {total_params:,}\")\n",
    "print(f\"  Treináveis: {trainable_params:,}\")\n",
    "print(f\"  Gradient checkpointing: Ativado\")\n",
    "\n",
    "memory_checkpoint(\"Modelo criado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6889a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configurações de treinamento otimizadas\n",
    "EPOCHS = 50  # Reduzido para teste\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Otimizador com configurações de memória\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eps=1e-8  # Estabilidade numérica\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mixed Precision para economizar memória (se GPU disponível)\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "print(f\"Configuração de treinamento:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Mixed precision: {'Ativado' if use_amp else 'Desativado'}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "memory_checkpoint(\"Treinamento configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d9df7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_memory_control():\n",
    "    \"\"\"Treinamento com controle rigoroso de memória\"\"\"\n",
    "    \n",
    "    print(\"Iniciando treinamento com controle de memória...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    train_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # === TREINO ===\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move para device\n",
    "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass com mixed precision\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, target)\n",
    "                \n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Estatísticas\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Limpeza de memória a cada N batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # Print progresso\n",
    "                print(f'Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, '\n",
    "                      f'Acc: {100*train_correct/train_total:.1f}%, '\n",
    "                      f'Mem: {memory_monitor.get_status()[\"current_cpu_mb\"]:.0f}MB')\n",
    "        \n",
    "        # Médias do treino\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # === VALIDAÇÃO ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(data)\n",
    "                        loss = criterion(outputs, target)\n",
    "                else:\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Atualiza scheduler\n",
    "        scheduler.step(val_loss_avg)\n",
    "        \n",
    "        # Salva histórico\n",
    "        train_history['train_loss'].append(train_loss_avg)\n",
    "        train_history['train_acc'].append(train_acc)\n",
    "        train_history['val_loss'].append(val_loss_avg)\n",
    "        train_history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Época completa\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{EPOCHS} Summary:')\n",
    "        print(f'  Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'  Time: {epoch_time:.1f}s, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        # Salva melhor modelo\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_history': train_history\n",
    "            }, 'best_model_memory_efficient.pth')\n",
    "            print(f'  ✓ Novo melhor modelo salvo: {val_acc:.2f}%')\n",
    "        \n",
    "        # Limpeza de memória\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        memory_checkpoint(f\"Epoch {epoch+1}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Early stopping\n",
    "        if len(train_history['val_acc']) > 10:\n",
    "            recent_accs = train_history['val_acc'][-5:]\n",
    "            if max(recent_accs) - min(recent_accs) < 0.5:\n",
    "                print(\"Early stopping: sem melhoria significativa\")\n",
    "                break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTreinamento concluído em {total_time/60:.1f} minutos\")\n",
    "    print(f\"Melhor validação accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return train_history, best_val_acc\n",
    "\n",
    "# Executa treinamento\n",
    "print(\"=== INICIANDO TREINAMENTO COM CONTROLE DE MEMÓRIA ===\")\n",
    "history, best_acc = train_with_memory_control()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
