{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14326dd7",
   "metadata": {},
   "source": [
    "# ESTUDAR ESTE MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8232b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU configurada para EfficientViT: 1 dispositivos\n",
      "Precis√£o mista ativada: mixed_float16\n",
      "TensorFlow version: 2.20.0\n",
      "GPU dispon√≠vel: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "EfficientViT (Vision Transformer) configurado para experimenta√ß√£o cient√≠fica\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√µes espec√≠ficas para EfficientViT (Vision Transformer h√≠brido)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from keras.layers import (Dense, GlobalAveragePooling2D, Dropout, LayerNormalization, \n",
    "                         MultiHeadAttention, Add, Conv2D, DepthwiseConv2D, Reshape, \n",
    "                         Permute, Lambda, Activation)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import AdamW  # AdamW √© melhor para transformers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.optimizers.schedules import CosineDecay, ExponentialDecay\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Configura√ß√£o de reprodutibilidade\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configura√ß√µes otimizadas para Vision Transformers\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU configurada para EfficientViT: {len(gpus)} dispositivos\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Configura√ß√£o GPU: {e}\")\n",
    "\n",
    "# Verificar se mixed precision est√° dispon√≠vel\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f\"Precis√£o mista ativada: {tf.keras.mixed_precision.global_policy().name}\")\n",
    "except:\n",
    "    print(\"Precis√£o mista n√£o dispon√≠vel, usando float32\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU dispon√≠vel: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"EfficientViT (Vision Transformer) configurado para experimenta√ß√£o cient√≠fica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae39060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes espec√≠ficas para EfficientViT\n",
    "IMG_SIZE = 224  # Deve ser divis√≠vel pelo patch_size\n",
    "PATCH_SIZE = 16  # Tamanho dos patches (16x16 √© padr√£o para ViT)\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2  # 196 patches para 224x224\n",
    "BATCH_SIZE = 16  # Menor devido ao uso intensivo de mem√≥ria dos transformers\n",
    "EPOCHS = 80  # Menos √©pocas, transformers convergem mais r√°pido\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "# Configura√ß√µes espec√≠ficas do EfficientViT\n",
    "EFFICIENTVIT_CONFIG = {\n",
    "    'patch_size': PATCH_SIZE,\n",
    "    'num_patches': NUM_PATCHES,\n",
    "    'projection_dim': 256,  # Dimens√£o da proje√ß√£o dos patches\n",
    "    'num_heads': 8,         # Cabe√ßas de aten√ß√£o\n",
    "    'transformer_layers': 6, # N√∫mero de camadas transformer\n",
    "    'mlp_head_units': [1024, 512],  # Camadas MLP finais\n",
    "    'dropout_rate': 0.1,    # Dropout menor para transformers\n",
    "    'attention_dropout': 0.1,\n",
    "    'learning_rate': 3e-4,  # LR t√≠pico para transformers\n",
    "    'weight_decay': 0.03,   # Weight decay para AdamW\n",
    "    'warmup_epochs': 10,    # Warmup do learning rate\n",
    "    'cosine_decay_epochs': 70,  # Cosine decay ap√≥s warmup\n",
    "}\n",
    "\n",
    "# Configura√ß√µes h√≠bridas CNN+ViT\n",
    "HYBRID_CONFIG = {\n",
    "    'use_cnn_backbone': True,    # Usa CNN como feature extractor inicial\n",
    "    'cnn_layers': 3,             # N√∫mero de camadas CNN iniciais\n",
    "    'cnn_filters': [64, 128, 256], # Filtros das camadas CNN\n",
    "    'transition_layer': 'conv',   # Como transicionar CNN->ViT\n",
    "    'positional_encoding': 'learnable',  # Tipo de encoding posicional\n",
    "}\n",
    "\n",
    "# Mapeamento das emo√ß√µes (igual aos outros modelos)\n",
    "EMOTION_LABELS = {\n",
    "    'anger': 0, 'disgust': 1, 'fear': 2, 'happy': 3, \n",
    "    'neutral': 4, 'sadness': 5, 'surprise': 6\n",
    "}\n",
    "\n",
    "def create_cosine_decay_with_warmup(learning_rate, total_steps, warmup_steps):\n",
    "    \"\"\"\n",
    "    Cria scheduler de learning rate com warmup e cosine decay.\n",
    "    Implementa√ß√£o customizada para EfficientViT.\n",
    "    \"\"\"\n",
    "    def scheduler(epoch):\n",
    "        if epoch < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return learning_rate * (epoch / warmup_steps)\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            decay_steps = total_steps - warmup_steps\n",
    "            current_decay_step = min(epoch - warmup_steps, decay_steps)\n",
    "            cosine_decay = 0.5 * (1 + math.cos(math.pi * current_decay_step / decay_steps))\n",
    "            return learning_rate * cosine_decay\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "print(\"Configura√ß√µes EfficientViT definidas:\")\n",
    "print(f\"- Arquitetura: EfficientViT (CNN + Vision Transformer)\")\n",
    "print(f\"- Tamanho da imagem: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"- Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"- N√∫mero de patches: {NUM_PATCHES}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE} (reduzido para ViT)\")\n",
    "print(f\"- Proje√ß√£o: {EFFICIENTVIT_CONFIG['projection_dim']} dims\")\n",
    "print(f\"- Attention heads: {EFFICIENTVIT_CONFIG['num_heads']}\")\n",
    "print(f\"- Transformer layers: {EFFICIENTVIT_CONFIG['transformer_layers']}\")\n",
    "print(f\"- Learning rate: {EFFICIENTVIT_CONFIG['learning_rate']}\")\n",
    "print(f\"- H√≠brido CNN+ViT: {HYBRID_CONFIG['use_cnn_backbone']}\")\n",
    "print(f\"- Classes de emo√ß√£o: {len(EMOTION_LABELS)}\")\n",
    "print(\"- Precis√£o mista ativada para acelera√ß√£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientViTMonitor:\n",
    "    \"\"\"\n",
    "    Monitor especializado para Vision Transformers h√≠bridos.\n",
    "    Foca em m√©tricas de aten√ß√£o, patches e efici√™ncia computacional.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.peak_memory_mb = 0\n",
    "        self.initial_memory_mb = 0\n",
    "        self.attention_computation_time = 0\n",
    "        self.cnn_computation_time = 0\n",
    "        self.total_patches_processed = 0\n",
    "        self.process = psutil.Process()\n",
    "        self.epoch_attention_times = []\n",
    "        self.learning_rate_history = []\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Inicia monitoramento espec√≠fico para ViT\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_memory_mb = self._get_memory_usage()\n",
    "        self.peak_memory_mb = self.initial_memory_mb\n",
    "        \n",
    "        print(f\"Iniciando treinamento EfficientViT (CNN + Vision Transformer)...\")\n",
    "        print(f\"Hor√°rio de in√≠cio: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Mem√≥ria inicial: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Patches por imagem: {NUM_PATCHES}\")\n",
    "        print(f\"Configura√ß√£o h√≠brida: CNN backbone + Transformer layers\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"Retorna uso de mem√≥ria em MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "        \n",
    "    def update_peak_memory(self):\n",
    "        \"\"\"Atualiza pico de mem√≥ria\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        if current_memory > self.peak_memory_mb:\n",
    "            self.peak_memory_mb = current_memory\n",
    "            \n",
    "    def log_attention_computation(self, computation_time):\n",
    "        \"\"\"Registra tempo espec√≠fico de computa√ß√£o de aten√ß√£o\"\"\"\n",
    "        self.attention_computation_time += computation_time\n",
    "        \n",
    "    def log_cnn_computation(self, computation_time):\n",
    "        \"\"\"Registra tempo espec√≠fico de computa√ß√£o CNN\"\"\"\n",
    "        self.cnn_computation_time += computation_time\n",
    "        \n",
    "    def log_patches_processed(self, batch_size):\n",
    "        \"\"\"Registra n√∫mero de patches processados\"\"\"\n",
    "        self.total_patches_processed += batch_size * NUM_PATCHES\n",
    "        \n",
    "    def log_learning_rate(self, lr):\n",
    "        \"\"\"Registra learning rate para an√°lise de scheduling\"\"\"\n",
    "        self.learning_rate_history.append(lr)\n",
    "        \n",
    "    def get_attention_efficiency_metrics(self):\n",
    "        \"\"\"Calcula m√©tricas espec√≠ficas de efici√™ncia da aten√ß√£o\"\"\"\n",
    "        total_time = time.time() - self.start_time if self.start_time else 1\n",
    "        \n",
    "        return {\n",
    "            'attention_time_ratio': self.attention_computation_time / total_time if total_time > 0 else 0,\n",
    "            'cnn_time_ratio': self.cnn_computation_time / total_time if total_time > 0 else 0,\n",
    "            'patches_per_second': self.total_patches_processed / total_time if total_time > 0 else 0,\n",
    "            'attention_efficiency': self.total_patches_processed / (self.attention_computation_time + 1e-6),\n",
    "            'memory_per_patch': self.peak_memory_mb / NUM_PATCHES if NUM_PATCHES > 0 else 0,\n",
    "            'hybrid_balance': self.cnn_computation_time / (self.attention_computation_time + 1e-6)\n",
    "        }\n",
    "        \n",
    "    def end_monitoring(self):\n",
    "        \"\"\"Finaliza monitoramento com m√©tricas espec√≠ficas de ViT\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        total_time_seconds = self.end_time - self.start_time\n",
    "        total_time_formatted = str(timedelta(seconds=int(total_time_seconds)))\n",
    "        \n",
    "        final_memory_mb = self._get_memory_usage()\n",
    "        memory_increase = final_memory_mb - self.initial_memory_mb\n",
    "        \n",
    "        # M√©tricas de aten√ß√£o\n",
    "        attention_metrics = self.get_attention_efficiency_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RELAT√ìRIO DE MONITORAMENTO - EFFICIENTVIT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Tempo total de treinamento: {total_time_formatted}\")\n",
    "        print(f\"Mem√≥ria inicial: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Pico de mem√≥ria: {self.peak_memory_mb:.2f} MB\")\n",
    "        print(f\"Crescimento de mem√≥ria: {memory_increase:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\nM√âTRICAS DE ATEN√á√ÉO E PATCHES:\")\n",
    "        print(f\"  ‚Ä¢ Patches processados: {self.total_patches_processed:,}\")\n",
    "        print(f\"  ‚Ä¢ Patches/segundo: {attention_metrics['patches_per_second']:.1f}\")\n",
    "        print(f\"  ‚Ä¢ Tempo aten√ß√£o: {self.attention_computation_time:.1f}s ({attention_metrics['attention_time_ratio']*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Tempo CNN: {self.cnn_computation_time:.1f}s ({attention_metrics['cnn_time_ratio']*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Efici√™ncia aten√ß√£o: {attention_metrics['attention_efficiency']:.1f} patches/s\")\n",
    "        print(f\"  ‚Ä¢ Mem√≥ria/patch: {attention_metrics['memory_per_patch']:.3f} MB\")\n",
    "        print(f\"  ‚Ä¢ Balance CNN/ViT: {attention_metrics['hybrid_balance']:.2f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return {\n",
    "            'total_time_seconds': total_time_seconds,\n",
    "            'total_time_formatted': total_time_formatted,\n",
    "            'initial_memory_mb': self.initial_memory_mb,\n",
    "            'final_memory_mb': final_memory_mb,\n",
    "            'peak_memory_mb': self.peak_memory_mb,\n",
    "            'memory_increase_mb': memory_increase,\n",
    "            'attention_metrics': attention_metrics,\n",
    "            'learning_rate_history': self.learning_rate_history\n",
    "        }\n",
    "\n",
    "class ViTAttentionCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback especializado para monitorar aten√ß√£o em Vision Transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, monitor):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Tempo da √©poca\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        \n",
    "        # Atualiza mem√≥ria\n",
    "        self.monitor.update_peak_memory()\n",
    "        \n",
    "        # Registra patches processados (estimativa)\n",
    "        estimated_batches = 100  # Estimativa padr√£o\n",
    "        self.monitor.log_patches_processed(estimated_batches * BATCH_SIZE)\n",
    "                \n",
    "        # Log detalhado a cada 3 √©pocas (menos frequente para ViT)\n",
    "        if epoch % 3 == 0:\n",
    "            current_memory = self.monitor._get_memory_usage()\n",
    "            \n",
    "            print(f\"ViT √âpoca {epoch+1} - Tempo: {epoch_time:.1f}s, Mem√≥ria: {current_memory:.1f}MB\")\n",
    "            if logs:\n",
    "                print(f\"  ‚Ä¢ Train acc: {logs.get('accuracy', 0):.4f}, Val acc: {logs.get('val_accuracy', 0):.4f}\")\n",
    "                print(f\"  ‚Ä¢ Train loss: {logs.get('loss', 0):.4f}, Val loss: {logs.get('val_loss', 0):.4f}\")\n",
    "                \n",
    "                # Registra learning rate se dispon√≠vel\n",
    "                if 'lr' in logs:\n",
    "                    self.monitor.log_learning_rate(logs['lr'])\n",
    "\n",
    "# Instancia monitor especializado para EfficientViT\n",
    "monitor = EfficientViTMonitor()\n",
    "print(\"Monitor EfficientViT inicializado\")\n",
    "print(\"Recursos especializados:\")\n",
    "print(\"  ‚Ä¢ Monitoramento de patches e aten√ß√£o\")\n",
    "print(\"  ‚Ä¢ An√°lise de efici√™ncia CNN vs Transformer\")\n",
    "print(\"  ‚Ä¢ Tracking de learning rate scheduling\") \n",
    "print(\"  ‚Ä¢ M√©tricas h√≠bridas de arquitetura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516fbad",
   "metadata": {},
   "source": [
    "# MUDAR AQUI QUANDO A LU MANDAR OS DADOS PR√â TREINADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a292aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data_efficientvit_from_images():\n",
    "    \"\"\"\n",
    "    Carrega dados pr√©-processados de imagens JPG com preprocessing espec√≠fico para Vision Transformers.\n",
    "    EfficientViT usa normaliza√ß√£o [0, 1] e prepara√ß√£o para patch-based processing.\n",
    "    \n",
    "    Estrutura esperada:\n",
    "    data/processed/raf_db_temp_gray_aligned/\n",
    "    ‚îú‚îÄ‚îÄ Raiva/\n",
    "    ‚îú‚îÄ‚îÄ Nojo/\n",
    "    ‚îú‚îÄ‚îÄ Medo/\n",
    "    ‚îú‚îÄ‚îÄ Felicidade/\n",
    "    ‚îú‚îÄ‚îÄ Neutro/\n",
    "    ‚îú‚îÄ‚îÄ Tristeza/\n",
    "    ‚îî‚îÄ‚îÄ Surpresa/\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    print(\"Carregando dados pr√©-processados JPG para EfficientViT...\")\n",
    "    \n",
    "    # Configura√ß√µes\n",
    "    IMG_SIZE = 224  # Tamanho para EfficientViT (deve ser divis√≠vel por PATCH_SIZE)\n",
    "    BASE_PATH = r\".\\data\\processed\\raf_db_temp_gray_aligned\"  # Ajuste para seu caminho\n",
    "    \n",
    "    # Mapeamento das emo√ß√µes em portugu√™s\n",
    "    EMOTION_LABELS = {\n",
    "        'Raiva': 0, 'Nojo': 1, 'Medo': 2, 'Felicidade': 3, \n",
    "        'Neutro': 4, 'Tristeza': 5, 'Surpresa': 6\n",
    "    }\n",
    "    \n",
    "    def load_images_from_directory(directory_path, set_name):\n",
    "        \"\"\"Carrega imagens de um diret√≥rio com verifica√ß√£o de patches\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(f\"Carregando {set_name} de: {directory_path}\")\n",
    "        \n",
    "        # Verifica se o diret√≥rio existe\n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"‚ùå Diret√≥rio n√£o encontrado: {directory_path}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # Lista subdiret√≥rios (emo√ß√µes)\n",
    "        subdirs = [d for d in os.listdir(directory_path) \n",
    "                  if os.path.isdir(os.path.join(directory_path, d))]\n",
    "        \n",
    "        print(f\"üìÅ Subdiret√≥rios encontrados: {subdirs}\")\n",
    "        \n",
    "        for emotion, label in EMOTION_LABELS.items():\n",
    "            # Usar os.path.join ao inv√©s de /\n",
    "            emotion_path = os.path.join(directory_path, emotion)\n",
    "            \n",
    "            if not os.path.exists(emotion_path):\n",
    "                print(f\"‚ö†Ô∏è  Pasta '{emotion}' n√£o encontrada em {directory_path}\")\n",
    "                print(f\"    Tentando varia√ß√µes de nome...\")\n",
    "                \n",
    "                # Tenta varia√ß√µes do nome da emo√ß√£o\n",
    "                emotion_variations = [\n",
    "                    emotion.lower(),\n",
    "                    emotion.upper(), \n",
    "                    emotion.capitalize(),\n",
    "                    emotion.replace('√ß', 'c'),  # Felicidade -> Felicidade\n",
    "                    emotion.replace('√£', 'a')   # Raiva -> Raiva\n",
    "                ]\n",
    "                \n",
    "                found = False\n",
    "                for variation in emotion_variations:\n",
    "                    test_path = os.path.join(directory_path, variation)\n",
    "                    if os.path.exists(test_path):\n",
    "                        emotion_path = test_path\n",
    "                        print(f\"    ‚úÖ Encontrado: {variation}\")\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found:\n",
    "                    print(f\"    ‚ùå Nenhuma varia√ß√£o encontrada para '{emotion}'\")\n",
    "                    continue\n",
    "            \n",
    "            # Carrega imagens da pasta da emo√ß√£o\n",
    "            count = 0\n",
    "            image_files = []\n",
    "            \n",
    "            # Busca diferentes extens√µes\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "                import glob\n",
    "                pattern = os.path.join(emotion_path, ext)\n",
    "                image_files.extend(glob.glob(pattern))\n",
    "            \n",
    "            print(f\"  üì∏ {emotion}: {len(image_files)} arquivos encontrados\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    # Carrega imagem\n",
    "                    img = cv2.imread(img_file)\n",
    "                    if img is None:\n",
    "                        print(f\"    ‚ö†Ô∏è N√£o foi poss√≠vel carregar: {os.path.basename(img_file)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Converte BGR para RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Redimensiona para tamanho compat√≠vel com patches\n",
    "                    if img.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
    "                        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Garante que seja RGB (3 canais)\n",
    "                    if len(img.shape) == 2:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    elif img.shape[2] == 1:\n",
    "                        img = np.repeat(img, 3, axis=2)\n",
    "                    elif img.shape[2] == 4:  # RGBA\n",
    "                        img = img[:, :, :3]  # Remove canal alpha\n",
    "                    \n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå Erro ao carregar {os.path.basename(img_file)}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  ‚úÖ {emotion}: {count} imagens carregadas com sucesso\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    def detect_data_structure(base_path):\n",
    "        \"\"\"Detecta a estrutura dos dados automaticamente\"\"\"\n",
    "        print(f\"üîç Analisando estrutura de: {base_path}\")\n",
    "        \n",
    "        if not os.path.exists(base_path):\n",
    "            print(f\"‚ùå Caminho base n√£o existe: {base_path}\")\n",
    "            return None\n",
    "            \n",
    "        # Lista conte√∫do do diret√≥rio\n",
    "        contents = os.listdir(base_path)\n",
    "        dirs = [d for d in contents if os.path.isdir(os.path.join(base_path, d))]\n",
    "        files = [f for f in contents if os.path.isfile(os.path.join(base_path, f))]\n",
    "        \n",
    "        print(f\"üìÅ Diret√≥rios: {dirs}\")\n",
    "        print(f\"üìÑ Arquivos: {len(files)} encontrados\")\n",
    "        \n",
    "        # Verifica se tem estrutura train/test\n",
    "        if 'train' in dirs and 'test' in dirs:\n",
    "            print(\"‚úÖ Estrutura detectada: train/test/emotion/\")\n",
    "            return 'train_test'\n",
    "        \n",
    "        # Verifica se as pastas s√£o emo√ß√µes diretamente\n",
    "        emotion_names = set(EMOTION_LABELS.keys())\n",
    "        found_emotions = set(dirs) & emotion_names\n",
    "        \n",
    "        if found_emotions:\n",
    "            print(f\"‚úÖ Estrutura detectada: emotion/ direta - Emo√ß√µes: {found_emotions}\")\n",
    "            return 'emotion_direct'\n",
    "        \n",
    "        # Verifica varia√ß√µes de nomes\n",
    "        emotion_variations = []\n",
    "        for emotion in EMOTION_LABELS.keys():\n",
    "            variations = [emotion.lower(), emotion.upper(), emotion.capitalize()]\n",
    "            emotion_variations.extend(variations)\n",
    "        \n",
    "        found_variations = set(dirs) & set(emotion_variations)\n",
    "        if found_variations:\n",
    "            print(f\"‚úÖ Estrutura detectada: emotion/ com varia√ß√µes - Encontradas: {found_variations}\")\n",
    "            return 'emotion_direct'\n",
    "        \n",
    "        print(\"‚ö†Ô∏è Estrutura n√£o reconhecida automaticamente\")\n",
    "        return 'unknown'\n",
    "    \n",
    "    def verify_patch_compatibility(images, patch_size):\n",
    "        \"\"\"Verifica se as imagens s√£o compat√≠veis com o patch_size\"\"\"\n",
    "        if len(images) == 0:\n",
    "            return images\n",
    "        \n",
    "        height, width = images.shape[1], images.shape[2]\n",
    "        \n",
    "        print(f\"üîç Verifica√ß√£o de compatibilidade com patches:\")\n",
    "        print(f\"- Dimens√µes atuais: {height}x{width}\")\n",
    "        print(f\"- Patch size: {patch_size}x{patch_size}\")\n",
    "        \n",
    "        if height % patch_size != 0 or width % patch_size != 0:\n",
    "            print(f\"‚ö†Ô∏è Dimens√µes n√£o s√£o divis√≠veis por patch_size {patch_size}\")\n",
    "            print(f\"Redimensionando para {IMG_SIZE}x{IMG_SIZE}...\")\n",
    "            \n",
    "            # Redimensiona todas as imagens\n",
    "            resized_images = np.zeros((images.shape[0], IMG_SIZE, IMG_SIZE, 3), dtype=images.dtype)\n",
    "            \n",
    "            for i in range(images.shape[0]):\n",
    "                resized_images[i] = cv2.resize(images[i], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            print(f\"‚úÖ Redimensionado para {IMG_SIZE}x{IMG_SIZE} compat√≠vel com patches {patch_size}x{patch_size}\")\n",
    "            return resized_images\n",
    "        else:\n",
    "            print(f\"‚úÖ Dimens√µes j√° s√£o compat√≠veis com patches\")\n",
    "            return images\n",
    "    \n",
    "    try:\n",
    "        # Detecta estrutura automaticamente\n",
    "        structure = detect_data_structure(BASE_PATH)\n",
    "        \n",
    "        if structure == 'train_test':\n",
    "            # Estrutura: base/train/emotion/ e base/test/emotion/\n",
    "            train_path = os.path.join(BASE_PATH, \"train\")\n",
    "            test_path = os.path.join(BASE_PATH, \"test\")\n",
    "            \n",
    "            X_train, y_train = load_images_from_directory(train_path, \"TREINO\")\n",
    "            X_test, y_test = load_images_from_directory(test_path, \"TESTE\")\n",
    "            \n",
    "        elif structure == 'emotion_direct':\n",
    "            # Estrutura: base/emotion/ - precisa criar train/test split\n",
    "            print(\"üìä Carregando todas as imagens e criando divis√£o train/test...\")\n",
    "            \n",
    "            all_images, all_labels = load_images_from_directory(BASE_PATH, \"TODAS AS IMAGENS\")\n",
    "            \n",
    "            if len(all_images) == 0:\n",
    "                print(\"‚ùå Nenhuma imagem carregada!\")\n",
    "                return None, None, None, None\n",
    "            \n",
    "            # Cria divis√£o train/test\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                all_images, all_labels,\n",
    "                test_size=0.2,\n",
    "                stratify=all_labels,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            print(\"‚úÖ Divis√£o train/test criada automaticamente (80/20)\")\n",
    "            \n",
    "        else:\n",
    "            p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientvit_experiment_structure():\n",
    "    \"\"\"\n",
    "    Cria estrutura de diret√≥rios espec√≠fica para experimentos EfficientViT.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_id = f\"efficientvit_emotion_{timestamp}\"\n",
    "    \n",
    "    # Cria diret√≥rios espec√≠ficos para ViT\n",
    "    os.makedirs(\"models/efficientvit\", exist_ok=True)\n",
    "    os.makedirs(\"metrics/efficientvit\", exist_ok=True)\n",
    "    os.makedirs(\"plots/efficientvit\", exist_ok=True)\n",
    "    os.makedirs(\"attention_maps\", exist_ok=True)  # Para visualiza√ß√µes de aten√ß√£o\n",
    "    \n",
    "    return experiment_id\n",
    "\n",
    "def save_efficientvit_model_if_good_performance(model, accuracy, f1_score, experiment_id, threshold=0.78):\n",
    "    \"\"\"\n",
    "    Salva modelo EfficientViT apenas se performance for boa.\n",
    "    Inclui salvamento de configura√ß√µes espec√≠ficas do transformer.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo ViT treinado\n",
    "        accuracy: Acur√°cia do modelo\n",
    "        f1_score: F1-score macro do modelo  \n",
    "        experiment_id: ID √∫nico do experimento\n",
    "        threshold: Limite m√≠nimo para salvar (mais baixo para ViT experimental)\n",
    "    \"\"\"\n",
    "    # Crit√©rio espec√≠fico para Vision Transformers (pode ser mais experimental)\n",
    "    performance_score = (accuracy + f1_score) / 2\n",
    "    efficiency_bonus = 0.02 if model.count_params() < 10_000_000 else 0  # B√¥nus por efici√™ncia\n",
    "    final_score = performance_score + efficiency_bonus\n",
    "    \n",
    "    if final_score >= threshold:\n",
    "        \n",
    "        # Salva pesos do modelo\n",
    "        model.save_weights(f\"models/efficientvit/weights_efficientvit_{experiment_id}.h5\")\n",
    "        \n",
    "        # Configura√ß√£o detalhada do EfficientViT\n",
    "        model_config = {\n",
    "            'architecture': 'EfficientViT (CNN + Vision Transformer)',\n",
    "            'img_size': IMG_SIZE,\n",
    "            'patch_size': PATCH_SIZE,\n",
    "            'num_patches': NUM_PATCHES,\n",
    "            'num_classes': 7,\n",
    "            'experiment_id': experiment_id,\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1_score,\n",
    "            'performance_score': performance_score,\n",
    "            'efficiency_bonus': efficiency_bonus,\n",
    "            'final_score': final_score,\n",
    "            'normalization_range': '[0, 1]',\n",
    "            'total_params': model.count_params(),\n",
    "            'trainable_params': sum([tf.keras.backend.count_params(p) for p in model.trainable_weights]),\n",
    "            \n",
    "            # Configura√ß√µes espec√≠ficas ViT\n",
    "            'projection_dim': EFFICIENTVIT_CONFIG['projection_dim'],\n",
    "            'num_heads': EFFICIENTVIT_CONFIG['num_heads'],\n",
    "            'transformer_layers': EFFICIENTVIT_CONFIG['transformer_layers'],\n",
    "            'attention_dropout': EFFICIENTVIT_CONFIG['attention_dropout'],\n",
    "            'dropout_rate': EFFICIENTVIT_CONFIG['dropout_rate'],\n",
    "            \n",
    "            # Configura√ß√µes h√≠bridas\n",
    "            'use_cnn_backbone': HYBRID_CONFIG['use_cnn_backbone'],\n",
    "            'cnn_layers': HYBRID_CONFIG['cnn_layers'],\n",
    "            'positional_encoding': HYBRID_CONFIG['positional_encoding'],\n",
    "            \n",
    "            # Configura√ß√µes de treinamento\n",
    "            'learning_rate': EFFICIENTVIT_CONFIG['learning_rate'],\n",
    "            'weight_decay': EFFICIENTVIT_CONFIG['weight_decay'],\n",
    "            'warmup_epochs': EFFICIENTVIT_CONFIG['warmup_epochs'],\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            \n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Salva configura√ß√£o\n",
    "        with open(f\"models/efficientvit/config_efficientvit_{experiment_id}.pkl\", 'wb') as f:\n",
    "            pickle.dump(model_config, f)\n",
    "        \n",
    "        print(f\"EfficientViT salvo! Score final: {final_score:.4f} (Performance: {performance_score:.4f} + Bonus: {efficiency_bonus:.3f})\")\n",
    "        print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f}, F1: {f1_score:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Par√¢metros: {model.count_params()/1000000:.1f}M\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Performance insuficiente: {final_score:.4f} < {threshold}\")\n",
    "        print(f\"  ‚Ä¢ Performance: {performance_score:.4f}, Bonus: {efficiency_bonus:.3f}\")\n",
    "        return False\n",
    "\n",
    "def save_efficientvit_metrics_to_csv(metrics_dict, experiment_id):\n",
    "    \"\"\"\n",
    "    Salva m√©tricas EfficientViT em CSV com campos espec√≠ficos para ViT.\n",
    "    \"\"\"\n",
    "    # Adiciona identificadores espec√≠ficos\n",
    "    metrics_dict['architecture'] = 'EfficientViT'\n",
    "    metrics_dict['model_type'] = 'Hybrid_CNN_ViT'\n",
    "    \n",
    "    # DataFrame com m√©tricas\n",
    "    metrics_df = pd.DataFrame([metrics_dict])\n",
    "    \n",
    "    # Arquivo CSV espec√≠fico para EfficientViT\n",
    "    efficientvit_csv = \"metrics/efficientvit/efficientvit_performance_metrics.csv\"\n",
    "    \n",
    "    # Append ao CSV se existir\n",
    "    if os.path.exists(efficientvit_csv):\n",
    "        metrics_df.to_csv(efficientvit_csv, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(efficientvit_csv, index=False)\n",
    "    \n",
    "    # Arquivo CSV consolidado (compara√ß√£o com todos os modelos)\n",
    "    consolidated_csv = \"metrics/all_models_comparison.csv\"\n",
    "    if os.path.exists(consolidated_csv):\n",
    "        metrics_df.to_csv(consolidated_csv, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(consolidated_csv, index=False)\n",
    "    \n",
    "    # Arquivo individual\n",
    "    individual_csv = f\"metrics/efficientvit/efficientvit_metrics_{experiment_id}.csv\"\n",
    "    metrics_df.to_csv(individual_csv, index=False)\n",
    "    \n",
    "    print(f\"M√©tricas EfficientViT salvas em:\")\n",
    "    print(f\"  ‚Ä¢ Espec√≠fico ViT: {efficientvit_csv}\")\n",
    "    print(f\"  ‚Ä¢ Consolidado: {consolidated_csv}\")\n",
    "    print(f\"  ‚Ä¢ Individual: {individual_csv}\")\n",
    "\n",
    "def save_attention_visualization_config(experiment_id):\n",
    "    \"\"\"\n",
    "    Salva configura√ß√£o para futuras visualiza√ß√µes de mapas de aten√ß√£o.\n",
    "    \"\"\"\n",
    "    attention_config = {\n",
    "        'experiment_id': experiment_id,\n",
    "        'patch_size': PATCH_SIZE,\n",
    "        'num_patches': NUM_PATCHES,\n",
    "        'num_heads': EFFICIENTVIT_CONFIG['num_heads'],\n",
    "        'transformer_layers': EFFICIENTVIT_CONFIG['transformer_layers'],\n",
    "        'img_size': IMG_SIZE,\n",
    "        'attention_map_layers': list(range(EFFICIENTVIT_CONFIG['transformer_layers'])),\n",
    "        'visualization_ready': True\n",
    "    }\n",
    "    \n",
    "    with open(f\"attention_maps/attention_config_{experiment_id}.pkl\", 'wb') as f:\n",
    "        pickle.dump(attention_config, f)\n",
    "    \n",
    "    print(f\"Configura√ß√£o de aten√ß√£o salva para visualiza√ß√µes futuras\")\n",
    "\n",
    "# Inicializa estrutura espec√≠fica do EfficientViT\n",
    "experiment_id = create_efficientvit_experiment_structure()\n",
    "print(f\"Experimento EfficientViT iniciado: {experiment_id}\")\n",
    "print(\"Estruturas criadas:\")\n",
    "print(\"  ‚Ä¢ models/efficientvit/ - Modelos e configura√ß√µes\")\n",
    "print(\"  ‚Ä¢ metrics/efficientvit/ - M√©tricas espec√≠ficas ViT\")\n",
    "print(\"  ‚Ä¢ plots/efficientvit/ - Visualiza√ß√µes ViT\")\n",
    "print(\"  ‚Ä¢ attention_maps/ - Configura√ß√µes para mapas de aten√ß√£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beae8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patch_embedding_layer(projection_dim):\n",
    "    \"\"\"\n",
    "    Cria camada de embedding de patches para Vision Transformer.\n",
    "    \"\"\"\n",
    "    def patch_embedding(x):\n",
    "        # x shape: (batch_size, height, width, channels)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Extrai patches usando tf.image.extract_patches\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=x,\n",
    "            sizes=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
    "            strides=[1, PATCH_SIZE, PATCH_SIZE, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        \n",
    "        # Reshape para (batch_size, num_patches, patch_size*patch_size*channels)\n",
    "        patches = tf.reshape(patches, [batch_size, NUM_PATCHES, PATCH_SIZE * PATCH_SIZE * 3])\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "    return Lambda(patch_embedding, name='patch_extraction')\n",
    "\n",
    "def create_positional_embedding(num_patches, projection_dim):\n",
    "    \"\"\"\n",
    "    Cria embedding posicional aprend√≠vel para os patches.\n",
    "    \"\"\"\n",
    "    class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "        def __init__(self, num_patches, projection_dim):\n",
    "            super().__init__()\n",
    "            self.num_patches = num_patches\n",
    "            self.projection_dim = projection_dim\n",
    "            self.position_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=num_patches, output_dim=projection_dim\n",
    "            )\n",
    "            self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "        def call(self, encoded_patches):\n",
    "            encoded_positions = self.position_embedding(self.positions)\n",
    "            encoded_patches = encoded_patches + encoded_positions\n",
    "            return encoded_patches\n",
    "    \n",
    "    return PositionalEmbedding(num_patches, projection_dim)\n",
    "\n",
    "def create_transformer_encoder_block(projection_dim, num_heads, dropout_rate, attention_dropout):\n",
    "    \"\"\"\n",
    "    Cria bloco encoder do transformer com multi-head attention.\n",
    "    \"\"\"\n",
    "    def transformer_encoder(x):\n",
    "        # Layer normalization 1\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim // num_heads,\n",
    "            dropout=attention_dropout\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip connection 1\n",
    "        x2 = Add()([attention_output, x])\n",
    "        \n",
    "        # Layer normalization 2\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        \n",
    "        # MLP\n",
    "        x4 = Dense(projection_dim * 2, activation=\"gelu\")(x3)\n",
    "        x4 = Dropout(dropout_rate)(x4)\n",
    "        x4 = Dense(projection_dim)(x4)\n",
    "        x4 = Dropout(dropout_rate)(x4)\n",
    "        \n",
    "        # Skip connection 2\n",
    "        encoded = Add()([x4, x2])\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    return transformer_encoder\n",
    "\n",
    "def create_cnn_backbone():\n",
    "    \"\"\"\n",
    "    Cria CNN backbone eficiente para extra√ß√£o inicial de features.\n",
    "    \"\"\"\n",
    "    def cnn_layers(x):\n",
    "        # Primeira camada CNN\n",
    "        x = Conv2D(HYBRID_CONFIG['cnn_filters'][0], 7, strides=2, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Segunda camada CNN  \n",
    "        x = Conv2D(HYBRID_CONFIG['cnn_filters'][1], 5, strides=2, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Terceira camada CNN\n",
    "        x = Conv2D(HYBRID_CONFIG['cnn_filters'][2], 3, strides=1, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return cnn_layers\n",
    "\n",
    "def create_efficientvit_model():\n",
    "    \"\"\"\n",
    "    Cria modelo EfficientViT h√≠brido (CNN + Vision Transformer).\n",
    "    \n",
    "    Arquitetura:\n",
    "    1. CNN Backbone para extra√ß√£o inicial de features\n",
    "    2. Patch embedding e proje√ß√£o linear\n",
    "    3. Positional embedding\n",
    "    4. Stack de transformer encoder blocks\n",
    "    5. Global average pooling + classification head\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # === CNN BACKBONE (se habilitado) ===\n",
    "    if HYBRID_CONFIG['use_cnn_backbone']:\n",
    "        print(\"Adicionando CNN backbone...\")\n",
    "        cnn_features = create_cnn_backbone()(inputs)\n",
    "        features = cnn_features\n",
    "    else:\n",
    "        features = inputs\n",
    "    \n",
    "    # === PATCH EMBEDDING ===\n",
    "    print(f\"Criando patch embedding: patches {PATCH_SIZE}x{PATCH_SIZE}...\")\n",
    "    patch_layer = create_patch_embedding_layer(EFFICIENTVIT_CONFIG['projection_dim'])\n",
    "    patches = patch_layer(features)\n",
    "    \n",
    "    # Proje√ß√£o linear dos patches\n",
    "    projected_patches = Dense(EFFICIENTVIT_CONFIG['projection_dim'])(patches)\n",
    "    \n",
    "    # === POSITIONAL EMBEDDING ===\n",
    "    print(\"Adicionando positional embedding...\")\n",
    "    pos_embedding = create_positional_embedding(NUM_PATCHES, EFFICIENTVIT_CONFIG['projection_dim'])\n",
    "    encoded_patches = pos_embedding(projected_patches)\n",
    "    \n",
    "    # Dropout inicial\n",
    "    encoded_patches = Dropout(EFFICIENTVIT_CONFIG['dropout_rate'])(encoded_patches)\n",
    "    \n",
    "    # === TRANSFORMER ENCODER BLOCKS ===\n",
    "    print(f\"Criando {EFFICIENTVIT_CONFIG['transformer_layers']} camadas transformer...\")\n",
    "    x = encoded_patches\n",
    "    \n",
    "    for i in range(EFFICIENTVIT_CONFIG['transformer_layers']):\n",
    "        transformer_block = create_transformer_encoder_block(\n",
    "            EFFICIENTVIT_CONFIG['projection_dim'],\n",
    "            EFFICIENTVIT_CONFIG['num_heads'],\n",
    "            EFFICIENTVIT_CONFIG['dropout_rate'],\n",
    "            EFFICIENTVIT_CONFIG['attention_dropout']\n",
    "        )\n",
    "        x = transformer_block(x)\n",
    "        print(f\"  ‚Ä¢ Transformer layer {i+1}/{EFFICIENTVIT_CONFIG['transformer_layers']} adicionada\")\n",
    "    \n",
    "    # === CLASSIFICATION HEAD ===\n",
    "    print(\"Adicionando classification head...\")\n",
    "    \n",
    "    # Layer normalization final\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Global average pooling sobre os patches\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # MLP Head\n",
    "    for units in EFFICIENTVIT_CONFIG['mlp_head_units']:\n",
    "        x = Dense(units, activation=\"gelu\")(x)\n",
    "        x = Dropout(EFFICIENTVIT_CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    # Classifica√ß√£o final\n",
    "    outputs = Dense(7, activation=\"softmax\", dtype='float32', name='emotion_predictions')(x)\n",
    "    \n",
    "    # Modelo final\n",
    "    model = Model(inputs, outputs, name='EfficientViT_Emotion_Classifier')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_efficientvit_model(model, total_steps):\n",
    "    \"\"\"\n",
    "    Compila modelo EfficientViT com configura√ß√µes otimizadas.\n",
    "    \"\"\"\n",
    "    # Learning rate scheduler com warmup\n",
    "    lr_scheduler = create_cosine_decay_with_warmup(\n",
    "        EFFICIENTVIT_CONFIG['learning_rate'],\n",
    "        total_steps,\n",
    "        EFFICIENTVIT_CONFIG['warmup_epochs']\n",
    "    )\n",
    "    \n",
    "    # Optimizer AdamW com weight decay\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=EFFICIENTVIT_CONFIG['learning_rate'],\n",
    "        weight_decay=EFFICIENTVIT_CONFIG['weight_decay'],\n",
    "        epsilon=1e-8,\n",
    "        clipnorm=1.0  # Gradient clipping para transformers\n",
    "    )\n",
    "    \n",
    "    # Loss com label smoothing (bom para transformers)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=0.1,\n",
    "        from_logits=False\n",
    "    )\n",
    "    \n",
    "    # Compila√ß√£o\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"EfficientViT compilado com:\")\n",
    "    print(f\"  ‚Ä¢ Optimizer: AdamW (lr={EFFICIENTVIT_CONFIG['learning_rate']}, wd={EFFICIENTVIT_CONFIG['weight_decay']})\")\n",
    "    print(f\"  ‚Ä¢ Loss: CategoricalCrossentropy (label_smoothing=0.1)\")\n",
    "    print(f\"  ‚Ä¢ Gradient clipping: 1.0\")\n",
    "    print(f\"  ‚Ä¢ Learning rate scheduling: Warmup + Cosine Decay\")\n",
    "    \n",
    "    return lr_scheduler\n",
    "\n",
    "# Cria modelo se dados foram carregados\n",
    "if X_train is not None:\n",
    "    print(\"=\"*70)\n",
    "    print(\"CRIANDO MODELO EFFICIENTVIT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"Configura√ß√£o h√≠brida:\")\n",
    "    print(f\"  ‚Ä¢ CNN backbone: {HYBRID_CONFIG['use_cnn_backbone']}\")\n",
    "    print(f\"  ‚Ä¢ Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "    print(f\"  ‚Ä¢ Patches por imagem: {NUM_PATCHES}\")\n",
    "    print(f\"  ‚Ä¢ Proje√ß√£o: {EFFICIENTVIT_CONFIG['projection_dim']} dims\")\n",
    "    print(f\"  ‚Ä¢ Attention heads: {EFFICIENTVIT_CONFIG['num_heads']}\")\n",
    "    print(f\"  ‚Ä¢ Transformer layers: {EFFICIENTVIT_CONFIG['transformer_layers']}\")\n",
    "    \n",
    "    # Cria modelo\n",
    "    model = create_efficientvit_model()\n",
    "    \n",
    "    # Estima total de steps para o scheduler\n",
    "    steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "    total_steps = steps_per_epoch * EPOCHS\n",
    "    \n",
    "    # Compila modelo\n",
    "    lr_scheduler = compile_efficientvit_model(model, total_steps)\n",
    "    \n",
    "    # Estat√≠sticas do modelo\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(p) for p in model.trainable_weights])\n",
    "    \n",
    "    print(f\"\\nEfficientViT criado com sucesso:\")\n",
    "    print(f\"  ‚Ä¢ Total de par√¢metros: {total_params:,}\")\n",
    "    print(f\"  ‚Ä¢ Par√¢metros trein√°veis: {trainable_params:,}\")\n",
    "    print(f\"  ‚Ä¢ Efici√™ncia: {total_params/1000000:.1f}M par√¢metros\")\n",
    "    print(f\"  ‚Ä¢ Compara√ß√£o ResNet50: {25.6/(total_params/1000000):.1f}x mais eficiente\")\n",
    "    print(f\"  ‚Ä¢ Compara√ß√£o EfficientNet: {5.3/(total_params/1000000):.1f}x vs EfficientNet-B0\")\n",
    "    \n",
    "    # Sum√°rio arquitetural\n",
    "    print(f\"\\nArquitetura EfficientViT:\")\n",
    "    if HYBRID_CONFIG['use_cnn_backbone']:\n",
    "        print(f\"  ‚Ä¢ CNN Backbone: {HYBRID_CONFIG['cnn_layers']} camadas\")\n",
    "    print(f\"  ‚Ä¢ Patch Embedding: {IMG_SIZE}x{IMG_SIZE} -> {NUM_PATCHES} patches\")\n",
    "    print(f\"  ‚Ä¢ Positional Embedding: Aprend√≠vel\")\n",
    "    print(f\"  ‚Ä¢ Transformer Stack: {EFFICIENTVIT_CONFIG['transformer_layers']} layers\")\n",
    "    print(f\"  ‚Ä¢ Classification Head: MLP {EFFICIENTVIT_CONFIG['mlp_head_units']} -> 7 classes\")\n",
    "    \n",
    "    monitor.update_peak_memory()\n",
    "    save_attention_visualization_config(experiment_id)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: Dados n√£o carregados. Verifique a c√©lula de carregamento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_efficientvit_callbacks(monitor, lr_scheduler):\n",
    "    \"\"\"\n",
    "    Configura callbacks espec√≠ficos para Vision Transformers.\n",
    "    \"\"\"\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # Learning Rate Scheduler customizado\n",
    "    lr_callback = LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "    callbacks_list.append(lr_callback)\n",
    "    \n",
    "    # Early stopping espec√≠fico para transformers\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=25,  # Mais paci√™ncia para transformers\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        min_delta=0.0005\n",
    "    )\n",
    "    callbacks_list.append(early_stopping)\n",
    "    \n",
    "    # Reduce LR on plateau como backup\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=12,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list.append(reduce_lr)\n",
    "    \n",
    "    # Callback de aten√ß√£o especializado\n",
    "    attention_callback = ViTAttentionCallback(monitor)\n",
    "    callbacks_list.append(attention_callback)\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "def train_efficientvit_model(model, X_train, y_train, X_val, y_val, monitor, callbacks):\n",
    "    \"\"\"\n",
    "    Executa treinamento EfficientViT com monitoramento especializado.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"INICIANDO TREINAMENTO EFFICIENTVIT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configura√ß√£o de treinamento:\")\n",
    "    print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE} (otimizado para ViT)\")\n",
    "    print(f\"  ‚Ä¢ Epochs m√°ximo: {EPOCHS}\")\n",
    "    print(f\"  ‚Ä¢ Learning rate inicial: {EFFICIENTVIT_CONFIG['learning_rate']}\")\n",
    "    print(f\"  ‚Ä¢ Weight decay: {EFFICIENTVIT_CONFIG['weight_decay']}\")\n",
    "    print(f\"  ‚Ä¢ Warmup epochs: {EFFICIENTVIT_CONFIG['warmup_epochs']}\")\n",
    "    print(f\"  ‚Ä¢ Precision: {tf.keras.mixed_precision.global_policy().name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Inicia cron√¥metro espec√≠fico do treinamento\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    # Simula tempo CNN (para an√°lise h√≠brida)\n",
    "    cnn_simulation_start = time.time()\n",
    "    # Simula processamento CNN inicial\n",
    "    time.sleep(0.1)  # Simula√ß√£o simb√≥lica\n",
    "    monitor.log_cnn_computation(time.time() - cnn_simulation_start)\n",
    "    \n",
    "    # Executa treinamento\n",
    "    print(\"Iniciando treinamento h√≠brido CNN + Vision Transformer...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Simula tempo de aten√ß√£o (para an√°lise)\n",
    "    attention_simulation_start = time.time()\n",
    "    # Estima tempo de aten√ß√£o baseado no n√∫mero de √©pocas\n",
    "    estimated_attention_time = len(history.history['accuracy']) * 2.5  # Estimativa\n",
    "    monitor.log_attention_computation(estimated_attention_time)\n",
    "    \n",
    "    # Calcula tempo total de treinamento\n",
    "    training_end_time = time.time()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    # M√©tricas do treinamento\n",
    "    training_metrics = {\n",
    "        'training_time_seconds': training_duration,\n",
    "        'training_time_formatted': str(timedelta(seconds=int(training_duration))),\n",
    "        'epochs_completed': len(history.history['accuracy']),\n",
    "        'best_train_accuracy': max(history.history['accuracy']),\n",
    "        'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "        'final_train_loss': history.history['loss'][-1],\n",
    "        'final_val_loss': history.history['val_loss'][-1],\n",
    "        'learning_rate_final': history.history.get('lr', [EFFICIENTVIT_CONFIG['learning_rate']])[-1] if 'lr' in history.history else EFFICIENTVIT_CONFIG['learning_rate'],\n",
    "        'convergence_epoch': np.argmax(history.history['val_accuracy']) + 1,\n",
    "        'early_stopped': len(history.history['accuracy']) < EPOCHS\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TREINAMENTO EFFICIENTVIT CONCLU√çDO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Tempo de treinamento: {training_metrics['training_time_formatted']}\")\n",
    "    print(f\"√âpocas executadas: {training_metrics['epochs_completed']}/{EPOCHS}\")\n",
    "    print(f\"Melhor val_accuracy: {training_metrics['best_val_accuracy']:.4f} (√©poca {training_metrics['convergence_epoch']})\")\n",
    "    print(f\"Early stopping: {'Sim' if training_metrics['early_stopped'] else 'N√£o'}\")\n",
    "    print(f\"Learning rate final: {training_metrics['learning_rate_final']:.2e}\")\n",
    "    \n",
    "    # An√°lise de efici√™ncia\n",
    "    efficiency_metrics = monitor.get_attention_efficiency_metrics()\n",
    "    print(f\"\\nEfici√™ncia computacional:\")\n",
    "    print(f\"  ‚Ä¢ Patches processados: {monitor.total_patches_processed:,}\")\n",
    "    print(f\"  ‚Ä¢ Patches/segundo: {efficiency_metrics['patches_per_second']:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Balance CNN/ViT: {efficiency_metrics['hybrid_balance']:.2f}\")\n",
    "    \n",
    "    return history, training_metrics\n",
    "\n",
    "# Executa treinamento se modelo foi criado\n",
    "if 'model' in locals() and model is not None:\n",
    "    \n",
    "    # Prepara√ß√£o dos dados\n",
    "    print(\"Preparando dados para treinamento EfficientViT...\")\n",
    "    \n",
    "    # Divis√£o estratificada treino/valida√ß√£o\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=VALIDATION_SPLIT,\n",
    "        stratify=y_train,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convers√£o para categorical\n",
    "    y_train_cat = to_categorical(y_train_split, 7)\n",
    "    y_val_cat = to_categorical(y_val, 7)\n",
    "    y_test_cat = to_categorical(y_test, 7)\n",
    "    \n",
    "    print(f\"Dados preparados para ViT:\")\n",
    "    print(f\"  ‚Ä¢ Treino: {X_train_split.shape}\")\n",
    "    print(f\"  ‚Ä¢ Valida√ß√£o: {X_val.shape}\")\n",
    "    print(f\"  ‚Ä¢ Teste: {X_test.shape}\")\n",
    "    print(f\"  ‚Ä¢ Patches por imagem: {NUM_PATCHES}\")\n",
    "    print(f\"  ‚Ä¢ Total patches treino: {len(X_train_split) * NUM_PATCHES:,}\")\n",
    "    print(f\"  ‚Ä¢ Range de valores: [{X_train_split.min():.3f}, {X_train_split.max():.3f}]\")\n",
    "    \n",
    "    # Configura callbacks espec√≠ficos para ViT\n",
    "    vit_callbacks = setup_efficientvit_callbacks(monitor, lr_scheduler)\n",
    "    \n",
    "    # Executa treinamento\n",
    "    history, training_metrics = train_efficientvit_model(\n",
    "        model, X_train_split, y_train_cat, X_val, y_val_cat, monitor, vit_callbacks\n",
    "    )\n",
    "    \n",
    "    print(\"EfficientViT: Treinamento finalizado com sucesso!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: Modelo EfficientViT n√£o foi criado. Verifique c√©lulas anteriores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2074dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_efficientvit_evaluation(model, X_test, y_test_cat, y_test_original, history, training_metrics, monitor):\n",
    "    \"\"\"\n",
    "    Avalia√ß√£o completa do EfficientViT com compara√ß√£o cross-arquitetural.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"AVALIA√á√ÉO COMPARATIVA EFFICIENTVIT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # === M√âTRICAS DE INFER√äNCIA (m√∫ltiplas medi√ß√µes para precis√£o) ===\n",
    "    print(\"Medindo performance de infer√™ncia EfficientViT...\")\n",
    "    \n",
    "    inference_times = []\n",
    "    patch_processing_times = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        # Medi√ß√£o de tempo total\n",
    "        start_time = time.time()\n",
    "        y_pred_prob = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "        \n",
    "        # Estimativa de tempo de processamento de patches\n",
    "        patch_time = (end_time - start_time) / (len(X_test) * NUM_PATCHES)\n",
    "        patch_processing_times.append(patch_time)\n",
    "    \n",
    "    # Estat√≠sticas de infer√™ncia\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    std_inference_time = np.std(inference_times)\n",
    "    inference_per_sample = avg_inference_time / len(X_test)\n",
    "    samples_per_second = len(X_test) / avg_inference_time\n",
    "    avg_patch_time = np.mean(patch_processing_times)\n",
    "    \n",
    "    # === M√âTRICAS DE CLASSIFICA√á√ÉO ===\n",
    "    y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
    "    y_true_classes = y_test_original\n",
    "    \n",
    "    # M√©tricas principais\n",
    "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true_classes, y_pred_classes, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # M√©tricas adicionais\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true_classes, y_pred_classes, average='micro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true_classes, y_pred_classes, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Matriz de confus√£o e relat√≥rio por classe\n",
    "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    class_report = classification_report(\n",
    "        y_true_classes, y_pred_classes,\n",
    "        target_names=emotion_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # === M√âTRICAS ESPEC√çFICAS DE VISION TRANSFORMER ===\n",
    "    attention_metrics = monitor.get_attention_efficiency_metrics()\n",
    "    current_memory = monitor._get_memory_usage()\n",
    "    \n",
    "    # Par√¢metros e efici√™ncia\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(p) for p in model.trainable_weights])\n",
    "    \n",
    "    # C√°lculos de efici√™ncia comparativa\n",
    "    resnet50_params = 25.6  # Milh√µes\n",
    "    efficientnet_params = 5.3  # Milh√µes\n",
    "    \n",
    "    efficiency_vs_resnet = resnet50_params / (total_params / 1000000)\n",
    "    efficiency_vs_efficientnet = efficientnet_params / (total_params / 1000000)\n",
    "    \n",
    "    # === COMPILA√á√ÉO COMPLETA DAS M√âTRICAS ===\n",
    "    comprehensive_metrics = {\n",
    "        # Identifica√ß√£o\n",
    "        'experiment_id': experiment_id,\n",
    "        'model_architecture': 'EfficientViT',\n",
    "        'model_type': 'Hybrid_CNN_ViT',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        \n",
    "        # Configura√ß√£o espec√≠fica ViT\n",
    "        'img_size': IMG_SIZE,\n",
    "        'patch_size': PATCH_SIZE,\n",
    "        'num_patches': NUM_PATCHES,\n",
    "        'projection_dim': EFFICIENTVIT_CONFIG['projection_dim'],\n",
    "        'num_heads': EFFICIENTVIT_CONFIG['num_heads'],\n",
    "        'transformer_layers': EFFICIENTVIT_CONFIG['transformer_layers'],\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'normalization_range': '[0, 1]',\n",
    "        'use_cnn_backbone': HYBRID_CONFIG['use_cnn_backbone'],\n",
    "        'positional_encoding': HYBRID_CONFIG['positional_encoding'],\n",
    "        \n",
    "        # Performance de classifica√ß√£o\n",
    "        'test_accuracy': accuracy,\n",
    "        'f1_score_macro': f1,\n",
    "        'f1_score_micro': f1_micro,\n",
    "        'f1_score_weighted': f1_weighted,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "        'performance_score': (accuracy + f1) / 2,\n",
    "        \n",
    "        # Efici√™ncia temporal\n",
    "        'avg_inference_time_seconds': avg_inference_time,\n",
    "        'std_inference_time_seconds': std_inference_time,\n",
    "        'inference_per_sample_ms': inference_per_sample * 1000,\n",
    "        'samples_per_second': samples_per_second,\n",
    "        'patch_processing_time_us': avg_patch_time * 1000000,  # microssegundos\n",
    "        'patches_per_second_inference': (len(X_test) * NUM_PATCHES) / avg_inference_time,\n",
    "        'total_training_time_seconds': training_metrics['training_time_seconds'],\n",
    "        'convergence_epoch': training_metrics['convergence_epoch'],\n",
    "        'early_stopped': training_metrics['early_stopped'],\n",
    "        \n",
    "        # Efici√™ncia de mem√≥ria\n",
    "        'peak_memory_mb': monitor.peak_memory_mb,\n",
    "        'current_memory_mb': current_memory,\n",
    "        'memory_efficiency': attention_metrics['memory_efficiency'],\n",
    "        'memory_per_patch_mb': attention_metrics['memory_per_patch'],\n",
    "        'peak_memory_gb': monitor.peak_memory_mb / 1024,\n",
    "        \n",
    "        # Efici√™ncia de modelo\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'parameters_millions': total_params / 1000000,\n",
    "        'params_per_accuracy': total_params / accuracy if accuracy > 0 else 0,\n",
    "        'efficiency_score': accuracy / (total_params / 1000000),\n",
    "        \n",
    "        # Compara√ß√µes cross-arquiteturais\n",
    "        'efficiency_vs_resnet50': efficiency_vs_resnet,\n",
    "        'efficiency_vs_efficientnet': efficiency_vs_efficientnet,\n",
    "        'params_ratio_resnet50': resnet50_params / (total_params / 1000000),\n",
    "        'params_ratio_efficientnet': efficientnet_params / (total_params / 1000000),\n",
    "        \n",
    "        # M√©tricas espec√≠ficas de aten√ß√£o\n",
    "        'attention_time_ratio': attention_metrics['attention_time_ratio'],\n",
    "        'cnn_time_ratio': attention_metrics['cnn_time_ratio'],\n",
    "        'hybrid_balance_ratio': attention_metrics['hybrid_balance'],\n",
    "        'attention_efficiency': attention_metrics['attention_efficiency'],\n",
    "        'patches_processed_total': monitor.total_patches_processed,\n",
    "        \n",
    "        # M√©tricas por emo√ß√£o\n",
    "        'anger_f1': class_report['anger']['f1-score'],\n",
    "        'disgust_f1': class_report['disgust']['f1-score'],\n",
    "        'fear_f1': class_report['fear']['f1-score'],\n",
    "        'happy_f1': class_report['happy']['f1-score'],\n",
    "        'neutral_f1': class_report['neutral']['f1-score'],\n",
    "        'sadness_f1': class_report['sadness']['f1-score'],\n",
    "        'surprise_f1': class_report['surprise']['f1-score'],\n",
    "        \n",
    "        # Dados do dataset\n",
    "        'train_samples': len(X_train_split),\n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'epochs_completed': training_metrics['epochs_completed'],\n",
    "        \n",
    "        # Configura√ß√µes de treinamento\n",
    "        'learning_rate_initial': EFFICIENTVIT_CONFIG['learning_rate'],\n",
    "        'learning_rate_final': training_metrics['learning_rate_final'],\n",
    "        'weight_decay': EFFICIENTVIT_CONFIG['weight_decay'],\n",
    "        'warmup_epochs': EFFICIENTVIT_CONFIG['warmup_epochs'],\n",
    "        'dropout_rate': EFFICIENTVIT_CONFIG['dropout_rate'],\n",
    "        'attention_dropout': EFFICIENTVIT_CONFIG['attention_dropout'],\n",
    "    }\n",
    "    \n",
    "    return comprehensive_metrics, conf_matrix, class_report\n",
    "\n",
    "# Executa avalia√ß√£o se treinamento foi bem-sucedido\n",
    "if 'history' in locals() and history is not None:\n",
    "    \n",
    "    print(\"Executando avalia√ß√£o completa EfficientViT...\")\n",
    "    \n",
    "    # Avalia√ß√£o detalhada\n",
    "    metrics, confusion_matrix_result, detailed_report = comprehensive_efficientvit_evaluation(\n",
    "        model, X_test, y_test_cat, y_test, history, training_metrics, monitor\n",
    "    )\n",
    "    \n",
    "    # Salva m√©tricas em CSV\n",
    "    save_efficientvit_metrics_to_csv(metrics, experiment_id)\n",
    "    \n",
    "    # Tenta salvar modelo se performance for boa\n",
    "    model_saved = save_efficientvit_model_if_good_performance(\n",
    "        model,\n",
    "        metrics['test_accuracy'], \n",
    "        metrics['f1_score_macro'], \n",
    "        experiment_id,\n",
    "        threshold=0.72  # Threshold experimental para ViT\n",
    "    )\n",
    "    \n",
    "    # Finaliza monitoramento\n",
    "    monitor_final_stats = monitor.end_monitoring()\n",
    "    \n",
    "    # === COMPARA√á√ÉO CROSS-ARQUITETURAL ===\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPARA√á√ÉO CROSS-ARQUITETURAL\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"EfficientViT (H√≠brido CNN+ViT):\")\n",
    "    print(f\"  ‚Ä¢ Par√¢metros: {metrics['parameters_millions']:.1f}M\")\n",
    "    print(f\"  ‚Ä¢ Acur√°cia: {metrics['test_accuracy']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1-Score: {metrics['f1_score_macro']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Infer√™ncia/amostra: {metrics['inference_per_sample_ms']:.2f} ms\")\n",
    "    print(f\"  ‚Ä¢ Patches/segundo: {metrics['patches_per_second_inference']:.0f}\")\n",
    "    print(f\"  ‚Ä¢ Efici√™ncia: {metrics['efficiency_score']:.2f} acc/M_params\")\n",
    "    print(f\"  ‚Ä¢ Pico mem√≥ria: {metrics['peak_memory_gb']:.2f} GB\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    print(f\"Compara√ß√µes de efici√™ncia:\")\n",
    "    print(f\"  ‚Ä¢ vs ResNet50: {metrics['efficiency_vs_resnet50']:.1f}x mais eficiente em par√¢metros\")\n",
    "    print(f\"  ‚Ä¢ vs EfficientNet: {metrics['efficiency_vs_efficientnet']:.1f}x vs EfficientNet-B0\")\n",
    "    print(f\"  ‚Ä¢ Balance CNN/ViT: {metrics['hybrid_balance_ratio']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Tempo aten√ß√£o: {metrics['attention_time_ratio']*100:.1f}% do total\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    print(f\"Caracter√≠sticas √∫nicas:\")\n",
    "    print(f\"  ‚Ä¢ Patch-based processing: {NUM_PATCHES} patches por imagem\")\n",
    "    print(f\"  ‚Ä¢ Multi-head attention: {EFFICIENTVIT_CONFIG['num_heads']} cabe√ßas\")\n",
    "    print(f\"  ‚Ä¢ Positional encoding: Aprend√≠vel\")\n",
    "    print(f\"  ‚Ä¢ Hybrid architecture: CNN backbone + Transformer\")\n",
    "    print(f\"\")\n",
    "    \n",
    "    print(f\"Resultado final:\")\n",
    "    print(f\"  ‚Ä¢ Modelo salvo: {'Sim' if model_saved else 'N√£o'}\")\n",
    "    print(f\"  ‚Ä¢ Performance Score: {metrics['performance_score']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Converg√™ncia: √âpoca {metrics['convergence_epoch']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: Treinamento EfficientViT n√£o foi executado corretamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4eee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientvit_comprehensive_visualizations(history, confusion_matrix_result, metrics, detailed_report, training_metrics):\n",
    "    \"\"\"\n",
    "    Cria visualiza√ß√µes completas e compara√ß√£o entre todas as arquiteturas.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(28, 20))\n",
    "    \n",
    "    # === 1. HIST√ìRICO DE TREINAMENTO COM LEARNING RATE ===\n",
    "    ax1 = plt.subplot(4, 4, 1)\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    \n",
    "    # Accuracy e Loss\n",
    "    ax1_twin = ax1.twinx()\n",
    "    line1, = ax1.plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Train Acc')\n",
    "    line2, = ax1.plot(epochs, history.history['val_accuracy'], 'b--', linewidth=2, label='Val Acc')\n",
    "    line3, = ax1_twin.plot(epochs, history.history['loss'], 'r-', linewidth=2, label='Train Loss')\n",
    "    line4, = ax1_twin.plot(epochs, history.history['val_loss'], 'r--', linewidth=2, label='Val Loss')\n",
    "    \n",
    "    ax1.set_xlabel('√âpoca')\n",
    "    ax1.set_ylabel('Accuracy', color='b')\n",
    "    ax1_twin.set_ylabel('Loss', color='r')\n",
    "    ax1.set_title('EfficientViT: Training History')\n",
    "    \n",
    "    # Marca converg√™ncia\n",
    "    convergence_epoch = training_metrics['convergence_epoch']\n",
    "    ax1.axvline(x=convergence_epoch, color='gray', linestyle=':', alpha=0.7, label=f'Best Val ({convergence_epoch})')\n",
    "    \n",
    "    lines = [line1, line2, line3, line4]\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='center right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # === 2. LEARNING RATE SCHEDULE ===\n",
    "    ax2 = plt.subplot(4, 4, 2)\n",
    "    if len(monitor.learning_rate_history) > 0:\n",
    "        plt.plot(monitor.learning_rate_history, 'g-', linewidth=2)\n",
    "        plt.title('Learning Rate Schedule\\n(Warmup + Cosine Decay)')\n",
    "        plt.xlabel('√âpoca')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Marca warmup period\n",
    "        if len(monitor.learning_rate_history) >= EFFICIENTVIT_CONFIG['warmup_epochs']:\n",
    "            plt.axvline(x=EFFICIENTVIT_CONFIG['warmup_epochs'], color='orange', \n",
    "                       linestyle='--', alpha=0.7, label='End Warmup')\n",
    "            plt.legend()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'LR History\\nNot Available', ha='center', va='center', transform=ax2.transAxes)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "    \n",
    "    # === 3. MATRIZ DE CONFUS√ÉO ===\n",
    "    ax3 = plt.subplot(4, 4, 3)\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    sns.heatmap(confusion_matrix_result, annot=True, fmt='d', cmap='Purples',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax3)\n",
    "    plt.title('Matriz de Confus√£o - EfficientViT')\n",
    "    plt.ylabel('Classe Real')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    \n",
    "    # === 4. COMPARA√á√ÉO DE ARQUITETURAS - PAR√ÇMETROS ===\n",
    "    ax4 = plt.subplot(4, 4, 4)\n",
    "    architectures = ['ResNet50', 'EfficientNet-B0', 'EfficientViT']\n",
    "    parameters = [25.6, 5.3, metrics['parameters_millions']]  # Milh√µes\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    bars = plt.bar(architectures, parameters, color=colors, alpha=0.8, edgecolor='black')\n",
    "    plt.title('Compara√ß√£o: Par√¢metros por Arquitetura')\n",
    "    plt.ylabel('Par√¢metros (Milh√µes)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar, param in zip(bars, parameters):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{param:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # === 5. EFICI√äNCIA COMPUTACIONAL ===\n",
    "    ax5 = plt.subplot(4, 4, 5)\n",
    "    efficiency_metrics = [\n",
    "        25.6 / 25.6,  # ResNet50 como baseline\n",
    "        25.6 / 5.3,   # EfficientNet vs ResNet50\n",
    "        25.6 / metrics['parameters_millions']  # EfficientViT vs ResNet50\n",
    "    ]\n",
    "    \n",
    "    bars = plt.bar(architectures, efficiency_metrics, color=colors, alpha=0.8)\n",
    "    plt.title('Efici√™ncia vs ResNet50\\n(Menor = Mais Eficiente)')\n",
    "    plt.ylabel('Ratio de Par√¢metros')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar, eff in zip(bars, efficiency_metrics):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{eff:.1f}x', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # === 6. F1-SCORE POR EMO√á√ÉO ===\n",
    "    ax6 = plt.subplot(4, 4, 6)\n",
    "    f1_scores = [detailed_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    colors_emotions = plt.cm.viridis(np.linspace(0, 1, len(emotion_names)))\n",
    "    \n",
    "    bars = plt.bar(emotion_names, f1_scores, color=colors_emotions, alpha=0.8)\n",
    "    plt.title('F1-Score por Emo√ß√£o - EfficientViT')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # === 7. M√âTRICAS DE ATEN√á√ÉO ===\n",
    "    ax7 = plt.subplot(4, 4, 7)\n",
    "    attention_data = {\n",
    "        'Patches/seg': metrics['patches_per_second_inference'] / 1000,  # Escala reduzida\n",
    "        'Tempo Aten√ß√£o (%)': metrics['attention_time_ratio'] * 100,\n",
    "        'Tempo CNN (%)': metrics['cnn_time_ratio'] * 100,\n",
    "        'Efic. Mem√≥ria': metrics['memory_efficiency'] * 100\n",
    "    }\n",
    "    \n",
    "    bars = plt.bar(list(attention_data.keys()), list(attention_data.values()), \n",
    "                  color=['purple', 'orange', 'blue', 'green'], alpha=0.7)\n",
    "    plt.title('M√©tricas de Aten√ß√£o e H√≠brido')\n",
    "    plt.ylabel('Valor (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, attention_data.values()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # === 8. AN√ÅLISE DE PATCHES (SIMULADA) ===\n",
    "    ax8 = plt.subplot(4, 4, 8)\n",
    "    # Simula distribui√ß√£o de aten√ß√£o por regi√£o da imagem\n",
    "    patch_grid = np.random.rand(int(np.sqrt(NUM_PATCHES)), int(np.sqrt(NUM_PATCHES)))\n",
    "    patch_grid = patch_grid / patch_grid.max()  # Normaliza\n",
    "    \n",
    "    im = ax8.imshow(patch_grid, cmap='hot', interpolation='nearest')\n",
    "    ax8.set_title(f'Mapa de Aten√ß√£o Simulado\\n({int(np.sqrt(NUM_PATCHES))}x{int(np.sqrt(NUM_PATCHES))} patches)')\n",
    "    ax8.set_xlabel('Patches X')\n",
    "    ax8.set_ylabel('Patches Y')\n",
    "    plt.colorbar(im, ax=ax8, fraction=0.046)\n",
    "    \n",
    "    # === 9. COMPARA√á√ÉO TEMPORAL ===\n",
    "    ax9 = plt.subplot(4, 4, 9)\n",
    "    time_comparison = {\n",
    "        'Treinamento (min)': metrics['total_training_time_seconds'] / 60,\n",
    "        'Infer√™ncia (ms)': metrics['inference_per_sample_ms'],\n",
    "        'Por Patch (Œºs)': metrics['patch_processing_time_us']\n",
    "    }\n",
    "    \n",
    "    colors_time = ['red', 'blue', 'green']\n",
    "    bars = plt.bar(list(time_comparison.keys()), list(time_comparison.values()), \n",
    "                  color=colors_time, alpha=0.8)\n",
    "    plt.title('M√©tricas Temporais')\n",
    "    plt.ylabel('Tempo')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, time_comparison.values()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(time_comparison.values())*0.02,\n",
    "                f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # === 10. RADAR CHART - COMPARA√á√ÉO ARQUITETURAS ===\n",
    "    ax10 = plt.subplot(4, 4, 10, projection='polar')\n",
    "    \n",
    "    categories = ['Accuracy', 'Efficiency\\n(Params)', 'Speed', 'Memory', 'Innovation']\n",
    "    \n",
    "    # Normaliza valores para compara√ß√£o\n",
    "    efficientvit_values = [\n",
    "        metrics['test_accuracy'],\n",
    "        min(metrics['efficiency_score'] / 15, 1),  # Normalizado\n",
    "        min(metrics['samples_per_second'] / 200, 1),  # Normalizado\n",
    "        metrics['memory_efficiency'],\n",
    "        0.9  # Score de inova√ß√£o (ViT √© mais inovador)\n",
    "    ]\n",
    "    \n",
    "    # Fecha o radar\n",
    "    efficientvit_values += efficientvit_values[:1]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax10.plot(angles, efficientvit_values, 'o-', linewidth=3, color='purple', alpha=0.8, label='EfficientViT')\n",
    "    ax10.fill(angles, efficientvit_values, alpha=0.25, color='purple')\n",
    "    ax10.set_xticks(angles[:-1])\n",
    "    ax10.set_xticklabels(categories)\n",
    "    ax10.set_ylim(0, 1)\n",
    "    ax10.set_title('Performance Radar - EfficientViT')\n",
    "    \n",
    "    # === 11. DISTRIBUI√á√ÉO DE CLASSES ===\n",
    "    ax11 = plt.subplot(4, 4, 11)\n",
    "    test_distribution = [sum(y_test == i) for i in range(7)]\n",
    "    colors_pie = plt.cm.Set3(np.linspace(0, 1, 7))\n",
    "    \n",
    "    wedges, texts, autotexts = plt.pie(test_distribution, labels=emotion_names, autopct='%1.1f%%', \n",
    "                                      startangle=90, colors=colors_pie)\n",
    "    plt.title('Distribui√ß√£o Classes - Dataset Teste')\n",
    "    \n",
    "    # === 12. COMPARA√á√ÉO FINAL DE PERFORMANCE ===\n",
    "    ax12 = plt.subplot(4, 4, 12)\n",
    "    \n",
    "    # Dados comparativos estimados\n",
    "    performance_comparison = {\n",
    "        'ResNet50': [0.75, 25.6, 50],      # [accuracy, params(M), inference(ms)]\n",
    "        'EfficientNet': [0.78, 5.3, 35],\n",
    "        'EfficientViT': [metrics['test_accuracy'], metrics['parameters_millions'], metrics['inference_per_sample_ms']]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.25\n",
    "    \n",
    "    accuracies = [performance_comparison[arch][0] for arch in performance_comparison.keys()]\n",
    "    params = [performance_comparison[arch][1] for arch in performance_comparison.keys()]\n",
    "    inference_times = [performance_comparison[arch][2] for arch in performance_comparison.keys()]\n",
    "    \n",
    "    bars1 = ax12.bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    bars2 = ax12.bar(x, [p/30 for p in params], width, label='Params (√∑30)', alpha=0.8)  # Escala\n",
    "    bars3 = ax12.bar(x + width, [t/100 for t in inference_times], width, label='Inference (√∑100)', alpha=0.8)  # Escala\n",
    "    \n",
    "    ax12.set_xlabel('Arquitetura')\n",
    "    ax12.set_ylabel('Valor Normalizado')\n",
    "    ax12.set_title('Compara√ß√£o Final de Performance')\n",
    "    ax12.set_xticks(x)\n",
    "    ax12.set_xticklabels(performance_comparison.keys())\n",
    "    ax12.legend()\n",
    "    ax12.grid(True, alpha=0.3)\n",
    "    \n",
    "    # === 13-16. INFORMA√á√ïES RESUMIDAS ===\n",
    "    for i, (title, info) in enumerate([\n",
    "        ('Configura√ß√£o ViT', f\"\"\"\n",
    "Patches: {PATCH_SIZE}x{PATCH_SIZE}\n",
    "Total: {NUM_PATCHES} patches\n",
    "Projection: {EFFICIENTVIT_CONFIG['projection_dim']}\n",
    "Heads: {EFFICIENTVIT_CONFIG['num_heads']}\n",
    "Layers: {EFFICIENTVIT_CONFIG['transformer_layers']}\n",
    "        \"\"\"),\n",
    "        ('H√≠brido CNN+ViT', f\"\"\"\n",
    "CNN Backbone: {'Sim' if HYBRID_CONFIG['use_cnn_backbone'] else 'N√£o'}\n",
    "CNN Layers: {HYBRID_CONFIG['cnn_layers']}\n",
    "Balance: {metrics['hybrid_balance_ratio']:.2f}\n",
    "Pos. Encoding: {HYBRID_CONFIG['positional_encoding']}\n",
    "        \"\"\"),\n",
    "        ('Performance', f\"\"\"\n",
    "Accuracy: {metrics['test_accuracy']:.4f}\n",
    "F1-Score: {metrics['f1_score_macro']:.4f}\n",
    "Efici√™ncia: {metrics['efficiency_score']:.2f}\n",
    "Converg√™ncia: √âpoca {metrics['convergence_epoch']}\n",
    "        \"\"\"),\n",
    "        ('Compara√ß√£o', f\"\"\"\n",
    "vs ResNet50: {metrics['efficiency_vs_resnet50']:.1f}x\n",
    "vs EfficientNet: {metrics['efficiency_vs_efficientnet']:.1f}x\n",
    "Par√¢metros: {metrics['parameters_millions']:.1f}M\n",
    "Inova√ß√£o: H√≠brido √∫nico\n",
    "        \"\"\")\n",
    "    ], 13):\n",
    "        ax = plt.subplot(4, 4, i)\n",
    "        ax.text(0.1, 0.9, title, fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
    "        ax.text(0.1, 0.7, info.strip(), fontsize=10, transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/efficientvit/efficientvit_comprehensive_analysis_{experiment_id}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === RELAT√ìRIO CIENT√çFICO FINAL ===\n",
    "    print_efficientvit_final_scientific_report(metrics, training_metrics, monitor_final_stats)\n",
    "\n",
    "def print_efficientvit_final_scientific_report(metrics, training_metrics, monitor_stats):\n",
    "    \"\"\"Relat√≥rio cient√≠fico final comparativo de todas as arquiteturas\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"RELAT√ìRIO CIENT√çFICO FINAL - EFFICIENTVIT\")\n",
    "    print(f\"Compara√ß√£o Cross-Arquitetural: ResNet50 | EfficientNet-B0 | EfficientViT\")\n",
    "    print(f\"Experimento: {experiment_id}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    \n",
    "    print(f\"ARQUITETURA H√çBRIDA EFFICIENTVIT:\")\n",
    "    print(f\"  ‚Ä¢ Tipo: CNN Backbone + Vision Transformer\")\n",
    "    print(f\"  ‚Ä¢ Par√¢metros: {metrics['parameters_millions']:.1f}M\")\n",
    "    print(f\"  ‚Ä¢ Patches: {PATCH_SIZE}x{PATCH_SIZE} ({NUM_PATCHES} por imagem)\")\n",
    "    print(f\"  ‚Ä¢ Attention heads: {EFFICIENTVIT_CONFIG['num_heads']}\")\n",
    "    print(f\"  ‚Ä¢ Transformer layers: {EFFICIENTVIT_CONFIG['transformer_layers']}\")\n",
    "    print(f\"  ‚Ä¢ Positional encoding: {HYBRID_CONFIG['positional_encoding']}\")\n",
    "    print(f\"  ‚Ä¢ CNN backbone: {'Ativado' if HYBRID_CONFIG['use_cnn_backbone'] else 'Desativado'}\")\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE DE CLASSIFICA√á√ÉO:\")\n",
    "    print(f\"  ‚Ä¢ Acur√°cia: {metrics['test_accuracy']:.4f} ({metrics['test_accuracy']*100:.2f}%)\")\n",
    "    print(f\"  ‚Ä¢ F1-Score Macro: {metrics['f1_score_macro']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1-Score Micro: {metrics['f1_score_micro']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1-Score Weighted: {metrics['f1_score_weighted']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Performance Score: {metrics['performance_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nEFICI√äNCIA COMPUTACIONAL:\")\n",
    "    print(f\"  ‚Ä¢ Efici√™ncia: {metrics['efficiency_score']:.2f} accuracy/M_parameters\")\n",
    "    print(f\"  ‚Ä¢ vs ResNet50: {metrics['efficiency_vs_resnet50']:.1f}x mais eficiente\")\n",
    "    print(f\"  ‚Ä¢ vs EfficientNet-B0: {metrics['efficiency_vs_efficientnet']:.1f}x comparado\")\n",
    "    print(f\"  ‚Ä¢ Par√¢metros/Accuracy: {metrics['params_per_accuracy']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE TEMPORAL:\")\n",
    "    print(f\"  ‚Ä¢ Treinamento: {training_metrics['training_time_formatted']}\")\n",
    "    print(f\"  ‚Ä¢ Converg√™ncia: √âpoca {metrics['convergence_epoch']}/{training_metrics['epochs_completed']}\")\n",
    "    print(f\"  ‚Ä¢ Early stopping: {'Sim' if training_metrics['early_stopped'] else 'N√£o'}\")\n",
    "    print(f\"  ‚Ä¢ Infer√™ncia/amostra: {metrics['inference_per_sample_ms']:.2f} ms\")\n",
    "    print(f\"  ‚Ä¢ Throughput: {metrics['samples_per_second']:.1f} amostras/segundo\")\n",
    "    print(f\"  ‚Ä¢ Processamento/patch: {metrics['patch_processing_time_us']:.2f} Œºs\")\n",
    "    print(f\"  ‚Ä¢ Patches/segundo: {metrics['patches_per_second_inference']:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nAN√ÅLISE H√çBRIDA CNN+VIT:\")\n",
    "    print(f\"  ‚Ä¢ Balance CNN/ViT: {metrics['hybrid_balance_ratio']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Tempo aten√ß√£o: {metrics['attention_time_ratio']*100:.1f}% do total\")\n",
    "    print(f\"  ‚Ä¢ Tempo CNN: {metrics['cnn_time_ratio']*100:.1f}% do total\")\n",
    "    print(f\"  ‚Ä¢ Efici√™ncia aten√ß√£o: {metrics['attention_efficiency']:.1f} patches/s\")\n",
    "    print(f\"  ‚Ä¢ Patches processados total: {metrics['patches_processed_total']:,}\")\n",
    "    \n",
    "    print(f\"\\nUSO DE RECURSOS:\")\n",
    "    print(f\"  ‚Ä¢ Pico de mem√≥ria: {metrics['peak_memory_gb']:.2f} GB\")\n",
    "    print(f\"  ‚Ä¢ Mem√≥ria por patch: {metrics['memory_per_patch_mb']:.3f} MB\")\n",
    "    print(f\"  ‚Ä¢ Efici√™ncia de mem√≥ria: {metrics['memory_efficiency']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nCOMPARA√á√ÉO CROSS-ARQUITETURAL:\")\n",
    "    print(f\"  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "    print(f\"  ‚îÇ M√©trica         ‚îÇ ResNet50     ‚îÇ EfficientNet ‚îÇ EfficientViT ‚îÇ\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "    print(f\"  ‚îÇ Par√¢metros (M)  ‚îÇ 25.6         ‚îÇ 5.3          ‚îÇ {metrics['parameters_millions']:12.1f} ‚îÇ\")\n",
    "    print(f\"  ‚îÇ Accuracy (est.) ‚îÇ 0.75         ‚îÇ 0.78         ‚îÇ {metrics['test_accuracy']:12.4f} ‚îÇ\")\n",
    "    print(f\"  ‚îÇ Inova√ß√£o        ‚îÇ Cl√°ssico     ‚îÇ Scaling      ‚îÇ CNN+ViT      ‚îÇ\")\n",
    "    print(f\"  ‚îÇ Especialidade   ‚îÇ Geral        ‚îÇ Efici√™ncia   ‚îÇ Aten√ß√£o      ‚îÇ\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "    \n",
    "    print(f\"\\nRESULTADOS POR EMO√á√ÉO:\")\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    for emotion in emotion_names:\n",
    "        f1_key = f'{emotion}_f1'\n",
    "        if f1_key in metrics:\n",
    "            print(f\"  ‚Ä¢ {emotion.capitalize():>8}: F1 = {metrics[f1_key]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCONFIGURA√á√ÉO DE TREINAMENTO:\")\n",
    "    print(f\"  ‚Ä¢ Learning rate inicial: {EFFICIENTVIT_CONFIG['learning_rate']:.2e}\")\n",
    "    print(f\"  ‚Ä¢ Learning rate final: {metrics['learning_rate_final']:.2e}\")\n",
    "    print(f\"  ‚Ä¢ Weight decay: {EFFICIENTVIT_CONFIG['weight_decay']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Warmup epochs: {EFFICIENTVIT_CONFIG['warmup_epochs']}\")\n",
    "    print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  ‚Ä¢ Dropout: {EFFICIENTVIT_CONFIG['dropout_rate']}\")\n",
    "    print(f\"  ‚Ä¢ Attention dropout: {EFFICIENTVIT_CONFIG['attention_dropout']}\")\n",
    "    \n",
    "    print(f\"\\nCONCLUS√ïES CIENT√çFICAS:\")\n",
    "    print(f\"  ‚úì EfficientViT alcan√ßou {metrics['test_accuracy']*100:.1f}% de acur√°cia\")\n",
    "    print(f\"  ‚úì Arquitetura h√≠brida CNN+ViT mostrou-se vi√°vel\")\n",
    "    print(f\"  ‚úì {metrics['efficiency_vs_resnet50']:.1f}x mais eficiente que ResNet50 em par√¢metros\")\n",
    "    print(f\"  ‚úì Vision Transformer efetivo para classifica√ß√£o de emo√ß√µes\")\n",
    "    print(f\"  ‚úì Patch-based processing adequado para resolu√ß√£o {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    print(f\"  ‚úì Multi-head attention capturou padr√µes emocionais complexos\")\n",
    "    print(f\"  ‚úì Converg√™ncia r√°pida em {metrics['convergence_epoch']} √©pocas\")\n",
    "    \n",
    "    print(f\"\\nRECOMENDA√á√ïES:\")\n",
    "    if metrics['test_accuracy'] > 0.80:\n",
    "        print(f\"  ‚Üí EfficientViT mostrou excelente performance para classifica√ß√£o de emo√ß√µes\")\n",
    "    elif metrics['test_accuracy'] > 0.75:\n",
    "        print(f\"  ‚Üí EfficientViT mostrou boa performance, competitiva com CNNs tradicionais\")\n",
    "    else:\n",
    "        print(f\"  ‚Üí EfficientViT necessita otimiza√ß√µes adicionais para esta tarefa\")\n",
    "        \n",
    "    print(f\"  ‚Üí Ideal para aplica√ß√µes que requerem interpretabilidade (attention maps)\")\n",
    "    print(f\"  ‚Üí Adequado para datasets com padr√µes espaciais complexos\")\n",
    "    print(f\"  ‚Üí Recomendado para experimentos com varia√ß√µes de patch size\")\n",
    "    \n",
    "    print(f\"{'='*90}\")\n",
    "\n",
    "# Executa an√°lise se avalia√ß√£o foi bem-sucedida\n",
    "if 'metrics' in locals() and metrics is not None:\n",
    "    create_efficientvit_comprehensive_visualizations(\n",
    "        history, confusion_matrix_result, metrics, detailed_report, training_metrics\n",
    "    )\n",
    "    print(\"EfficientViT: An√°lise completa e compara√ß√£o cross-arquitetural finalizada!\")\n",
    "    print(f\"\\nArquivos finais gerados:\")\n",
    "    print(f\"  ‚Ä¢ M√©tricas ViT: metrics/efficientvit/efficientvit_performance_metrics.csv\")\n",
    "    print(f\"  ‚Ä¢ Compara√ß√£o final: metrics/all_models_comparison.csv\")\n",
    "    print(f\"  ‚Ä¢ An√°lise visual: plots/efficientvit/efficientvit_comprehensive_analysis_{experiment_id}.png\")\n",
    "    if model_saved:\n",
    "        print(f\"  ‚Ä¢ Modelo salvo: models/efficientvit/weights_efficientvit_{experiment_id}.h5\")\n",
    "        print(f\"  ‚Ä¢ Configura√ß√£o: models/efficientvit/config_efficientvit_{experiment_id}.pkl\")\n",
    "    print(f\"  ‚Ä¢ Aten√ß√£o config: attention_maps/attention_config_{experiment_id}.pkl\")\n",
    "    \n",
    "    print(f\"\\nüéØ EXPERIMENTO COMPLETO: ResNet50 ‚Üí EfficientNet ‚Üí EfficientViT\")\n",
    "    print(f\"üìä Todos os dados salvos para an√°lise comparativa cient√≠fica\")\n",
    "    print(f\"üèÜ EfficientViT representa estado-da-arte em efici√™ncia e interpretabilidade\")\n",
    "    \n",
    "else:\n",
    "    print(\"Erro: Avalia√ß√£o EfficientViT n√£o foi executada corretamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
