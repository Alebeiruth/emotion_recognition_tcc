{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet PyTorch Implementation for Emotion Classification\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# GPU configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 224  # EfficientNet-B0 standard input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "# EfficientNet specific configuration\n",
    "EFFICIENTNET_CONFIG = {\n",
    "    'base_learning_rate': 0.001,\n",
    "    'fine_tune_learning_rate': 0.0001,\n",
    "    'dropout_rate': 0.5,\n",
    "    'fine_tune_layers': 20,\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "# Emotion labels mapping\n",
    "EMOTION_LABELS = {\n",
    "    'Raiva': 0, 'Nojo': 1, 'Medo': 2, 'Felicidade': 3, \n",
    "    'Neutro': 4, 'Tristeza': 5, 'Surpresa': 6\n",
    "}\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\"\"Custom dataset for emotion classification\"\"\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image format for transforms\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Ensure image is in uint8 format [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class EfficientNetEmotionClassifier(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 model for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(EfficientNetEmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained EfficientNet-B0\n",
    "        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace the classifier\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()  # Remove original classifier\n",
    "        \n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),  # Feature dropout (similar to drop_connect)\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize custom layers\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize custom classifier weights\"\"\"\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        # Classify\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone for feature extraction phase\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_backbone(self, num_layers_to_unfreeze=20):\n",
    "        \"\"\"Unfreeze last layers for fine-tuning\"\"\"\n",
    "        # Get all backbone parameters\n",
    "        backbone_params = list(self.backbone.parameters())\n",
    "        \n",
    "        # Unfreeze last num_layers_to_unfreeze parameters\n",
    "        for param in backbone_params[-num_layers_to_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "class EfficientNetMonitor:\n",
    "    \"\"\"Monitor for EfficientNet training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.peak_memory_mb = 0\n",
    "        self.initial_memory_mb = 0\n",
    "        self.phase1_time = 0\n",
    "        self.phase2_time = 0\n",
    "        self.process = psutil.Process()\n",
    "        self.training_phases = {'phase1': None, 'phase2': None}\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start monitoring\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_memory_mb = self._get_memory_usage()\n",
    "        self.peak_memory_mb = self.initial_memory_mb\n",
    "        print(f\"Starting EfficientNet-B0 training...\")\n",
    "        print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    def start_phase(self, phase_name):\n",
    "        \"\"\"Start a training phase\"\"\"\n",
    "        self.training_phases[phase_name] = time.time()\n",
    "        print(f\"Starting {phase_name} - EfficientNet\")\n",
    "        \n",
    "    def end_phase(self, phase_name):\n",
    "        \"\"\"End a training phase\"\"\"\n",
    "        if self.training_phases[phase_name] is not None:\n",
    "            phase_duration = time.time() - self.training_phases[phase_name]\n",
    "            if phase_name == 'phase1':\n",
    "                self.phase1_time = phase_duration\n",
    "            elif phase_name == 'phase2':\n",
    "                self.phase2_time = phase_duration\n",
    "            print(f\"{phase_name} completed in: {timedelta(seconds=int(phase_duration))}\")\n",
    "            return phase_duration\n",
    "        return 0\n",
    "        \n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "        \n",
    "    def update_peak_memory(self):\n",
    "        \"\"\"Update peak memory if necessary\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        if current_memory > self.peak_memory_mb:\n",
    "            self.peak_memory_mb = current_memory\n",
    "            \n",
    "    def get_efficiency_metrics(self):\n",
    "        \"\"\"Calculate efficiency metrics\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        total_time = self.phase1_time + self.phase2_time\n",
    "        \n",
    "        return {\n",
    "            'memory_efficiency': self.initial_memory_mb / self.peak_memory_mb if self.peak_memory_mb > 0 else 0,\n",
    "            'time_efficiency': total_time / 3600,  # Hours\n",
    "            'peak_memory_gb': self.peak_memory_mb / 1024,\n",
    "            'memory_growth_factor': self.peak_memory_mb / self.initial_memory_mb if self.initial_memory_mb > 0 else 1,\n",
    "            'phase1_ratio': self.phase1_time / total_time if total_time > 0 else 0,\n",
    "            'phase2_ratio': self.phase2_time / total_time if total_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "    def end_monitoring(self):\n",
    "        \"\"\"End monitoring and return statistics\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        total_time_seconds = self.end_time - self.start_time\n",
    "        total_time_formatted = str(timedelta(seconds=int(total_time_seconds)))\n",
    "        \n",
    "        final_memory_mb = self._get_memory_usage()\n",
    "        memory_increase = final_memory_mb - self.initial_memory_mb\n",
    "        \n",
    "        efficiency_metrics = self.get_efficiency_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EFFICIENTNET-B0 MONITORING REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total training time: {total_time_formatted}\")\n",
    "        print(f\"  • Phase 1 (Base layers): {timedelta(seconds=int(self.phase1_time))}\")\n",
    "        print(f\"  • Phase 2 (Fine-tuning): {timedelta(seconds=int(self.phase2_time))}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Final memory: {final_memory_mb:.2f} MB\")\n",
    "        print(f\"Peak memory: {self.peak_memory_mb:.2f} MB\")\n",
    "        print(f\"Memory efficiency: {efficiency_metrics['memory_efficiency']:.3f}\")\n",
    "        print(f\"Growth factor: {efficiency_metrics['memory_growth_factor']:.2f}x\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'total_time_seconds': total_time_seconds,\n",
    "            'total_time_formatted': total_time_formatted,\n",
    "            'initial_memory_mb': self.initial_memory_mb,\n",
    "            'final_memory_mb': final_memory_mb,\n",
    "            'peak_memory_mb': self.peak_memory_mb,\n",
    "            'memory_increase_mb': memory_increase,\n",
    "            'phase1_time': self.phase1_time,\n",
    "            'phase2_time': self.phase2_time,\n",
    "            'efficiency_metrics': efficiency_metrics\n",
    "        }\n",
    "\n",
    "def load_preprocessed_data_efficientnet_from_images():\n",
    "    \"\"\"Load preprocessed data from images for EfficientNet\"\"\"\n",
    "    print(\"Loading preprocessed JPG data for EfficientNet...\")\n",
    "    \n",
    "    BASE_PATH = r\"../data/augmented/raf_db_balanced\"\n",
    "    \n",
    "    def load_images_from_directory(directory_path, set_name):\n",
    "        \"\"\"Load images from directory\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(f\"Loading {set_name} from: {directory_path}\")\n",
    "        \n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"❌ Directory not found: {directory_path}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        subdirs = [d for d in os.listdir(directory_path) \n",
    "                  if os.path.isdir(os.path.join(directory_path, d))]\n",
    "        \n",
    "        print(f\"📁 Subdirectories found: {subdirs}\")\n",
    "        \n",
    "        for emotion, label in EMOTION_LABELS.items():\n",
    "            emotion_path = os.path.join(directory_path, emotion)\n",
    "            \n",
    "            if not os.path.exists(emotion_path):\n",
    "                print(f\"⚠️  Folder '{emotion}' not found in {directory_path}\")\n",
    "                continue\n",
    "            \n",
    "            count = 0\n",
    "            image_files = []\n",
    "            \n",
    "            # Search for different extensions\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "                import glob\n",
    "                pattern = os.path.join(emotion_path, ext)\n",
    "                image_files.extend(glob.glob(pattern))\n",
    "            \n",
    "            print(f\"  📸 {emotion}: {len(image_files)} files found\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    # Load image\n",
    "                    img = cv2.imread(img_file)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert BGR to RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Resize if necessary\n",
    "                    if img.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
    "                        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Ensure RGB (3 channels)\n",
    "                    if len(img.shape) == 2:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    elif img.shape[2] == 1:\n",
    "                        img = np.repeat(img, 3, axis=2)\n",
    "                    elif img.shape[2] == 4:  # RGBA\n",
    "                        img = img[:, :, :3]  # Remove alpha channel\n",
    "                    \n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ❌ Error loading {os.path.basename(img_file)}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  ✅ {emotion}: {count} images loaded successfully\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    try:\n",
    "        # Load train and test data\n",
    "        train_path = os.path.join(BASE_PATH, \"train\")\n",
    "        test_path = os.path.join(BASE_PATH, \"test\")\n",
    "        \n",
    "        X_train, y_train = load_images_from_directory(train_path, \"TRAIN\")\n",
    "        X_test, y_test = load_images_from_directory(test_path, \"TEST\")\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(\"❌ No images loaded. Check paths and folder names!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        print(f\"\\n📊 Data loaded successfully:\")\n",
    "        print(f\"- X_train: {X_train.shape}\")\n",
    "        print(f\"- y_train: {y_train.shape}\")\n",
    "        print(f\"- X_test: {X_test.shape}\")\n",
    "        print(f\"- y_test: {y_test.shape}\")\n",
    "        \n",
    "        # Verify final format\n",
    "        print(f\"\\n🔍 Final verification:\")\n",
    "        print(f\"- X_train range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "        print(f\"- X_test range: [{X_test.min():.3f}, {X_test.max():.3f}]\")\n",
    "        print(f\"- Image format: {X_train.shape[1:]} (should be {IMG_SIZE}x{IMG_SIZE}x3)\")\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Get transforms for training and validation\"\"\"\n",
    "    # Training transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    # Validation/test transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Create PyTorch data loaders\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = EmotionDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = EmotionDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_efficientnet_two_phase(model, train_loader, val_loader, monitor):\n",
    "    \"\"\"Train EfficientNet in two phases\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING EFFICIENTNET 2-PHASE TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Feature extraction\n",
    "    print(\"PHASE 1: FEATURE EXTRACTION\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase1')\n",
    "    \n",
    "    # Freeze backbone\n",
    "    model.freeze_backbone()\n",
    "    \n",
    "    # Setup optimizer and criterion for phase 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=EFFICIENTNET_CONFIG['base_learning_rate'], \n",
    "                          weight_decay=EFFICIENTNET_CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training phase 1\n",
    "    best_val_acc_phase1 = 0\n",
    "    phase1_epochs = 30\n",
    "    \n",
    "    history_phase1 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(phase1_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history_phase1['train_loss'].append(train_loss)\n",
    "        history_phase1['train_acc'].append(train_acc)\n",
    "        history_phase1['val_loss'].append(val_loss)\n",
    "        history_phase1['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase1:\n",
    "            best_val_acc_phase1 = val_acc\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Phase 1 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    phase1_duration = monitor.end_phase('phase1')\n",
    "    print(f\"Phase 1 - Best val_accuracy: {best_val_acc_phase1:.4f}\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning\n",
    "    print(\"\\nPHASE 2: FINE-TUNING\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase2')\n",
    "    \n",
    "    # Unfreeze backbone layers\n",
    "    model.unfreeze_backbone(EFFICIENTNET_CONFIG['fine_tune_layers'])\n",
    "    \n",
    "    # Setup optimizer for phase 2 with lower learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=EFFICIENTNET_CONFIG['fine_tune_learning_rate'], \n",
    "                          weight_decay=EFFICIENTNET_CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.3, verbose=True)\n",
    "    \n",
    "    # Training phase 2\n",
    "    best_val_acc_phase2 = 0\n",
    "    phase2_epochs = EPOCHS - phase1_epochs\n",
    "    \n",
    "    history_phase2 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(phase2_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history_phase2['train_loss'].append(train_loss)\n",
    "        history_phase2['train_acc'].append(train_acc)\n",
    "        history_phase2['val_loss'].append(val_loss)\n",
    "        history_phase2['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase2:\n",
    "            best_val_acc_phase2 = val_acc\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_efficientnet_model.pth')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Phase 2 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    phase2_duration = monitor.end_phase('phase2')\n",
    "    print(f\"Phase 2 - Best val_accuracy: {best_val_acc_phase2:.4f}\")\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_history = {\n",
    "        'train_loss': history_phase1['train_loss'] + history_phase2['train_loss'],\n",
    "        'train_acc': history_phase1['train_acc'] + history_phase2['train_acc'],\n",
    "        'val_loss': history_phase1['val_loss'] + history_phase2['val_loss'],\n",
    "        'val_acc': history_phase1['val_acc'] + history_phase2['val_acc'],\n",
    "        'phase1_epochs': phase1_epochs,\n",
    "        'phase2_epochs': phase2_epochs,\n",
    "        'phase1_duration': phase1_duration,\n",
    "        'phase2_duration': phase2_duration,\n",
    "        'best_val_acc_phase1': best_val_acc_phase1,\n",
    "        'best_val_acc_phase2': best_val_acc_phase2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTRAINING COMPLETE:\")\n",
    "    print(f\"  • Total epochs: {phase1_epochs + phase2_epochs}\")\n",
    "    print(f\"  • Best final accuracy: {max(combined_history['val_acc']):.4f}\")\n",
    "    print(f\"  • Total time: {timedelta(seconds=int(phase1_duration + phase2_duration))}\")\n",
    "    \n",
    "    return combined_history\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    class_report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=emotion_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return accuracy, f1, precision, recall, conf_matrix, class_report\n",
    "\n",
    "def create_comprehensive_visualizations_efficientnet_pytorch(history, conf_matrix, metrics, class_report, experiment_id, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Cria visualizações completas dos resultados EfficientNet PyTorch com foco em dados desbalanceados\n",
    "    \n",
    "    Args:\n",
    "        history: Histórico de treinamento\n",
    "        conf_matrix: Matriz de confusão\n",
    "        metrics: Métricas do modelo\n",
    "        class_report: Relatório de classificação\n",
    "        experiment_id: ID do experimento\n",
    "        y_true: Labels verdadeiros\n",
    "        y_pred: Predições do modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuração da figura\n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    \n",
    "    # 1. HISTÓRICO DE TREINAMENTO EM 2 FASES\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    epochs_phase1 = range(1, history['phase1_epochs'] + 1)\n",
    "    epochs_phase2 = range(history['phase1_epochs'] + 1, \n",
    "                         history['phase1_epochs'] + history['phase2_epochs'] + 1)\n",
    "    \n",
    "    # Plotar accuracy por fase\n",
    "    plt.plot(epochs_phase1, history['train_acc'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Fase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_acc'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Fase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_acc'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Fase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_acc'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Fase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7, \n",
    "                label='Phase Transition')\n",
    "    plt.title('EfficientNet: Accuracy - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. LOSS EM 2 FASES\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    plt.plot(epochs_phase1, history['train_loss'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Fase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_loss'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Fase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_loss'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Fase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_loss'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Fase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7)\n",
    "    plt.title('EfficientNet: Loss - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. MATRIZ DE CONFUSÃO RAW (DESBALANCEADA)\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    \n",
    "    # Matriz bruta mostra o desbalanceamento\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax3,\n",
    "                cbar_kws={'label': 'Número de Amostras'})\n",
    "    plt.title('Matriz Confusão RAW\\n(Mostra Desbalanceamento)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 4. MATRIZ DE CONFUSÃO NORMALIZADA (POR LINHA)\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    \n",
    "    # Normalização por linha (recall por classe)\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Substitui NaN por 0 (caso alguma classe não tenha amostras)\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.3f', cmap='Greens',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax4,\n",
    "                cbar_kws={'label': 'Proporção (Recall)'})\n",
    "    plt.title('Matriz Confusão NORMALIZADA\\n(Recall por Classe)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 5. DISTRIBUIÇÃO DE CLASSES (DESBALANCEAMENTO)\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    \n",
    "    # Conta amostras por classe no conjunto de teste\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    # Ordena por quantidade\n",
    "    sorted_classes = sorted(class_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "    class_names_sorted = [emotion_names[i] for i, count in sorted_classes]\n",
    "    class_counts_sorted = [count for i, count in sorted_classes]\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names_sorted)))\n",
    "    bars = plt.bar(class_names_sorted, class_counts_sorted, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('Distribuição de Classes - Teste\\n(Dados Desbalanceados)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Número de Amostras')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar, count in zip(bars, class_counts_sorted):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adiciona linha da média\n",
    "    mean_samples = np.mean(class_counts_sorted)\n",
    "    plt.axhline(y=mean_samples, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Média: {mean_samples:.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 6. F1-SCORE POR EMOÇÃO COM ANÁLISE DE DESBALANCEAMENTO\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    \n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Criar gráfico de barras colorido por quantidade de amostras\n",
    "    colors = plt.cm.viridis(np.array(support_counts) / max(support_counts))\n",
    "    bars = plt.bar(emotion_names, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('F1-Score por Emoção\\n(Cor = Qtd Amostras)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Adiciona valores e support\n",
    "    for bar, score, support in zip(bars, f1_scores, support_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}\\n(n={support})', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Colorbar para indicar quantidade de amostras\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, \n",
    "                               norm=plt.Normalize(vmin=min(support_counts), vmax=max(support_counts)))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=ax6, label='Amostras de Teste')\n",
    "    \n",
    "    # 7. PRECISION, RECALL, F1 POR CLASSE\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    \n",
    "    precision_scores = [class_report[emotion]['precision'] for emotion in emotion_names]\n",
    "    recall_scores = [class_report[emotion]['recall'] for emotion in emotion_names]\n",
    "    \n",
    "    x = np.arange(len(emotion_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x, recall_scores, width, label='Recall', alpha=0.8, color='lightblue')\n",
    "    bars3 = plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    plt.title('Métricas Detalhadas por Classe', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Emoção')\n",
    "    plt.xticks(x, emotion_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 8. ANÁLISE DE CORRELAÇÃO DESBALANCEAMENTO vs PERFORMANCE\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    \n",
    "    # Scatter plot: Quantidade de amostras vs F1-Score\n",
    "    plt.scatter(support_counts, f1_scores, c=support_counts, cmap='viridis', \n",
    "                s=100, alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    # Adiciona labels para cada ponto\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        plt.annotate(emotion, (support_counts[i], f1_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Linha de tendência\n",
    "    z = np.polyfit(support_counts, f1_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(support_counts, p(support_counts), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title('Amostras vs Performance\\n(Correlação)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Número de Amostras de Teste')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcula correlação\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correlação: {correlation:.3f}', \n",
    "             transform=ax8.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 9. HEATMAP DE ERROS POR CLASSE\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    \n",
    "    # Matriz de erros (off-diagonal elements)\n",
    "    error_matrix = conf_matrix.copy()\n",
    "    np.fill_diagonal(error_matrix, 0)  # Remove diagonal (acertos)\n",
    "    \n",
    "    # Normaliza por linha para mostrar proporção de erros\n",
    "    error_matrix_norm = error_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    error_matrix_norm = np.nan_to_num(error_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(error_matrix_norm, annot=True, fmt='.3f', cmap='Reds',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax9,\n",
    "                cbar_kws={'label': 'Proporção de Erros'})\n",
    "    plt.title('Heatmap de Erros\\n(Confusões entre Classes)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita (Erro)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 10. MÉTRICAS COMPARATIVAS WEIGHTED vs MACRO\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    \n",
    "    # Calcula métricas weighted e macro\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics_comparison = {\n",
    "        'Precision': [precision_macro, precision_weighted],\n",
    "        'Recall': [recall_macro, recall_weighted],\n",
    "        'F1-Score': [f1_macro, f1_weighted]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(metrics_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    macro_values = [metrics_comparison[metric][0] for metric in metrics_comparison]\n",
    "    weighted_values = [metrics_comparison[metric][1] for metric in metrics_comparison]\n",
    "    \n",
    "    bars1 = plt.bar(x - width/2, macro_values, width, label='Macro (Unbalanced)', \n",
    "                   alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x + width/2, weighted_values, width, label='Weighted (Balanced)', \n",
    "                   alpha=0.8, color='lightblue')\n",
    "    \n",
    "    plt.title('Macro vs Weighted Metrics\\n(Impacto do Desbalanceamento)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Métrica')\n",
    "    plt.xticks(x, metrics_comparison.keys())\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 11. RECURSOS COMPUTACIONAIS E EFICIÊNCIA\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    \n",
    "    resource_data = {\n",
    "        'Tempo (min)': metrics['training_time_seconds'] / 60,\n",
    "        'Memória (GB)': metrics['peak_memory_mb'] / 1024,\n",
    "        'Parâmetros (M)': metrics['total_parameters'] / 1_000_000,\n",
    "        'Eficiência\\n(Acc/M_params)': metrics['test_accuracy'] / (metrics['total_parameters'] / 1_000_000)\n",
    "    }\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    bars = plt.bar(range(len(resource_data)), list(resource_data.values()), \n",
    "                  color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.title('Recursos Computacionais\\nEfficientNet-B0', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(range(len(resource_data)), resource_data.keys(), rotation=45)\n",
    "    plt.ylabel('Valor')\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar, (key, value) in zip(bars, resource_data.items()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.02,\n",
    "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 12. RESUMO CIENTÍFICO\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    # Calcula estatísticas do desbalanceamento\n",
    "    imbalance_ratio = max(support_counts) / min(support_counts)\n",
    "    class_std = np.std(support_counts)\n",
    "    class_cv = class_std / np.mean(support_counts)  # Coefficient of variation\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "EFFICIENTNET-B0 PYTORCH\n",
    "ANÁLISE CIENTÍFICA\n",
    "\n",
    "PERFORMANCE:\n",
    "• Accuracy: {metrics['test_accuracy']:.4f}\n",
    "• F1-Macro: {f1_macro:.4f}\n",
    "• F1-Weighted: {f1_weighted:.4f}\n",
    "\n",
    "DESBALANCEAMENTO:\n",
    "• Ratio max/min: {imbalance_ratio:.1f}x\n",
    "• Coef. Variação: {class_cv:.3f}\n",
    "• Correlação amostras-F1: {correlation:.3f}\n",
    "\n",
    "EFICIÊNCIA:\n",
    "• Parâmetros: {metrics['total_parameters']/1_000_000:.1f}M\n",
    "• Tempo: {metrics['training_time_seconds']/60:.1f} min\n",
    "• Memória: {metrics['peak_memory_mb']/1024:.2f} GB\n",
    "\n",
    "CONCLUSÃO:\n",
    "Modelo {('robusto' if f1_macro > 0.7 else 'limitado')} para dados\n",
    "desbalanceados. {'Boa' if correlation > 0.3 else 'Baixa'} correlação\n",
    "entre quantidade de amostras e performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.05, 0.95, summary_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax12.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva a figura\n",
    "    os.makedirs('plots/efficientnet', exist_ok=True)\n",
    "    plt.savefig(f'plots/efficientnet/efficientnet_comprehensive_analysis_pytorch_{experiment_id}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === ANÁLISE TEXTUAL DETALHADA ===\n",
    "    print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names)\n",
    "\n",
    "def print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names):\n",
    "    \"\"\"Análise textual detalhada do impacto do desbalanceamento\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISE DETALHADA - DADOS DESBALANCEADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Estatísticas de desbalanceamento\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    print(\"DISTRIBUIÇÃO DE CLASSES:\")\n",
    "    total_samples = len(y_true)\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        count = class_distribution.get(i, 0)\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  • {emotion}: {count} amostras ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Análise de desbalanceamento\n",
    "    max_samples = max(counts)\n",
    "    min_samples = min(counts)\n",
    "    imbalance_ratio = max_samples / min_samples\n",
    "    class_std = np.std(counts)\n",
    "    class_cv = class_std / np.mean(counts)\n",
    "    \n",
    "    print(f\"\\nESTATÍSTICAS DE DESBALANCEAMENTO:\")\n",
    "    print(f\"  • Ratio máximo/mínimo: {imbalance_ratio:.2f}x\")\n",
    "    print(f\"  • Desvio padrão: {class_std:.1f} amostras\")\n",
    "    print(f\"  • Coeficiente de variação: {class_cv:.3f}\")\n",
    "    print(f\"  • Classe majoritária: {emotion_names[np.argmax(counts)]} ({max_samples} amostras)\")\n",
    "    print(f\"  • Classe minoritária: {emotion_names[np.argmin(counts)]} ({min_samples} amostras)\")\n",
    "    \n",
    "    # Impacto na performance\n",
    "    print(f\"\\nIMPACTO NA PERFORMANCE:\")\n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Correlação entre quantidade e performance\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    print(f\"  • Correlação amostras-F1: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation > 0.5:\n",
    "        print(f\"    → FORTE correlação positiva: mais amostras = melhor F1\")\n",
    "    elif correlation > 0.3:\n",
    "        print(f\"    → MODERADA correlação positiva\")\n",
    "    elif correlation > -0.3:\n",
    "        print(f\"    → FRACA correlação: modelo relativamente robusto\")\n",
    "    else:\n",
    "        print(f\"    → Correlação negativa: possível overfitting em classes majoritárias\")\n",
    "    \n",
    "    # Classes com melhor/pior performance\n",
    "    best_class_idx = np.argmax(f1_scores)\n",
    "    worst_class_idx = np.argmin(f1_scores)\n",
    "    \n",
    "    print(f\"  • Melhor F1: {emotion_names[best_class_idx]} ({f1_scores[best_class_idx]:.3f}) - {support_counts[best_class_idx]} amostras\")\n",
    "    print(f\"  • Pior F1: {emotion_names[worst_class_idx]} ({f1_scores[worst_class_idx]:.3f}) - {support_counts[worst_class_idx]} amostras\")\n",
    "    \n",
    "    # Análise de confusões\n",
    "    print(f\"\\nMAIORES CONFUSÕES:\")\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    # Encontra maiores erros (off-diagonal)\n",
    "    np.fill_diagonal(conf_matrix_norm, 0)\n",
    "    max_confusion_idx = np.unravel_index(np.argmax(conf_matrix_norm), conf_matrix_norm.shape)\n",
    "    max_confusion_value = conf_matrix_norm[max_confusion_idx]\n",
    "    \n",
    "    print(f\"  • {emotion_names[max_confusion_idx[0]]} → {emotion_names[max_confusion_idx[1]]}: {max_confusion_value:.3f}\")\n",
    "    \n",
    "    # Top 3 confusões\n",
    "    flat_indices = np.argsort(conf_matrix_norm.flatten())[-3:][::-1]\n",
    "    for idx in flat_indices:\n",
    "        i, j = np.unravel_index(idx, conf_matrix_norm.shape)\n",
    "        if i != j:  # Ignora diagonal\n",
    "            print(f\"  • {emotion_names[i]} → {emotion_names[j]}: {conf_matrix_norm[i, j]:.3f}\")\n",
    "    \n",
    "    # Recomendações\n",
    "    print(f\"\\nRECOMENDAÇÕES:\")\n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"  • Desbalanceamento SEVERO - considere técnicas de balanceamento\")\n",
    "        print(\"  • Sugestões: SMOTE, class weighting, focal loss\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(\"  • Desbalanceamento MODERADO - monitorar métricas por classe\")\n",
    "        print(\"  • Sugestões: class weighting, stratified sampling\")\n",
    "    else:\n",
    "        print(\"  • Desbalanceamento LEVE - modelo pode lidar adequadamente\")\n",
    "    \n",
    "    if correlation < 0.3:\n",
    "        print(\"  • Modelo demonstra robustez ao desbalanceamento\")\n",
    "    else:\n",
    "        print(\"  • Considere estratégias para classes minoritárias\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize monitor\n",
    "    monitor = EfficientNetMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Generate experiment ID\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_id = f\"efficientnet_pytorch_emotion_{timestamp}\"\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('plots/efficientnet', exist_ok=True)\n",
    "    os.makedirs('models/efficientnet', exist_ok=True)\n",
    "    os.makedirs('metrics/efficientnet', exist_ok=True)\n",
    "    \n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_test, y_test = load_preprocessed_data_efficientnet_from_images()\n",
    "    \n",
    "    if X_train is not None:\n",
    "        # Split training data into train and validation\n",
    "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            stratify=y_train,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split:\")\n",
    "        print(f\"  • Train: {X_train_split.shape}\")\n",
    "        print(f\"  • Validation: {X_val.shape}\")\n",
    "        print(f\"  • Test: {X_test.shape}\")\n",
    "        \n",
    "        # Analyze data distribution\n",
    "        print(f\"\\nData distribution analysis:\")\n",
    "        unique_train, counts_train = np.unique(y_train_split, return_counts=True)\n",
    "        unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "        \n",
    "        emotion_names = list(EMOTION_LABELS.keys())\n",
    "        print(\"Training set:\")\n",
    "        for i, count in enumerate(counts_train):\n",
    "            print(f\"  • {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        print(\"Test set:\")\n",
    "        for i, count in enumerate(counts_test):\n",
    "            print(f\"  • {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        test_imbalance_ratio = max(counts_test) / min(counts_test)\n",
    "        print(f\"Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            X_train_split, y_train_split, X_val, y_val, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        print(\"\\nCreating EfficientNet-B0 model...\")\n",
    "        model = EfficientNetEmotionClassifier(num_classes=7, dropout_rate=EFFICIENTNET_CONFIG['dropout_rate'])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"EfficientNet-B0 created successfully:\")\n",
    "        print(f\"  • Total parameters: {total_params:,}\")\n",
    "        print(f\"  • Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  • Efficiency: {total_params/1000000:.1f}M parameters\")\n",
    "        print(f\"  • Trainable ratio: {(trainable_params/total_params)*100:.1f}%\")\n",
    "        \n",
    "        # Update monitor with GPU memory if available\n",
    "        if torch.cuda.is_available():\n",
    "            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            print(f\"  • Initial GPU memory: {initial_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        history = train_efficientnet_two_phase(model, train_loader, val_loader, monitor)\n",
    "        \n",
    "        # Load best model for evaluation\n",
    "        if os.path.exists('best_efficientnet_model.pth'):\n",
    "            model.load_state_dict(torch.load('best_efficientnet_model.pth'))\n",
    "            print(\"Loaded best model weights for evaluation\")\n",
    "        \n",
    "        # Update memory tracking\n",
    "        monitor.update_peak_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "            print(f\"Peak GPU memory: {peak_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Comprehensive evaluation and visualization\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Enhanced evaluation with visualization\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "        \n",
    "        # Detailed evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        inference_times = []\n",
    "        \n",
    "        print(\"Measuring inference performance...\")\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "                start_time = time.time()\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                inference_time = time.time() - start_time\n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "                _, predicted = output.max(1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Weighted metrics for imbalanced data\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "        class_report = classification_report(\n",
    "            all_targets, all_preds,\n",
    "            target_names=emotion_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Compile comprehensive metrics\n",
    "        avg_inference_time = np.mean(inference_times)\n",
    "        total_inference_time = sum(inference_times)\n",
    "        samples_per_second = len(all_targets) / total_inference_time\n",
    "        \n",
    "        metrics = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'test_accuracy': accuracy,\n",
    "            'f1_score_macro': f1,\n",
    "            'f1_score_weighted': f1_weighted,\n",
    "            'precision_macro': precision,\n",
    "            'precision_weighted': precision_weighted,\n",
    "            'recall_macro': recall,\n",
    "            'recall_weighted': recall_weighted,\n",
    "            'training_time_seconds': history['phase1_duration'] + history['phase2_duration'],\n",
    "            'phase1_duration': history['phase1_duration'],\n",
    "            'phase2_duration': history['phase2_duration'],\n",
    "            'peak_memory_mb': monitor.peak_memory_mb,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'avg_inference_time_seconds': avg_inference_time,\n",
    "            'samples_per_second': samples_per_second,\n",
    "            'test_samples': len(all_targets),\n",
    "            'imbalance_ratio': test_imbalance_ratio,\n",
    "            'best_val_acc_phase1': history['best_val_acc_phase1'],\n",
    "            'best_val_acc_phase2': history['best_val_acc_phase2']\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nCOMPREHENSIVE RESULTS:\")\n",
    "        print(f\"  • Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"  • F1-Score (Macro): {f1:.4f}\")\n",
    "        print(f\"  • F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "        print(f\"  • Precision (Macro): {precision:.4f}\")\n",
    "        print(f\"  • Recall (Macro): {recall:.4f}\")\n",
    "        print(f\"  • Inference Speed: {samples_per_second:.1f} samples/second\")\n",
    "        print(f\"  • Total Training Time: {timedelta(seconds=int(metrics['training_time_seconds']))}\")\n",
    "        \n",
    "        # Create comprehensive visualizations with imbalance analysis\n",
    "        print(\"\\nGenerating comprehensive visualizations...\")\n",
    "        create_comprehensive_visualizations_efficientnet_pytorch(\n",
    "            history, conf_matrix, metrics, class_report, experiment_id, all_targets, all_preds\n",
    "        )\n",
    "        \n",
    "        # Save model if performance is good\n",
    "        performance_threshold = 0.70  # Adjust based on your requirements\n",
    "        if accuracy >= performance_threshold:\n",
    "            model_path = f'models/efficientnet/efficientnet_model_{experiment_id}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'history': history,\n",
    "                'model_config': {\n",
    "                    'num_classes': 7,\n",
    "                    'dropout_rate': EFFICIENTNET_CONFIG['dropout_rate'],\n",
    "                    'img_size': IMG_SIZE,\n",
    "                    'architecture': 'EfficientNet-B0'\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"Model saved: {model_path}\")\n",
    "        else:\n",
    "            print(f\"Model not saved (accuracy {accuracy:.4f} < {performance_threshold})\")\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_csv = f'metrics/efficientnet/efficientnet_metrics_{experiment_id}.csv'\n",
    "        metrics_df.to_csv(metrics_csv, index=False)\n",
    "        print(f\"Metrics saved: {metrics_csv}\")\n",
    "        \n",
    "        # End monitoring\n",
    "        monitor_stats = monitor.end_monitoring()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Experiment ID: {experiment_id}\")\n",
    "        print(f\"Files generated:\")\n",
    "        print(f\"  • Visualizations: plots/efficientnet/efficientnet_comprehensive_analysis_pytorch_{experiment_id}.png\")\n",
    "        print(f\"  • Metrics: {metrics_csv}\")\n",
    "        if accuracy >= performance_threshold:\n",
    "            print(f\"  • Model: models/efficientnet/efficientnet_model_{experiment_id}.pth\")\n",
    "        print(f\"  • Best model: best_efficientnet_model.pth\")\n",
    "        \n",
    "        # Final summary for imbalanced data\n",
    "        print(f\"\\nIMBALANCE ANALYSIS SUMMARY:\")\n",
    "        print(f\"  • Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        print(f\"  • Macro F1 (unweighted): {f1:.4f}\")\n",
    "        print(f\"  • Weighted F1 (balanced): {f1_weighted:.4f}\")\n",
    "        print(f\"  • Performance difference: {abs(f1_weighted - f1):.4f}\")\n",
    "        \n",
    "        if f1_weighted - f1 > 0.05:\n",
    "            print(\"  → Model benefits significantly from class weighting\")\n",
    "        elif abs(f1_weighted - f1) < 0.02:\n",
    "            print(\"  → Model is robust to class imbalance\")\n",
    "        else:\n",
    "            print(\"  → Moderate impact of class imbalance\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to load data. Please check the data path and directory structure.\")\n",
    "        print(\"Expected structure: ../data/augmented/raf_db_balanced/train|test/Emotion_Name/*.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
