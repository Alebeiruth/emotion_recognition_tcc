{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet PyTorch Implementation for Emotion Classification\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import psutil\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# GPU configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 224  # EfficientNet-B0 standard input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "# EfficientNet specific configuration\n",
    "EFFICIENTNET_CONFIG = {\n",
    "    'base_learning_rate': 0.001,\n",
    "    'fine_tune_learning_rate': 0.0001,\n",
    "    'dropout_rate': 0.5,\n",
    "    'fine_tune_layers': 20,\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "# Emotion labels mapping\n",
    "EMOTION_LABELS = {\n",
    "    'Raiva': 0, 'Nojo': 1, 'Medo': 2, 'Felicidade': 3, \n",
    "    'Neutro': 4, 'Tristeza': 5, 'Surpresa': 6\n",
    "}\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"\"\"Custom dataset for emotion classification\"\"\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image format for transforms\n",
    "        if isinstance(image, np.ndarray):\n",
    "            # Ensure image is in uint8 format [0, 255]\n",
    "            if image.max() <= 1.0:\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "            else:\n",
    "                image = image.astype(np.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class EfficientNetEmotionClassifier(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 model for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(EfficientNetEmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained EfficientNet-B0\n",
    "        self.backbone = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace the classifier\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()  # Remove original classifier\n",
    "        \n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),  # Feature dropout (similar to drop_connect)\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize custom layers\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize custom classifier weights\"\"\"\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        # Classify\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone for feature extraction phase\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_backbone(self, num_layers_to_unfreeze=20):\n",
    "        \"\"\"Unfreeze last layers for fine-tuning\"\"\"\n",
    "        # Get all backbone parameters\n",
    "        backbone_params = list(self.backbone.parameters())\n",
    "        \n",
    "        # Unfreeze last num_layers_to_unfreeze parameters\n",
    "        for param in backbone_params[-num_layers_to_unfreeze:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "class EfficientNetMonitor:\n",
    "    \"\"\"Monitor for EfficientNet training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.peak_memory_mb = 0\n",
    "        self.initial_memory_mb = 0\n",
    "        self.phase1_time = 0\n",
    "        self.phase2_time = 0\n",
    "        self.process = psutil.Process()\n",
    "        self.training_phases = {'phase1': None, 'phase2': None}\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start monitoring\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_memory_mb = self._get_memory_usage()\n",
    "        self.peak_memory_mb = self.initial_memory_mb\n",
    "        print(f\"Starting EfficientNet-B0 training...\")\n",
    "        print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    def start_phase(self, phase_name):\n",
    "        \"\"\"Start a training phase\"\"\"\n",
    "        self.training_phases[phase_name] = time.time()\n",
    "        print(f\"Starting {phase_name} - EfficientNet\")\n",
    "        \n",
    "    def end_phase(self, phase_name):\n",
    "        \"\"\"End a training phase\"\"\"\n",
    "        if self.training_phases[phase_name] is not None:\n",
    "            phase_duration = time.time() - self.training_phases[phase_name]\n",
    "            if phase_name == 'phase1':\n",
    "                self.phase1_time = phase_duration\n",
    "            elif phase_name == 'phase2':\n",
    "                self.phase2_time = phase_duration\n",
    "            print(f\"{phase_name} completed in: {timedelta(seconds=int(phase_duration))}\")\n",
    "            return phase_duration\n",
    "        return 0\n",
    "        \n",
    "    def _get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        return self.process.memory_info().rss / 1024 / 1024\n",
    "        \n",
    "    def update_peak_memory(self):\n",
    "        \"\"\"Update peak memory if necessary\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        if current_memory > self.peak_memory_mb:\n",
    "            self.peak_memory_mb = current_memory\n",
    "            \n",
    "    def get_efficiency_metrics(self):\n",
    "        \"\"\"Calculate efficiency metrics\"\"\"\n",
    "        current_memory = self._get_memory_usage()\n",
    "        total_time = self.phase1_time + self.phase2_time\n",
    "        \n",
    "        return {\n",
    "            'memory_efficiency': self.initial_memory_mb / self.peak_memory_mb if self.peak_memory_mb > 0 else 0,\n",
    "            'time_efficiency': total_time / 3600,  # Hours\n",
    "            'peak_memory_gb': self.peak_memory_mb / 1024,\n",
    "            'memory_growth_factor': self.peak_memory_mb / self.initial_memory_mb if self.initial_memory_mb > 0 else 1,\n",
    "            'phase1_ratio': self.phase1_time / total_time if total_time > 0 else 0,\n",
    "            'phase2_ratio': self.phase2_time / total_time if total_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "    def end_monitoring(self):\n",
    "        \"\"\"End monitoring and return statistics\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        total_time_seconds = self.end_time - self.start_time\n",
    "        total_time_formatted = str(timedelta(seconds=int(total_time_seconds)))\n",
    "        \n",
    "        final_memory_mb = self._get_memory_usage()\n",
    "        memory_increase = final_memory_mb - self.initial_memory_mb\n",
    "        \n",
    "        efficiency_metrics = self.get_efficiency_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EFFICIENTNET-B0 MONITORING REPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total training time: {total_time_formatted}\")\n",
    "        print(f\"  ‚Ä¢ Phase 1 (Base layers): {timedelta(seconds=int(self.phase1_time))}\")\n",
    "        print(f\"  ‚Ä¢ Phase 2 (Fine-tuning): {timedelta(seconds=int(self.phase2_time))}\")\n",
    "        print(f\"Initial memory: {self.initial_memory_mb:.2f} MB\")\n",
    "        print(f\"Final memory: {final_memory_mb:.2f} MB\")\n",
    "        print(f\"Peak memory: {self.peak_memory_mb:.2f} MB\")\n",
    "        print(f\"Memory efficiency: {efficiency_metrics['memory_efficiency']:.3f}\")\n",
    "        print(f\"Growth factor: {efficiency_metrics['memory_growth_factor']:.2f}x\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'total_time_seconds': total_time_seconds,\n",
    "            'total_time_formatted': total_time_formatted,\n",
    "            'initial_memory_mb': self.initial_memory_mb,\n",
    "            'final_memory_mb': final_memory_mb,\n",
    "            'peak_memory_mb': self.peak_memory_mb,\n",
    "            'memory_increase_mb': memory_increase,\n",
    "            'phase1_time': self.phase1_time,\n",
    "            'phase2_time': self.phase2_time,\n",
    "            'efficiency_metrics': efficiency_metrics\n",
    "        }\n",
    "\n",
    "def load_preprocessed_data_efficientnet_from_images():\n",
    "    \"\"\"Load preprocessed data from images for EfficientNet\"\"\"\n",
    "    print(\"Loading preprocessed JPG data for EfficientNet...\")\n",
    "    \n",
    "    BASE_PATH = r\"../data/augmented/raf_db_balanced\"\n",
    "    \n",
    "    def load_images_from_directory(directory_path, set_name):\n",
    "        \"\"\"Load images from directory\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        print(f\"Loading {set_name} from: {directory_path}\")\n",
    "        \n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"‚ùå Directory not found: {directory_path}\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        subdirs = [d for d in os.listdir(directory_path) \n",
    "                  if os.path.isdir(os.path.join(directory_path, d))]\n",
    "        \n",
    "        print(f\"üìÅ Subdirectories found: {subdirs}\")\n",
    "        \n",
    "        for emotion, label in EMOTION_LABELS.items():\n",
    "            emotion_path = os.path.join(directory_path, emotion)\n",
    "            \n",
    "            if not os.path.exists(emotion_path):\n",
    "                print(f\"‚ö†Ô∏è  Folder '{emotion}' not found in {directory_path}\")\n",
    "                continue\n",
    "            \n",
    "            count = 0\n",
    "            image_files = []\n",
    "            \n",
    "            # Search for different extensions\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
    "                import glob\n",
    "                pattern = os.path.join(emotion_path, ext)\n",
    "                image_files.extend(glob.glob(pattern))\n",
    "            \n",
    "            print(f\"  üì∏ {emotion}: {len(image_files)} files found\")\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    # Load image\n",
    "                    img = cv2.imread(img_file)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert BGR to RGB\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Resize if necessary\n",
    "                    if img.shape[:2] != (IMG_SIZE, IMG_SIZE):\n",
    "                        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Ensure RGB (3 channels)\n",
    "                    if len(img.shape) == 2:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    elif img.shape[2] == 1:\n",
    "                        img = np.repeat(img, 3, axis=2)\n",
    "                    elif img.shape[2] == 4:  # RGBA\n",
    "                        img = img[:, :, :3]  # Remove alpha channel\n",
    "                    \n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "                    count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå Error loading {os.path.basename(img_file)}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  ‚úÖ {emotion}: {count} images loaded successfully\")\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    try:\n",
    "        # Load train and test data\n",
    "        train_path = os.path.join(BASE_PATH, \"train\")\n",
    "        test_path = os.path.join(BASE_PATH, \"test\")\n",
    "        \n",
    "        X_train, y_train = load_images_from_directory(train_path, \"TRAIN\")\n",
    "        X_test, y_test = load_images_from_directory(test_path, \"TEST\")\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(\"‚ùå No images loaded. Check paths and folder names!\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        print(f\"\\nüìä Data loaded successfully:\")\n",
    "        print(f\"- X_train: {X_train.shape}\")\n",
    "        print(f\"- y_train: {y_train.shape}\")\n",
    "        print(f\"- X_test: {X_test.shape}\")\n",
    "        print(f\"- y_test: {y_test.shape}\")\n",
    "        \n",
    "        # Verify final format\n",
    "        print(f\"\\nüîç Final verification:\")\n",
    "        print(f\"- X_train range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "        print(f\"- X_test range: [{X_test.min():.3f}, {X_test.max():.3f}]\")\n",
    "        print(f\"- Image format: {X_train.shape[1:]} (should be {IMG_SIZE}x{IMG_SIZE}x3)\")\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Get transforms for training and validation\"\"\"\n",
    "    # Training transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    # Validation/test transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Create PyTorch data loaders\"\"\"\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EmotionDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = EmotionDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = EmotionDataset(X_test, y_test, transform=val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_efficientnet_two_phase(model, train_loader, val_loader, monitor):\n",
    "    \"\"\"Train EfficientNet in two phases\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING EFFICIENTNET 2-PHASE TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Feature extraction\n",
    "    print(\"PHASE 1: FEATURE EXTRACTION\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase1')\n",
    "    \n",
    "    # Freeze backbone\n",
    "    model.freeze_backbone()\n",
    "    \n",
    "    # Setup optimizer and criterion for phase 1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=EFFICIENTNET_CONFIG['base_learning_rate'], \n",
    "                          weight_decay=EFFICIENTNET_CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training phase 1\n",
    "    best_val_acc_phase1 = 0\n",
    "    phase1_epochs = 30\n",
    "    \n",
    "    history_phase1 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(phase1_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history_phase1['train_loss'].append(train_loss)\n",
    "        history_phase1['train_acc'].append(train_acc)\n",
    "        history_phase1['val_loss'].append(val_loss)\n",
    "        history_phase1['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase1:\n",
    "            best_val_acc_phase1 = val_acc\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Phase 1 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    phase1_duration = monitor.end_phase('phase1')\n",
    "    print(f\"Phase 1 - Best val_accuracy: {best_val_acc_phase1:.4f}\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning\n",
    "    print(\"\\nPHASE 2: FINE-TUNING\")\n",
    "    print(\"-\" * 40)\n",
    "    monitor.start_phase('phase2')\n",
    "    \n",
    "    # Unfreeze backbone layers\n",
    "    model.unfreeze_backbone(EFFICIENTNET_CONFIG['fine_tune_layers'])\n",
    "    \n",
    "    # Setup optimizer for phase 2 with lower learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=EFFICIENTNET_CONFIG['fine_tune_learning_rate'], \n",
    "                          weight_decay=EFFICIENTNET_CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.3, verbose=True)\n",
    "    \n",
    "    # Training phase 2\n",
    "    best_val_acc_phase2 = 0\n",
    "    phase2_epochs = EPOCHS - phase1_epochs\n",
    "    \n",
    "    history_phase2 = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(phase2_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch + 1)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history_phase2['train_loss'].append(train_loss)\n",
    "        history_phase2['train_acc'].append(train_acc)\n",
    "        history_phase2['val_loss'].append(val_loss)\n",
    "        history_phase2['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc_phase2:\n",
    "            best_val_acc_phase2 = val_acc\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_efficientnet_model.pth')\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Phase 2 - Epoch {epoch+1}: Train Loss: {train_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    phase2_duration = monitor.end_phase('phase2')\n",
    "    print(f\"Phase 2 - Best val_accuracy: {best_val_acc_phase2:.4f}\")\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_history = {\n",
    "        'train_loss': history_phase1['train_loss'] + history_phase2['train_loss'],\n",
    "        'train_acc': history_phase1['train_acc'] + history_phase2['train_acc'],\n",
    "        'val_loss': history_phase1['val_loss'] + history_phase2['val_loss'],\n",
    "        'val_acc': history_phase1['val_acc'] + history_phase2['val_acc'],\n",
    "        'phase1_epochs': phase1_epochs,\n",
    "        'phase2_epochs': phase2_epochs,\n",
    "        'phase1_duration': phase1_duration,\n",
    "        'phase2_duration': phase2_duration,\n",
    "        'best_val_acc_phase1': best_val_acc_phase1,\n",
    "        'best_val_acc_phase2': best_val_acc_phase2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTRAINING COMPLETE:\")\n",
    "    print(f\"  ‚Ä¢ Total epochs: {phase1_epochs + phase2_epochs}\")\n",
    "    print(f\"  ‚Ä¢ Best final accuracy: {max(combined_history['val_acc']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Total time: {timedelta(seconds=int(phase1_duration + phase2_duration))}\")\n",
    "    \n",
    "    return combined_history\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    class_report = classification_report(\n",
    "        all_targets, all_preds,\n",
    "        target_names=emotion_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return accuracy, f1, precision, recall, conf_matrix, class_report\n",
    "\n",
    "def create_comprehensive_visualizations_efficientnet_pytorch(history, conf_matrix, metrics, class_report, experiment_id, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Cria visualiza√ß√µes completas dos resultados EfficientNet PyTorch com foco em dados desbalanceados\n",
    "    \n",
    "    Args:\n",
    "        history: Hist√≥rico de treinamento\n",
    "        conf_matrix: Matriz de confus√£o\n",
    "        metrics: M√©tricas do modelo\n",
    "        class_report: Relat√≥rio de classifica√ß√£o\n",
    "        experiment_id: ID do experimento\n",
    "        y_true: Labels verdadeiros\n",
    "        y_pred: Predi√ß√µes do modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configura√ß√£o da figura\n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    \n",
    "    # 1. HIST√ìRICO DE TREINAMENTO EM 2 FASES\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    epochs_phase1 = range(1, history['phase1_epochs'] + 1)\n",
    "    epochs_phase2 = range(history['phase1_epochs'] + 1, \n",
    "                         history['phase1_epochs'] + history['phase2_epochs'] + 1)\n",
    "    \n",
    "    # Plotar accuracy por fase\n",
    "    plt.plot(epochs_phase1, history['train_acc'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Fase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_acc'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Fase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_acc'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Fase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_acc'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Fase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7, \n",
    "                label='Phase Transition')\n",
    "    plt.title('EfficientNet: Accuracy - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. LOSS EM 2 FASES\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    plt.plot(epochs_phase1, history['train_loss'][:history['phase1_epochs']], \n",
    "             'b-', linewidth=2, label='Fase 1 - Train')\n",
    "    plt.plot(epochs_phase1, history['val_loss'][:history['phase1_epochs']], \n",
    "             'b--', linewidth=2, label='Fase 1 - Val')\n",
    "    plt.plot(epochs_phase2, history['train_loss'][history['phase1_epochs']:], \n",
    "             'r-', linewidth=2, label='Fase 2 - Train')\n",
    "    plt.plot(epochs_phase2, history['val_loss'][history['phase1_epochs']:], \n",
    "             'r--', linewidth=2, label='Fase 2 - Val')\n",
    "    \n",
    "    plt.axvline(x=history['phase1_epochs'], color='gray', linestyle=':', alpha=0.7)\n",
    "    plt.title('EfficientNet: Loss - 2 Phases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. MATRIZ DE CONFUS√ÉO RAW (DESBALANCEADA)\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    emotion_names = list(EMOTION_LABELS.keys())\n",
    "    \n",
    "    # Matriz bruta mostra o desbalanceamento\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax3,\n",
    "                cbar_kws={'label': 'N√∫mero de Amostras'})\n",
    "    plt.title('Matriz Confus√£o RAW\\n(Mostra Desbalanceamento)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 4. MATRIZ DE CONFUS√ÉO NORMALIZADA (POR LINHA)\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    \n",
    "    # Normaliza√ß√£o por linha (recall por classe)\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Substitui NaN por 0 (caso alguma classe n√£o tenha amostras)\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.3f', cmap='Greens',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax4,\n",
    "                cbar_kws={'label': 'Propor√ß√£o (Recall)'})\n",
    "    plt.title('Matriz Confus√£o NORMALIZADA\\n(Recall por Classe)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 5. DISTRIBUI√á√ÉO DE CLASSES (DESBALANCEAMENTO)\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    \n",
    "    # Conta amostras por classe no conjunto de teste\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    # Ordena por quantidade\n",
    "    sorted_classes = sorted(class_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "    class_names_sorted = [emotion_names[i] for i, count in sorted_classes]\n",
    "    class_counts_sorted = [count for i, count in sorted_classes]\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(class_names_sorted)))\n",
    "    bars = plt.bar(class_names_sorted, class_counts_sorted, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('Distribui√ß√£o de Classes - Teste\\n(Dados Desbalanceados)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('N√∫mero de Amostras')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar, count in zip(bars, class_counts_sorted):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adiciona linha da m√©dia\n",
    "    mean_samples = np.mean(class_counts_sorted)\n",
    "    plt.axhline(y=mean_samples, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'M√©dia: {mean_samples:.1f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 6. F1-SCORE POR EMO√á√ÉO COM AN√ÅLISE DE DESBALANCEAMENTO\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    \n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Criar gr√°fico de barras colorido por quantidade de amostras\n",
    "    colors = plt.cm.viridis(np.array(support_counts) / max(support_counts))\n",
    "    bars = plt.bar(emotion_names, f1_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    plt.title('F1-Score por Emo√ß√£o\\n(Cor = Qtd Amostras)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Adiciona valores e support\n",
    "    for bar, score, support in zip(bars, f1_scores, support_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}\\n(n={support})', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Colorbar para indicar quantidade de amostras\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, \n",
    "                               norm=plt.Normalize(vmin=min(support_counts), vmax=max(support_counts)))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, ax=ax6, label='Amostras de Teste')\n",
    "    \n",
    "    # 7. PRECISION, RECALL, F1 POR CLASSE\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    \n",
    "    precision_scores = [class_report[emotion]['precision'] for emotion in emotion_names]\n",
    "    recall_scores = [class_report[emotion]['recall'] for emotion in emotion_names]\n",
    "    \n",
    "    x = np.arange(len(emotion_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = plt.bar(x - width, precision_scores, width, label='Precision', alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x, recall_scores, width, label='Recall', alpha=0.8, color='lightblue')\n",
    "    bars3 = plt.bar(x + width, f1_scores, width, label='F1-Score', alpha=0.8, color='lightgreen')\n",
    "    \n",
    "    plt.title('M√©tricas Detalhadas por Classe', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Emo√ß√£o')\n",
    "    plt.xticks(x, emotion_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # 8. AN√ÅLISE DE CORRELA√á√ÉO DESBALANCEAMENTO vs PERFORMANCE\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    \n",
    "    # Scatter plot: Quantidade de amostras vs F1-Score\n",
    "    plt.scatter(support_counts, f1_scores, c=support_counts, cmap='viridis', \n",
    "                s=100, alpha=0.7, edgecolors='black')\n",
    "    \n",
    "    # Adiciona labels para cada ponto\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        plt.annotate(emotion, (support_counts[i], f1_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Linha de tend√™ncia\n",
    "    z = np.polyfit(support_counts, f1_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(support_counts, p(support_counts), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.title('Amostras vs Performance\\n(Correla√ß√£o)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('N√∫mero de Amostras de Teste')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcula correla√ß√£o\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correla√ß√£o: {correlation:.3f}', \n",
    "             transform=ax8.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # 9. HEATMAP DE ERROS POR CLASSE\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    \n",
    "    # Matriz de erros (off-diagonal elements)\n",
    "    error_matrix = conf_matrix.copy()\n",
    "    np.fill_diagonal(error_matrix, 0)  # Remove diagonal (acertos)\n",
    "    \n",
    "    # Normaliza por linha para mostrar propor√ß√£o de erros\n",
    "    error_matrix_norm = error_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    error_matrix_norm = np.nan_to_num(error_matrix_norm)\n",
    "    \n",
    "    sns.heatmap(error_matrix_norm, annot=True, fmt='.3f', cmap='Reds',\n",
    "                xticklabels=emotion_names, yticklabels=emotion_names, ax=ax9,\n",
    "                cbar_kws={'label': 'Propor√ß√£o de Erros'})\n",
    "    plt.title('Heatmap de Erros\\n(Confus√µes entre Classes)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Classe Verdadeira')\n",
    "    plt.xlabel('Classe Predita (Erro)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 10. M√âTRICAS COMPARATIVAS WEIGHTED vs MACRO\n",
    "    ax10 = plt.subplot(3, 4, 10)\n",
    "    \n",
    "    # Calcula m√©tricas weighted e macro\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    metrics_comparison = {\n",
    "        'Precision': [precision_macro, precision_weighted],\n",
    "        'Recall': [recall_macro, recall_weighted],\n",
    "        'F1-Score': [f1_macro, f1_weighted]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(metrics_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    macro_values = [metrics_comparison[metric][0] for metric in metrics_comparison]\n",
    "    weighted_values = [metrics_comparison[metric][1] for metric in metrics_comparison]\n",
    "    \n",
    "    bars1 = plt.bar(x - width/2, macro_values, width, label='Macro (Unbalanced)', \n",
    "                   alpha=0.8, color='lightcoral')\n",
    "    bars2 = plt.bar(x + width/2, weighted_values, width, label='Weighted (Balanced)', \n",
    "                   alpha=0.8, color='lightblue')\n",
    "    \n",
    "    plt.title('Macro vs Weighted Metrics\\n(Impacto do Desbalanceamento)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('M√©trica')\n",
    "    plt.xticks(x, metrics_comparison.keys())\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 11. RECURSOS COMPUTACIONAIS E EFICI√äNCIA\n",
    "    ax11 = plt.subplot(3, 4, 11)\n",
    "    \n",
    "    resource_data = {\n",
    "        'Tempo (min)': metrics['training_time_seconds'] / 60,\n",
    "        'Mem√≥ria (GB)': metrics['peak_memory_mb'] / 1024,\n",
    "        'Par√¢metros (M)': metrics['total_parameters'] / 1_000_000,\n",
    "        'Efici√™ncia\\n(Acc/M_params)': metrics['test_accuracy'] / (metrics['total_parameters'] / 1_000_000)\n",
    "    }\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    bars = plt.bar(range(len(resource_data)), list(resource_data.values()), \n",
    "                  color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.title('Recursos Computacionais\\nEfficientNet-B0', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(range(len(resource_data)), resource_data.keys(), rotation=45)\n",
    "    plt.ylabel('Valor')\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar, (key, value) in zip(bars, resource_data.items()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.02,\n",
    "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 12. RESUMO CIENT√çFICO\n",
    "    ax12 = plt.subplot(3, 4, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    # Calcula estat√≠sticas do desbalanceamento\n",
    "    imbalance_ratio = max(support_counts) / min(support_counts)\n",
    "    class_std = np.std(support_counts)\n",
    "    class_cv = class_std / np.mean(support_counts)  # Coefficient of variation\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "EFFICIENTNET-B0 PYTORCH\n",
    "AN√ÅLISE CIENT√çFICA\n",
    "\n",
    "PERFORMANCE:\n",
    "‚Ä¢ Accuracy: {metrics['test_accuracy']:.4f}\n",
    "‚Ä¢ F1-Macro: {f1_macro:.4f}\n",
    "‚Ä¢ F1-Weighted: {f1_weighted:.4f}\n",
    "\n",
    "DESBALANCEAMENTO:\n",
    "‚Ä¢ Ratio max/min: {imbalance_ratio:.1f}x\n",
    "‚Ä¢ Coef. Varia√ß√£o: {class_cv:.3f}\n",
    "‚Ä¢ Correla√ß√£o amostras-F1: {correlation:.3f}\n",
    "\n",
    "EFICI√äNCIA:\n",
    "‚Ä¢ Par√¢metros: {metrics['total_parameters']/1_000_000:.1f}M\n",
    "‚Ä¢ Tempo: {metrics['training_time_seconds']/60:.1f} min\n",
    "‚Ä¢ Mem√≥ria: {metrics['peak_memory_mb']/1024:.2f} GB\n",
    "\n",
    "CONCLUS√ÉO:\n",
    "Modelo {('robusto' if f1_macro > 0.7 else 'limitado')} para dados\n",
    "desbalanceados. {'Boa' if correlation > 0.3 else 'Baixa'} correla√ß√£o\n",
    "entre quantidade de amostras e performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.05, 0.95, summary_text, fontsize=11, verticalalignment='top',\n",
    "             transform=ax12.transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva a figura\n",
    "    os.makedirs('plots/efficientnet', exist_ok=True)\n",
    "    plt.savefig(f'plots/efficientnet/efficientnet_comprehensive_analysis_pytorch_{experiment_id}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # === AN√ÅLISE TEXTUAL DETALHADA ===\n",
    "    print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names)\n",
    "\n",
    "def print_detailed_imbalance_analysis(y_true, y_pred, conf_matrix, class_report, emotion_names):\n",
    "    \"\"\"An√°lise textual detalhada do impacto do desbalanceamento\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISE DETALHADA - DADOS DESBALANCEADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Estat√≠sticas de desbalanceamento\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    class_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    print(\"DISTRIBUI√á√ÉO DE CLASSES:\")\n",
    "    total_samples = len(y_true)\n",
    "    for i, emotion in enumerate(emotion_names):\n",
    "        count = class_distribution.get(i, 0)\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  ‚Ä¢ {emotion}: {count} amostras ({percentage:.1f}%)\")\n",
    "    \n",
    "    # An√°lise de desbalanceamento\n",
    "    max_samples = max(counts)\n",
    "    min_samples = min(counts)\n",
    "    imbalance_ratio = max_samples / min_samples\n",
    "    class_std = np.std(counts)\n",
    "    class_cv = class_std / np.mean(counts)\n",
    "    \n",
    "    print(f\"\\nESTAT√çSTICAS DE DESBALANCEAMENTO:\")\n",
    "    print(f\"  ‚Ä¢ Ratio m√°ximo/m√≠nimo: {imbalance_ratio:.2f}x\")\n",
    "    print(f\"  ‚Ä¢ Desvio padr√£o: {class_std:.1f} amostras\")\n",
    "    print(f\"  ‚Ä¢ Coeficiente de varia√ß√£o: {class_cv:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Classe majorit√°ria: {emotion_names[np.argmax(counts)]} ({max_samples} amostras)\")\n",
    "    print(f\"  ‚Ä¢ Classe minorit√°ria: {emotion_names[np.argmin(counts)]} ({min_samples} amostras)\")\n",
    "    \n",
    "    # Impacto na performance\n",
    "    print(f\"\\nIMPACTO NA PERFORMANCE:\")\n",
    "    f1_scores = [class_report[emotion]['f1-score'] for emotion in emotion_names]\n",
    "    support_counts = [class_report[emotion]['support'] for emotion in emotion_names]\n",
    "    \n",
    "    # Correla√ß√£o entre quantidade e performance\n",
    "    correlation = np.corrcoef(support_counts, f1_scores)[0, 1]\n",
    "    print(f\"  ‚Ä¢ Correla√ß√£o amostras-F1: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation > 0.5:\n",
    "        print(f\"    ‚Üí FORTE correla√ß√£o positiva: mais amostras = melhor F1\")\n",
    "    elif correlation > 0.3:\n",
    "        print(f\"    ‚Üí MODERADA correla√ß√£o positiva\")\n",
    "    elif correlation > -0.3:\n",
    "        print(f\"    ‚Üí FRACA correla√ß√£o: modelo relativamente robusto\")\n",
    "    else:\n",
    "        print(f\"    ‚Üí Correla√ß√£o negativa: poss√≠vel overfitting em classes majorit√°rias\")\n",
    "    \n",
    "    # Classes com melhor/pior performance\n",
    "    best_class_idx = np.argmax(f1_scores)\n",
    "    worst_class_idx = np.argmin(f1_scores)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Melhor F1: {emotion_names[best_class_idx]} ({f1_scores[best_class_idx]:.3f}) - {support_counts[best_class_idx]} amostras\")\n",
    "    print(f\"  ‚Ä¢ Pior F1: {emotion_names[worst_class_idx]} ({f1_scores[worst_class_idx]:.3f}) - {support_counts[worst_class_idx]} amostras\")\n",
    "    \n",
    "    # An√°lise de confus√µes\n",
    "    print(f\"\\nMAIORES CONFUS√ïES:\")\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    conf_matrix_norm = np.nan_to_num(conf_matrix_norm)\n",
    "    \n",
    "    # Encontra maiores erros (off-diagonal)\n",
    "    np.fill_diagonal(conf_matrix_norm, 0)\n",
    "    max_confusion_idx = np.unravel_index(np.argmax(conf_matrix_norm), conf_matrix_norm.shape)\n",
    "    max_confusion_value = conf_matrix_norm[max_confusion_idx]\n",
    "    \n",
    "    print(f\"  ‚Ä¢ {emotion_names[max_confusion_idx[0]]} ‚Üí {emotion_names[max_confusion_idx[1]]}: {max_confusion_value:.3f}\")\n",
    "    \n",
    "    # Top 3 confus√µes\n",
    "    flat_indices = np.argsort(conf_matrix_norm.flatten())[-3:][::-1]\n",
    "    for idx in flat_indices:\n",
    "        i, j = np.unravel_index(idx, conf_matrix_norm.shape)\n",
    "        if i != j:  # Ignora diagonal\n",
    "            print(f\"  ‚Ä¢ {emotion_names[i]} ‚Üí {emotion_names[j]}: {conf_matrix_norm[i, j]:.3f}\")\n",
    "    \n",
    "    # Recomenda√ß√µes\n",
    "    print(f\"\\nRECOMENDA√á√ïES:\")\n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"  ‚Ä¢ Desbalanceamento SEVERO - considere t√©cnicas de balanceamento\")\n",
    "        print(\"  ‚Ä¢ Sugest√µes: SMOTE, class weighting, focal loss\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(\"  ‚Ä¢ Desbalanceamento MODERADO - monitorar m√©tricas por classe\")\n",
    "        print(\"  ‚Ä¢ Sugest√µes: class weighting, stratified sampling\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Desbalanceamento LEVE - modelo pode lidar adequadamente\")\n",
    "    \n",
    "    if correlation < 0.3:\n",
    "        print(\"  ‚Ä¢ Modelo demonstra robustez ao desbalanceamento\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ Considere estrat√©gias para classes minorit√°rias\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize monitor\n",
    "    monitor = EfficientNetMonitor()\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    # Generate experiment ID\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_id = f\"efficientnet_pytorch_emotion_{timestamp}\"\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('plots/efficientnet', exist_ok=True)\n",
    "    os.makedirs('models/efficientnet', exist_ok=True)\n",
    "    os.makedirs('metrics/efficientnet', exist_ok=True)\n",
    "    \n",
    "    print(f\"Experiment ID: {experiment_id}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_test, y_test = load_preprocessed_data_efficientnet_from_images()\n",
    "    \n",
    "    if X_train is not None:\n",
    "        # Split training data into train and validation\n",
    "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            stratify=y_train,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Data split:\")\n",
    "        print(f\"  ‚Ä¢ Train: {X_train_split.shape}\")\n",
    "        print(f\"  ‚Ä¢ Validation: {X_val.shape}\")\n",
    "        print(f\"  ‚Ä¢ Test: {X_test.shape}\")\n",
    "        \n",
    "        # Analyze data distribution\n",
    "        print(f\"\\nData distribution analysis:\")\n",
    "        unique_train, counts_train = np.unique(y_train_split, return_counts=True)\n",
    "        unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "        \n",
    "        emotion_names = list(EMOTION_LABELS.keys())\n",
    "        print(\"Training set:\")\n",
    "        for i, count in enumerate(counts_train):\n",
    "            print(f\"  ‚Ä¢ {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        print(\"Test set:\")\n",
    "        for i, count in enumerate(counts_test):\n",
    "            print(f\"  ‚Ä¢ {emotion_names[i]}: {count} samples\")\n",
    "        \n",
    "        test_imbalance_ratio = max(counts_test) / min(counts_test)\n",
    "        print(f\"Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(\n",
    "            X_train_split, y_train_split, X_val, y_val, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        print(\"\\nCreating EfficientNet-B0 model...\")\n",
    "        model = EfficientNetEmotionClassifier(num_classes=7, dropout_rate=EFFICIENTNET_CONFIG['dropout_rate'])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"EfficientNet-B0 created successfully:\")\n",
    "        print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "        print(f\"  ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  ‚Ä¢ Efficiency: {total_params/1000000:.1f}M parameters\")\n",
    "        print(f\"  ‚Ä¢ Trainable ratio: {(trainable_params/total_params)*100:.1f}%\")\n",
    "        \n",
    "        # Update monitor with GPU memory if available\n",
    "        if torch.cuda.is_available():\n",
    "            initial_gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "            print(f\"  ‚Ä¢ Initial GPU memory: {initial_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        history = train_efficientnet_two_phase(model, train_loader, val_loader, monitor)\n",
    "        \n",
    "        # Load best model for evaluation\n",
    "        if os.path.exists('best_efficientnet_model.pth'):\n",
    "            model.load_state_dict(torch.load('best_efficientnet_model.pth'))\n",
    "            print(\"Loaded best model weights for evaluation\")\n",
    "        \n",
    "        # Update memory tracking\n",
    "        monitor.update_peak_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            peak_gpu_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "            print(f\"Peak GPU memory: {peak_gpu_memory:.1f} MB\")\n",
    "        \n",
    "        # Comprehensive evaluation and visualization\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING COMPREHENSIVE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Enhanced evaluation with visualization\n",
    "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "        \n",
    "        # Detailed evaluation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        inference_times = []\n",
    "        \n",
    "        print(\"Measuring inference performance...\")\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "                start_time = time.time()\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                inference_time = time.time() - start_time\n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "                _, predicted = output.max(1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Weighted metrics for imbalanced data\n",
    "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "            all_targets, all_preds, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "        class_report = classification_report(\n",
    "            all_targets, all_preds,\n",
    "            target_names=emotion_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Compile comprehensive metrics\n",
    "        avg_inference_time = np.mean(inference_times)\n",
    "        total_inference_time = sum(inference_times)\n",
    "        samples_per_second = len(all_targets) / total_inference_time\n",
    "        \n",
    "        metrics = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'test_accuracy': accuracy,\n",
    "            'f1_score_macro': f1,\n",
    "            'f1_score_weighted': f1_weighted,\n",
    "            'precision_macro': precision,\n",
    "            'precision_weighted': precision_weighted,\n",
    "            'recall_macro': recall,\n",
    "            'recall_weighted': recall_weighted,\n",
    "            'training_time_seconds': history['phase1_duration'] + history['phase2_duration'],\n",
    "            'phase1_duration': history['phase1_duration'],\n",
    "            'phase2_duration': history['phase2_duration'],\n",
    "            'peak_memory_mb': monitor.peak_memory_mb,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'avg_inference_time_seconds': avg_inference_time,\n",
    "            'samples_per_second': samples_per_second,\n",
    "            'test_samples': len(all_targets),\n",
    "            'imbalance_ratio': test_imbalance_ratio,\n",
    "            'best_val_acc_phase1': history['best_val_acc_phase1'],\n",
    "            'best_val_acc_phase2': history['best_val_acc_phase2']\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nCOMPREHENSIVE RESULTS:\")\n",
    "        print(f\"  ‚Ä¢ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"  ‚Ä¢ F1-Score (Macro): {f1:.4f}\")\n",
    "        print(f\"  ‚Ä¢ F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Precision (Macro): {precision:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Recall (Macro): {recall:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Inference Speed: {samples_per_second:.1f} samples/second\")\n",
    "        print(f\"  ‚Ä¢ Total Training Time: {timedelta(seconds=int(metrics['training_time_seconds']))}\")\n",
    "        \n",
    "        # Create comprehensive visualizations with imbalance analysis\n",
    "        print(\"\\nGenerating comprehensive visualizations...\")\n",
    "        create_comprehensive_visualizations_efficientnet_pytorch(\n",
    "            history, conf_matrix, metrics, class_report, experiment_id, all_targets, all_preds\n",
    "        )\n",
    "        \n",
    "        # Save model if performance is good\n",
    "        performance_threshold = 0.70  # Adjust based on your requirements\n",
    "        if accuracy >= performance_threshold:\n",
    "            model_path = f'models/efficientnet/efficientnet_model_{experiment_id}.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'history': history,\n",
    "                'model_config': {\n",
    "                    'num_classes': 7,\n",
    "                    'dropout_rate': EFFICIENTNET_CONFIG['dropout_rate'],\n",
    "                    'img_size': IMG_SIZE,\n",
    "                    'architecture': 'EfficientNet-B0'\n",
    "                }\n",
    "            }, model_path)\n",
    "            print(f\"Model saved: {model_path}\")\n",
    "        else:\n",
    "            print(f\"Model not saved (accuracy {accuracy:.4f} < {performance_threshold})\")\n",
    "        \n",
    "        # Save metrics to CSV\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_csv = f'metrics/efficientnet/efficientnet_metrics_{experiment_id}.csv'\n",
    "        metrics_df.to_csv(metrics_csv, index=False)\n",
    "        print(f\"Metrics saved: {metrics_csv}\")\n",
    "        \n",
    "        # End monitoring\n",
    "        monitor_stats = monitor.end_monitoring()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Experiment ID: {experiment_id}\")\n",
    "        print(f\"Files generated:\")\n",
    "        print(f\"  ‚Ä¢ Visualizations: plots/efficientnet/efficientnet_comprehensive_analysis_pytorch_{experiment_id}.png\")\n",
    "        print(f\"  ‚Ä¢ Metrics: {metrics_csv}\")\n",
    "        if accuracy >= performance_threshold:\n",
    "            print(f\"  ‚Ä¢ Model: models/efficientnet/efficientnet_model_{experiment_id}.pth\")\n",
    "        print(f\"  ‚Ä¢ Best model: best_efficientnet_model.pth\")\n",
    "        \n",
    "        # Final summary for imbalanced data\n",
    "        print(f\"\\nIMBALANCE ANALYSIS SUMMARY:\")\n",
    "        print(f\"  ‚Ä¢ Test set imbalance ratio: {test_imbalance_ratio:.1f}:1\")\n",
    "        print(f\"  ‚Ä¢ Macro F1 (unweighted): {f1:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Weighted F1 (balanced): {f1_weighted:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Performance difference: {abs(f1_weighted - f1):.4f}\")\n",
    "        \n",
    "        if f1_weighted - f1 > 0.05:\n",
    "            print(\"  ‚Üí Model benefits significantly from class weighting\")\n",
    "        elif abs(f1_weighted - f1) < 0.02:\n",
    "            print(\"  ‚Üí Model is robust to class imbalance\")\n",
    "        else:\n",
    "            print(\"  ‚Üí Moderate impact of class imbalance\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to load data. Please check the data path and directory structure.\")\n",
    "        print(\"Expected structure: ../data/augmented/raf_db_balanced/train|test/Emotion_Name/*.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
