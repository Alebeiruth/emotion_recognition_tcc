{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Imports e Configuração\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração\n",
    "BASE_PATH = r'../data/raw/RAF-DB/DATASET'\n",
    "OUTPUT_PATH = r'../data/processed/RAF-DB'\n",
    "TARGET_SIZE = (224, 224)\n",
    "LOG_PATH = r'../data/logs'\n",
    "\n",
    "print(\"🔧 Configuração do Pré-processamento\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📁 Input:  {BASE_PATH}\")\n",
    "print(f\"📁 Output: {OUTPUT_PATH}\")\n",
    "print(f\"📁 Logs:   {LOG_PATH}\")\n",
    "print(f\"📐 Target Size: {TARGET_SIZE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Verificação de Ambiente\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Verificar instalações necessárias\n",
    "required_packages = ['cv2', 'mtcnn', 'tensorflow', 'tqdm', 'matplotlib']\n",
    "\n",
    "print(\"📦 Verificando pacotes necessários...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✅ {package} instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"❌ {package} não encontrado\")\n",
    "        print(f\"   Execute: pip install {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Verificação de Caminhos\n",
    "# Verificar se o caminho base existe\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    print(f\"❌ ERRO: Caminho base não encontrado: {BASE_PATH}\")\n",
    "    print(f\"\\n📂 Estrutura de diretórios esperada:\")\n",
    "    print(\"\"\"\n",
    "    data/\n",
    "    └── raw/\n",
    "        └── RAF-DB/\n",
    "            ├── train/\n",
    "            │   ├── anger/\n",
    "            │   ├── disgust/\n",
    "            │   ├── fear/\n",
    "            │   ├── happiness/\n",
    "            │   ├── neutral/\n",
    "            │   ├── sadness/\n",
    "            │   └── surprise/\n",
    "            └── test/\n",
    "                ├── anger/\n",
    "                ├── disgust/\n",
    "                ├── fear/\n",
    "                ├── happiness/\n",
    "                ├── neutral/\n",
    "                ├── sadness/\n",
    "                └── surprise/\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"\\n🔍 Procurando por RAF-DB no diretório atual...\")\n",
    "    found = False\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        if 'RAF-DB' in root or 'raf-db' in root.lower():\n",
    "            print(f\"  📁 Encontrado: {root}\")\n",
    "            found = True\n",
    "    \n",
    "    if not found:\n",
    "        print(\"  ❌ Nenhum diretório RAF-DB encontrado\")\n",
    "else:\n",
    "    print(f\"✅ Caminho base encontrado: {BASE_PATH}\")\n",
    "    \n",
    "    # Criar pastas necessárias\n",
    "    os.makedirs(LOG_PATH, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    print(f\"✅ Pastas de saída criadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c88ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Inicialização do MTCNN\n",
    "# Inicializar o detector\n",
    "detector = None\n",
    "try:\n",
    "    # Tentar inicialização com parâmetros completos\n",
    "    try:\n",
    "        detector = MTCNN(\n",
    "            min_face_size=20,\n",
    "            scale_factor=0.709,\n",
    "            steps_threshold=[0.6, 0.7, 0.7]\n",
    "        )\n",
    "        print(\"✅ MTCNN inicializado com sucesso (com parâmetros)\")\n",
    "    except TypeError:\n",
    "        # Fallback: inicialização padrão sem parâmetros\n",
    "        detector = MTCNN()\n",
    "        print(\"✅ MTCNN inicializado com sucesso (parâmetros padrão)\")\n",
    "    \n",
    "    print(\"\\nDetector MTCNN pronto para uso!\")\n",
    "    print(\"  • Biblioteca: mtcnn\")\n",
    "    print(\"  • Backend: TensorFlow/Keras\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERRO ao inicializar MTCNN: {e}\")\n",
    "    print(\"\\n🔧 Solução:\")\n",
    "    print(\"  1. Desinstale versões conflitantes:\")\n",
    "    print(\"     pip uninstall mtcnn\")\n",
    "    print(\"  2. Instale a versão correta:\")\n",
    "    print(\"     pip install mtcnn\")\n",
    "    print(\"  3. Verifique se o TensorFlow está instalado:\")\n",
    "    print(\"     pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e778e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Estrutura de Logging\n",
    "# Estrutura para logging\n",
    "processing_stats = defaultdict(lambda: {\n",
    "    'total': 0,\n",
    "    'processed': 0,\n",
    "    'failed_read': [],\n",
    "    'no_face_detected': [],\n",
    "    'alignment_failed': [],\n",
    "    'crop_failed': [],\n",
    "    'other_errors': []\n",
    "})\n",
    "\n",
    "print(\"📊 Sistema de logging inicializado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Funções Auxiliares\n",
    "def validate_image(img_path):\n",
    "    \"\"\"\n",
    "    Valida se o arquivo é uma imagem válida.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return False, None\n",
    "        if img.shape[0] < 10 or img.shape[1] < 10:\n",
    "            return False, None\n",
    "        return True, img\n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "def safe_crop(image, bbox, padding=0.2):\n",
    "    \"\"\"\n",
    "    Recorta face com padding e tratamento de bordas.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x, y, w, h = bbox\n",
    "        \n",
    "        # Validar bbox\n",
    "        if w <= 0 or h <= 0:\n",
    "            return None\n",
    "            \n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Adicionar padding\n",
    "        pad_w = int(w * padding)\n",
    "        pad_h = int(h * padding)\n",
    "        \n",
    "        # Calcular nova bbox com padding\n",
    "        x1 = max(0, x - pad_w)\n",
    "        y1 = max(0, y - pad_h)\n",
    "        x2 = min(width, x + w + pad_w)\n",
    "        y2 = min(height, y + h + pad_h)\n",
    "        \n",
    "        # Validar coordenadas\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return None\n",
    "            \n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "        \n",
    "        if cropped.size == 0 or cropped.shape[0] < 10 or cropped.shape[1] < 10:\n",
    "            return None\n",
    "        \n",
    "        return cropped\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no crop: {e}\")\n",
    "        return None\n",
    "\n",
    "def align_face_safe(image, left_eye, right_eye, expand_factor=1.2):\n",
    "    \"\"\"\n",
    "    Alinhamento com expansão de canvas para evitar cortes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validar coordenadas dos olhos\n",
    "        if left_eye[0] == right_eye[0] and left_eye[1] == right_eye[1]:\n",
    "            return image  # Olhos no mesmo ponto, retornar original\n",
    "            \n",
    "        eye_dx = right_eye[0] - left_eye[0]\n",
    "        eye_dy = right_eye[1] - left_eye[1]\n",
    "        angle = np.degrees(np.arctan2(eye_dy, eye_dx))\n",
    "        \n",
    "        # Se o ângulo é muito pequeno, não rotacionar\n",
    "        if abs(angle) < 1.0:\n",
    "            return image\n",
    "        \n",
    "        (h, w) = image.shape[:2]\n",
    "        \n",
    "        # Limitar o fator de expansão\n",
    "        expand_factor = min(expand_factor, 1.5)\n",
    "        \n",
    "        # Expandir canvas para evitar cortes durante rotação\n",
    "        new_h = int(h * expand_factor)\n",
    "        new_w = int(w * expand_factor)\n",
    "        \n",
    "        # Criar imagem expandida\n",
    "        if len(image.shape) == 3:\n",
    "            expanded = np.zeros((new_h, new_w, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            expanded = np.zeros((new_h, new_w), dtype=np.uint8)\n",
    "            \n",
    "        y_offset = (new_h - h) // 2\n",
    "        x_offset = (new_w - w) // 2\n",
    "        expanded[y_offset:y_offset+h, x_offset:x_offset+w] = image\n",
    "        \n",
    "        # Centro da imagem expandida\n",
    "        center = (new_w // 2, new_h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        aligned = cv2.warpAffine(expanded, M, (new_w, new_h), \n",
    "                                 flags=cv2.INTER_CUBIC,\n",
    "                                 borderMode=cv2.BORDER_REPLICATE)\n",
    "        \n",
    "        return aligned\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no alinhamento: {e}\")\n",
    "        return image\n",
    "\n",
    "print(\"✅ Funções auxiliares carregadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad607d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Função Principal de Processamento\n",
    "def process_with_fallback(img_path, output_path, emotion, split):\n",
    "    \"\"\"\n",
    "    Processa imagem com múltiplas estratégias de fallback.\n",
    "    \"\"\"\n",
    "    stats_key = f\"{split}/{emotion}\"\n",
    "    stats = processing_stats[stats_key]\n",
    "    stats['total'] += 1\n",
    "    \n",
    "    # Validar e ler imagem\n",
    "    is_valid, img = validate_image(img_path)\n",
    "    if not is_valid:\n",
    "        stats['failed_read'].append(os.path.basename(img_path))\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Estratégia 1: Detecção padrão\n",
    "        results = detector.detect_faces(img)\n",
    "        \n",
    "        # Estratégia 2: Se falhar, tentar com imagem equalizada\n",
    "        if not results and len(img.shape) == 3:\n",
    "            gray_temp = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            equalized = cv2.equalizeHist(gray_temp)\n",
    "            img_eq = cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR)\n",
    "            results = detector.detect_faces(img_eq)\n",
    "            if results:\n",
    "                img = img_eq  # Usar imagem equalizada\n",
    "            \n",
    "        # Estratégia 3: Se ainda falhar, tentar com CLAHE\n",
    "        if not results and len(img.shape) == 3:\n",
    "            lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "            l_clahe = clahe.apply(l)\n",
    "            lab_clahe = cv2.merge([l_clahe, a, b])\n",
    "            img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "            results = detector.detect_faces(img_clahe)\n",
    "            if results:\n",
    "                img = img_clahe\n",
    "        \n",
    "        if not results:\n",
    "            stats['no_face_detected'].append(os.path.basename(img_path))\n",
    "            \n",
    "            # Fallback: salvar crop central\n",
    "            h, w = img.shape[:2]\n",
    "            center_size = min(h, w) // 2\n",
    "            y_start = (h - center_size) // 2\n",
    "            x_start = (w - center_size) // 2\n",
    "            center_crop = img[y_start:y_start+center_size, x_start:x_start+center_size]\n",
    "            \n",
    "            if center_crop.size > 0:\n",
    "                resized = cv2.resize(center_crop, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n",
    "                if len(resized.shape) == 3:\n",
    "                    gray_face = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "                else:\n",
    "                    gray_face = resized\n",
    "                cv2.imwrite(output_path, gray_face)\n",
    "                stats['processed'] += 1\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        # Processar com detecção bem-sucedida\n",
    "        face_data = results[0]\n",
    "        \n",
    "        # Verificar se tem keypoints para alinhamento\n",
    "        if 'keypoints' in face_data:\n",
    "            keypoints = face_data['keypoints']\n",
    "            if 'left_eye' in keypoints and 'right_eye' in keypoints:\n",
    "                left_eye = keypoints['left_eye']\n",
    "                right_eye = keypoints['right_eye']\n",
    "                \n",
    "                # Alinhamento seguro\n",
    "                aligned_img = align_face_safe(img, left_eye, right_eye)\n",
    "                \n",
    "                # Re-detectar na imagem alinhada\n",
    "                results_aligned = detector.detect_faces(aligned_img)\n",
    "                \n",
    "                if results_aligned:\n",
    "                    face_img = safe_crop(aligned_img, results_aligned[0]['box'])\n",
    "                else:\n",
    "                    face_img = safe_crop(img, face_data['box'])\n",
    "            else:\n",
    "                face_img = safe_crop(img, face_data['box'])\n",
    "        else:\n",
    "            face_img = safe_crop(img, face_data['box'])\n",
    "        \n",
    "        if face_img is None:\n",
    "            stats['crop_failed'].append(os.path.basename(img_path))\n",
    "            return False\n",
    "        \n",
    "        # Redimensionar e converter para escala de cinza\n",
    "        resized = cv2.resize(face_img, TARGET_SIZE, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        if len(resized.shape) == 3:\n",
    "            gray_face = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray_face = resized\n",
    "        \n",
    "        # Salvar imagem processada\n",
    "        success = cv2.imwrite(output_path, gray_face)\n",
    "        if success:\n",
    "            stats['processed'] += 1\n",
    "            return True\n",
    "        else:\n",
    "            stats['other_errors'].append({\n",
    "                'file': os.path.basename(img_path),\n",
    "                'error': 'Falha ao salvar imagem'\n",
    "            })\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        stats['other_errors'].append({\n",
    "            'file': os.path.basename(img_path),\n",
    "            'error': str(e),\n",
    "            'traceback': traceback.format_exc()\n",
    "        })\n",
    "        return False\n",
    "\n",
    "print(\"✅ Função de processamento carregada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Análise da Estrutura do Dataset\n",
    "print(\"=\"*60)\n",
    "print(\"📊 ANÁLISE DA ESTRUTURA DO DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset_info = {}\n",
    "\n",
    "# Verificar estrutura esperada\n",
    "for split in ['train', 'test']:\n",
    "    split_path = os.path.join(BASE_PATH, split)\n",
    "    if os.path.exists(split_path):\n",
    "        emotions = [d for d in os.listdir(split_path) \n",
    "                   if os.path.isdir(os.path.join(split_path, d))]\n",
    "        \n",
    "        dataset_info[split] = {}\n",
    "        print(f\"\\n✅ {split.upper()}: {len(emotions)} emoções encontradas\")\n",
    "        \n",
    "        total_split = 0\n",
    "        for emotion in emotions:\n",
    "            emotion_path = os.path.join(split_path, emotion)\n",
    "            n_images = len([f for f in os.listdir(emotion_path) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
    "            dataset_info[split][emotion] = n_images\n",
    "            total_split += n_images\n",
    "            print(f\"   • {emotion:12s}: {n_images:6d} imagens\")\n",
    "        \n",
    "        print(f\"   {'─'*30}\")\n",
    "        print(f\"   {'TOTAL':12s}: {total_split:6d} imagens\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {split.upper()}: pasta não encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 9: Visualização da Distribuição (Opcional)\n",
    "if dataset_info:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(dataset_info), figsize=(14, 6))\n",
    "    if len(dataset_info) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 7))\n",
    "    \n",
    "    for idx, (split, emotions_data) in enumerate(dataset_info.items()):\n",
    "        ax = axes[idx]\n",
    "        emotions = list(emotions_data.keys())\n",
    "        counts = list(emotions_data.values())\n",
    "        \n",
    "        bars = ax.bar(emotions, counts, color=colors)\n",
    "        ax.set_title(f'{split.upper()} Set Distribution')\n",
    "        ax.set_xlabel('Emotion')\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar, count in zip(bars, counts):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{count}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle('RAF-DB Dataset Distribution', fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a401dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10: Processamento Principal\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 INICIANDO PROCESSAMENTO...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    print(\"⚠️ Não é possível processar - caminho base não existe\")\n",
    "else:\n",
    "    # Processar imagens\n",
    "    for split in ['train', 'test']:\n",
    "        input_split_path = os.path.join(BASE_PATH, split)\n",
    "        output_split_path = os.path.join(OUTPUT_PATH, split)\n",
    "        \n",
    "        if not os.path.exists(input_split_path):\n",
    "            print(f\"\\n⚠️  Pulando {split}: pasta não encontrada\")\n",
    "            continue\n",
    "        \n",
    "        emotions = [d for d in os.listdir(input_split_path) \n",
    "                    if os.path.isdir(os.path.join(input_split_path, d))]\n",
    "        \n",
    "        print(f\"\\n📂 Processando {split}...\")\n",
    "        \n",
    "        # Usar tqdm notebook para melhor visualização\n",
    "        from tqdm.notebook import tqdm as tqdm_notebook\n",
    "        \n",
    "        for emotion in tqdm_notebook(emotions, desc=f\"{split}\"):\n",
    "            input_emotion_path = os.path.join(input_split_path, emotion)\n",
    "            output_emotion_path = os.path.join(output_split_path, emotion)\n",
    "            os.makedirs(output_emotion_path, exist_ok=True)\n",
    "            \n",
    "            images = [f for f in os.listdir(input_emotion_path) \n",
    "                     if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "            \n",
    "            # Processar com sub-progress bar\n",
    "            for image_name in tqdm_notebook(images, desc=f\"  {emotion}\", leave=False):\n",
    "                input_path = os.path.join(input_emotion_path, image_name)\n",
    "                output_path = os.path.join(output_emotion_path, image_name)\n",
    "                \n",
    "                # Pular se já foi processada\n",
    "                if os.path.exists(output_path):\n",
    "                    continue\n",
    "                    \n",
    "                process_with_fallback(input_path, output_path, emotion, split)\n",
    "    \n",
    "    print(\"\\n✅ Processamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 11: Salvar Estatísticas\n",
    "# Converter defaultdict para dict regular para serialização\n",
    "stats_to_save = {}\n",
    "for key, value in processing_stats.items():\n",
    "    stats_to_save[key] = dict(value)\n",
    "\n",
    "# Salvar estatísticas em JSON\n",
    "stats_file = os.path.join(LOG_PATH, 'raf_db_preprocessing_stats.json')\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(stats_to_save, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Estatísticas salvas em: {stats_file}\")\n",
    "\n",
    "# Também salvar um CSV para análise\n",
    "import pandas as pd\n",
    "\n",
    "stats_summary = []\n",
    "for key, stats in processing_stats.items():\n",
    "    split, emotion = key.split('/')\n",
    "    stats_summary.append({\n",
    "        'split': split,\n",
    "        'emotion': emotion,\n",
    "        'total': stats['total'],\n",
    "        'processed': stats['processed'],\n",
    "        'failed_read': len(stats['failed_read']),\n",
    "        'no_face': len(stats['no_face_detected']),\n",
    "        'crop_failed': len(stats['crop_failed']),\n",
    "        'other_errors': len(stats['other_errors'])\n",
    "    })\n",
    "\n",
    "if stats_summary:\n",
    "    df_stats = pd.DataFrame(stats_summary)\n",
    "    csv_file = os.path.join(LOG_PATH, 'raf_db_preprocessing_summary.csv')\n",
    "    df_stats.to_csv(csv_file, index=False)\n",
    "    print(f\"📊 Sumário salvo em: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 12: Relatório Final Detalhado\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RELATÓRIO DE PROCESSAMENTO DETALHADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_images = 0\n",
    "total_processed = 0\n",
    "total_failures = defaultdict(int)\n",
    "\n",
    "# Criar DataFrame para visualização\n",
    "results_data = []\n",
    "\n",
    "for key, stats in processing_stats.items():\n",
    "    if stats['total'] == 0:\n",
    "        continue\n",
    "        \n",
    "    split, emotion = key.split('/')\n",
    "    total = stats['total']\n",
    "    processed = stats['processed']\n",
    "    \n",
    "    success_rate = (processed/total*100) if total > 0 else 0\n",
    "    status = \"✅\" if success_rate > 90 else \"⚠️\" if success_rate > 70 else \"❌\"\n",
    "    \n",
    "    results_data.append({\n",
    "        'Split': split,\n",
    "        'Emotion': emotion,\n",
    "        'Status': status,\n",
    "        'Total': total,\n",
    "        'Processed': processed,\n",
    "        'Success Rate': f\"{success_rate:.1f}%\",\n",
    "        'Failed Read': len(stats['failed_read']),\n",
    "        'No Face': len(stats['no_face_detected']),\n",
    "        'Crop Failed': len(stats['crop_failed']),\n",
    "        'Other Errors': len(stats['other_errors'])\n",
    "    })\n",
    "    \n",
    "    # Acumular totais\n",
    "    total_images += total\n",
    "    total_processed += processed\n",
    "    \n",
    "    if stats['failed_read']:\n",
    "        total_failures['failed_read'] += len(stats['failed_read'])\n",
    "    if stats['no_face_detected']:\n",
    "        total_failures['no_face'] += len(stats['no_face_detected'])\n",
    "    if stats['crop_failed']:\n",
    "        total_failures['crop'] += len(stats['crop_failed'])\n",
    "    if stats['other_errors']:\n",
    "        total_failures['other'] += len(stats['other_errors'])\n",
    "\n",
    "# Mostrar tabela de resultados\n",
    "if results_data:\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    display(df_results.style.set_properties(**{'text-align': 'center'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2318b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 13: Resumo Geral e Gráficos\n",
    "if total_images > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📈 RESUMO GERAL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total de imagens: {total_images}\")\n",
    "    print(f\"✅ Processadas: {total_processed} ({total_processed/total_images*100:.1f}%)\")\n",
    "    print(f\"❌ Perdidas: {total_images - total_processed} ({(total_images-total_processed)/total_images*100:.1f}%)\")\n",
    "    \n",
    "    # Gráfico de pizza para causas de falha\n",
    "    if total_failures:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Gráfico 1: Sucesso vs Falha\n",
    "        success_data = [total_processed, total_images - total_processed]\n",
    "        success_labels = ['Processadas', 'Perdidas']\n",
    "        colors1 = ['#2ecc71', '#e74c3c']\n",
    "        \n",
    "        ax1.pie(success_data, labels=success_labels, colors=colors1, \n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        ax1.set_title('Taxa de Sucesso do Processamento')\n",
    "        \n",
    "        # Gráfico 2: Distribuição de Falhas\n",
    "        if sum(total_failures.values()) > 0:\n",
    "            failure_values = list(total_failures.values())\n",
    "            failure_labels = list(total_failures.keys())\n",
    "            colors2 = plt.cm.Set3(np.linspace(0, 1, len(failure_labels)))\n",
    "            \n",
    "            ax2.pie(failure_values, labels=failure_labels, colors=colors2, \n",
    "                    autopct='%1.1f%%', startangle=90)\n",
    "            ax2.set_title('Distribuição das Causas de Falha')\n",
    "        \n",
    "        plt.suptitle('Análise do Processamento RAF-DB', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n📉 Principais causas de perda:\")\n",
    "        for cause, count in sorted(total_failures.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   • {cause}: {count} ({count/total_images*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 14: Visualização de Exemplos Processados (Opcional)\n",
    "def show_processing_examples(n_examples=5):\n",
    "    \"\"\"\n",
    "    Mostra exemplos de imagens antes e depois do processamento\n",
    "    \"\"\"\n",
    "    print(\"🖼️ Exemplos de Processamento\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    processed_examples = []\n",
    "    \n",
    "    # Buscar exemplos processados\n",
    "    for split in ['train', 'test']:\n",
    "        output_split_path = os.path.join(OUTPUT_PATH, split)\n",
    "        if not os.path.exists(output_split_path):\n",
    "            continue\n",
    "            \n",
    "        for emotion in os.listdir(output_split_path):\n",
    "            emotion_path = os.path.join(output_split_path, emotion)\n",
    "            if not os.path.isdir(emotion_path):\n",
    "                continue\n",
    "                \n",
    "            for img_file in os.listdir(emotion_path)[:2]:  # Pegar 2 de cada emoção\n",
    "                if img_file.lower().endswith(('.jpg', '.png')):\n",
    "                    processed_examples.append({\n",
    "                        'path': os.path.join(emotion_path, img_file),\n",
    "                        'emotion': emotion,\n",
    "                        'split': split\n",
    "                    })\n",
    "                    \n",
    "                if len(processed_examples) >= n_examples:\n",
    "                    break\n",
    "            \n",
    "            if len(processed_examples) >= n_examples:\n",
    "                break\n",
    "    \n",
    "    # Mostrar exemplos\n",
    "    if processed_examples:\n",
    "        fig, axes = plt.subplots(1, min(n_examples, len(processed_examples)), \n",
    "                                 figsize=(15, 4))\n",
    "        if len(processed_examples) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, example in enumerate(processed_examples[:n_examples]):\n",
    "            img = cv2.imread(example['path'], cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                axes[idx].imshow(img, cmap='gray')\n",
    "                axes[idx].set_title(f\"{example['emotion']}\\n({example['split']})\")\n",
    "                axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Exemplos de Faces Processadas (224x224, Grayscale)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Nenhum exemplo processado encontrado\")\n",
    "\n",
    "# Chamar a função\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    show_processing_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fde3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 15: Análise de Qualidade (Opcional)\n",
    "def analyze_processing_quality():\n",
    "    \"\"\"\n",
    "    Analisa a qualidade das imagens processadas\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Análise de Qualidade\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    quality_metrics = []\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        output_split_path = os.path.join(OUTPUT_PATH, split)\n",
    "        if not os.path.exists(output_split_path):\n",
    "            continue\n",
    "        \n",
    "        for emotion in os.listdir(output_split_path):\n",
    "            emotion_path = os.path.join(output_split_path, emotion)\n",
    "            if not os.path.isdir(emotion_path):\n",
    "                continue\n",
    "            \n",
    "            # Amostrar algumas imagens para análise\n",
    "            sample_images = [f for f in os.listdir(emotion_path) \n",
    "                           if f.lower().endswith(('.jpg', '.png'))][:10]\n",
    "            \n",
    "            for img_file in sample_images:\n",
    "                img_path = os.path.join(emotion_path, img_file)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Calcular métricas\n",
    "                    quality_metrics.append({\n",
    "                        'split': split,\n",
    "                        'emotion': emotion,\n",
    "                        'brightness': np.mean(img),\n",
    "                        'contrast': np.std(img),\n",
    "                        'sharpness': cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "                    })\n",
    "    \n",
    "    if quality_metrics:\n",
    "        df_quality = pd.DataFrame(quality_metrics)\n",
    "        \n",
    "        # Estatísticas gerais\n",
    "        print(\"\\n📊 Métricas de Qualidade (Média):\")\n",
    "        print(f\"  • Brilho:    {df_quality['brightness'].mean():.2f} ± {df_quality['brightness'].std():.2f}\")\n",
    "        print(f\"  • Contraste: {df_quality['contrast'].mean():.2f} ± {df_quality['contrast'].std():.2f}\")\n",
    "        print(f\"  • Nitidez:   {df_quality['sharpness'].mean():.2f} ± {df_quality['sharpness'].std():.2f}\")\n",
    "        \n",
    "        # Gráficos de distribuição\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        axes[0].hist(df_quality['brightness'], bins=20, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('Distribuição de Brilho')\n",
    "        axes[0].set_xlabel('Brilho Médio')\n",
    "        \n",
    "        axes[1].hist(df_quality['contrast'], bins=20, color='lightcoral', edgecolor='black')\n",
    "        axes[1].set_title('Distribuição de Contraste')\n",
    "        axes[1].set_xlabel('Desvio Padrão')\n",
    "        \n",
    "        axes[2].hist(df_quality['sharpness'], bins=20, color='lightgreen', edgecolor='black')\n",
    "        axes[2].set_title('Distribuição de Nitidez')\n",
    "        axes[2].set_xlabel('Variância do Laplaciano')\n",
    "        \n",
    "        plt.suptitle('Análise de Qualidade das Imagens Processadas', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ Nenhuma imagem processada para análise\")\n",
    "\n",
    "# Executar análise\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    analyze_processing_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ebb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 16: Conclusão\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✨ PROCESSAMENTO FINALIZADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(LOG_PATH):\n",
    "    print(f\"\\n📁 Arquivos gerados:\")\n",
    "    print(f\"   • Logs: {LOG_PATH}\")\n",
    "    \n",
    "    log_files = [f for f in os.listdir(LOG_PATH) if f.endswith(('.json', '.csv'))]\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(LOG_PATH, log_file)\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"     - {log_file} ({file_size:.1f} KB)\")\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    print(f\"\\n   • Imagens processadas: {OUTPUT_PATH}\")\n",
    "    \n",
    "    total_processed_images = 0\n",
    "    for split in ['train', 'test']:\n",
    "        split_path = os.path.join(OUTPUT_PATH, split)\n",
    "        if os.path.exists(split_path):\n",
    "            for emotion in os.listdir(split_path):\n",
    "                emotion_path = os.path.join(split_path, emotion)\n",
    "                if os.path.isdir(emotion_path):\n",
    "                    n_images = len([f for f in os.listdir(emotion_path) \n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    total_processed_images += n_images\n",
    "    \n",
    "    print(f\"     - Total: {total_processed_images} imagens processadas\")\n",
    "    \n",
    "    # Calcular espaço em disco\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(OUTPUT_PATH):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    \n",
    "    total_size_mb = total_size / (1024 * 1024)\n",
    "    print(f\"     - Espaço utilizado: {total_size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n📝 Próximos passos:\")\n",
    "print(\"   1. Revisar os logs em:\", LOG_PATH)\n",
    "print(\"   2. Verificar estatísticas no CSV gerado\")\n",
    "print(\"   3. Validar qualidade das imagens processadas\")\n",
    "print(\"   4. Iniciar treinamento do modelo\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Notebook concluído com sucesso!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
